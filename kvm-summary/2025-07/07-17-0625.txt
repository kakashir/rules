From c05eb3e6a to 0685190e0
KVM mailing list update from c05eb3e6a to 0685190e0

Top 15 contributor Email domains (Based on Email Body)

      5 google.com
      4 gmail.com
      4 amd.com
      1 yandex-team.ru

Top 15 contributors (Based on Email Body)

      5  Keir Fraser <keirf@google.com>
      4  Nikunj A Dadhania <nikunj@amd.com>
      2  "Yury Norov (NVIDIA)" <yury.norov@gmail.com>
      1  Yury Norov <yury.norov@gmail.com>
      1  Yury Norov (NVIDIA) <yury.norov@gmail.com>
      1  Nikolay Kuratov <kniv@yandex-team.ru>

===== Patch list in this time period =====


===== Patch Commit Messages ====

New:  mips: kvm: simplify kvm_mips_deliver_interrupts()
[PATCH] mips: kvm: simplify kvm_mips_deliver_interrupts()
Author: Yury Norov <yury.norov@gmail.com>

The function opencodes for_each_set_bit() macro, which makes it bulky.
Using the proper API makes all the housekeeping code go away.

Signed-off-by: Yury Norov <yury.norov@gmail.com>
---
 arch/mips/kvm/interrupt.c | 20 ++------------------
 1 file changed, 2 insertions(+), 18 deletions(-)

----------------------------------------------------------------------

New:  LoongArch: KVM: rework kvm_send_pv_ipi()
[PATCH 1/2] LoongArch: KVM: rework kvm_send_pv_ipi()
Author: Yury Norov <yury.norov@gmail.com>


The function in fact traverses a "bitmap" stored in GPR regs A1 and A2,
but does it in a non-obvious way by creating a single-word bitmap twice.

This patch switches the function to create a single 2-word bitmap, and
also employs for_each_set_bit() macro, as it helps to drop most of
housekeeping code.

While there, convert the function to return void to not confuse readers
with unchecked result.

Signed-off-by: Yury Norov (NVIDIA) <yury.norov@gmail.com>
---
 arch/loongarch/kvm/exit.c | 31 ++++++++++++-------------------
 1 file changed, 12 insertions(+), 19 deletions(-)

----------------------------------------------------------------------

New:  LoongArch: KVM: simplify KVM routines
[PATCH 0/2] LoongArch: KVM: simplify KVM routines
Author: Yury Norov <yury.norov@gmail.com>


Switch KVM functions to use a proper bitmaps API.

Yury Norov (NVIDIA) (2):
  LoongArch: KVM: rework kvm_send_pv_ipi()
  LoongArch: KVM:: simplify kvm_deliver_intr()

 arch/loongarch/kvm/exit.c      | 31 ++++++++++++-------------------
 arch/loongarch/kvm/interrupt.c | 23 +++--------------------
 2 files changed, 15 insertions(+), 39 deletions(-)

----------------------------------------------------------------------

New:  vhost/net: Replace wait_queue with completion in ubufs reference
[PATCH] vhost/net: Replace wait_queue with completion in ubufs reference
Author: Nikolay Kuratov <kniv@yandex-team.ru>

When operating on struct vhost_net_ubuf_ref, the following execution
sequence is theoretically possible:
CPU0 is finalizing DMA operation                   CPU1 is doing VHOST_NET_SET_BACKEND
                             // &ubufs->refcount == 2
vhost_net_ubuf_put()                               vhost_net_ubuf_put_wait_and_free(oldubufs)
                                                     vhost_net_ubuf_put_and_wait()
                                                       vhost_net_ubuf_put()
                                                         int r = atomic_sub_return(1, &ubufs->refcount);
                                                         // r = 1
int r = atomic_sub_return(1, &ubufs->refcount);
// r = 0
                                                      wait_event(ubufs->wait, !atomic_read(&ubufs->refcount));
                                                      // no wait occurs here because condition is already true
                                                    kfree(ubufs);
if (unlikely(!r))
  wake_up(&ubufs->wait);  // use-after-free

This leads to use-after-free on ubufs access. This happens because CPU1
skips waiting for wake_up() when refcount is already zero.

To prevent that use a completion instead of wait_queue as the ubufs
notification mechanism. wait_for_completion() guarantees that there will
be complete() call prior to its return.

We also need to reinit completion because refcnt == 0 does not mean
freeing in case of vhost_net_flush() - it then sets refcnt back to 1.
AFAIK concurrent calls to vhost_net_ubuf_put_and_wait() with the same
ubufs object aren't possible since those calls (through vhost_net_flush()
or vhost_net_set_backend()) are protected by the device mutex.
So reinit_completion() right after wait_for_completion() should be fine.

Cc: stable@vger.kernel.org
Fixes: 0ad8b480d6ee9 ("vhost: fix ref cnt checking deadlock")
Reported-by: Andrey Ryabinin <arbn@yandex-team.com>
Suggested-by: Andrey Smetanin <asmetanin@yandex-team.ru>
Signed-off-by: Nikolay Kuratov <kniv@yandex-team.ru>
---
 drivers/vhost/net.c | 9 +++++----
 1 file changed, 5 insertions(+), 4 deletions(-)

----------------------------------------------------------------------

New:  KVM: arm64: vgic-init: Remove vgic_ready() macro
[PATCH v2 1/4] KVM: arm64: vgic-init: Remove vgic_ready() macro
Author: Keir Fraser <keirf@google.com>

It is now used only within kvm_vgic_map_resources(). vgic_dist::ready
is already written directly by this function, so it is clearer to
bypass the macro for reads as well.

Signed-off-by: Keir Fraser <keirf@google.com>
---
 arch/arm64/kvm/vgic/vgic-init.c | 5 ++---
 include/kvm/arm_vgic.h          | 1 -
 2 files changed, 2 insertions(+), 4 deletions(-)

----------------------------------------------------------------------

New:  KVM: Speed up MMIO registrations
[PATCH v2 0/4] KVM: Speed up MMIO registrations
Author: Keir Fraser <keirf@google.com>

This is version 2 of the patches I previously posted here:

 https://lore.kernel.org/all/20250624092256.1105524-1-keirf@google.com/

Changes since v1:

 * Memory barriers introduced (or implied and documented) on the
   kvm->buses[] SRCU read paths

 * Disallow kvm_get_bus() from SRCU readers

 * Rebased to v6.16-rc6

Thanks to Sean for the review feedback and patch suggestions!

Keir Fraser (4):
  KVM: arm64: vgic-init: Remove vgic_ready() macro
  KVM: arm64: vgic: Explicitly implement vgic_dist::ready ordering
  KVM: Implement barriers before accessing kvm->buses[] on SRCU read
    paths
  KVM: Avoid synchronize_srcu() in kvm_io_bus_register_dev()

 arch/arm64/kvm/vgic/vgic-init.c | 14 +++--------
 arch/x86/kvm/vmx/vmx.c          |  7 ++++++
 include/kvm/arm_vgic.h          |  1 -
 include/linux/kvm_host.h        | 11 ++++++---
 virt/kvm/kvm_main.c             | 43 +++++++++++++++++++++++++++------
 5 files changed, 53 insertions(+), 23 deletions(-)

----------------------------------------------------------------------

New:  x86/cpufeatures: Add SNP Secure TSC
[PATCH v9 1/2] x86/cpufeatures: Add SNP Secure TSC
Author: Nikunj A Dadhania <nikunj@amd.com>

The Secure TSC feature for SEV-SNP allows guests to securely use the RDTSC
and RDTSCP instructions, ensuring that the parameters used cannot be
altered by the hypervisor once the guest is launched. For more details,
refer to the AMD64 APM Vol 2, Section "Secure TSC".

Acked-by: Borislav Petkov (AMD) <bp@alien8.de>
Reviewed-by: Tom Lendacky <thomas.lendacky@amd.com>
Tested-by: Vaishali Thakkar <vaishali.thakkar@suse.com>
Signed-off-by: Nikunj A Dadhania <nikunj@amd.com>
---
 arch/x86/include/asm/cpufeatures.h | 1 +
 1 file changed, 1 insertion(+)

----------------------------------------------------------------------

New:  Enable Secure TSC for SEV-SNP
[PATCH v9 0/2] Enable Secure TSC for SEV-SNP
Author: Nikunj A Dadhania <nikunj@amd.com>

Patches are based on kvm-x86/next with [1] applied

Testing Secure TSC
-----------------

Secure TSC guest patches are available as part of v6.14.

QEMU changes:
https://github.com/AMDESE/qemu/tree/snp-securetsc-latest

QEMU command line SEV-SNP with Secure TSC:

  qemu-system-x86_64 -cpu EPYC-Milan-v2 -smp 4 \
    -object memory-backend-memfd,id=ram1,size=1G,share=true,prealloc=false,reserve=false \
    -object sev-snp-guest,id=sev0,cbitpos=51,reduced-phys-bits=1,secure-tsc=on,stsc-freq=2000000000 \
    -machine q35,confidential-guest-support=sev0,memory-backend=ram1 \
    ...

Changelog:
----------
v9:
* Set guest_tsc_protected during guest vCPU creation (Kai Huang)
* Improve error handling (Kai Huang)
* Disable MSR_AMD64_GUEST_TSC_FREQ write interception (Sean)

v8: https://lore.kernel.org/kvm/20250707101029.927906-1-nikunj@amd.com/
* Commit message improvements (Kai Huang)
* Remove 'desired_tsc_khz' from 'struct kvm_sev_snp_launch_start' (Kai Huang)

Nikunj A Dadhania (2):
  x86/cpufeatures: Add SNP Secure TSC
  KVM: SVM: Enable Secure TSC for SNP guests

 arch/x86/include/asm/cpufeatures.h |  1 +
 arch/x86/include/asm/svm.h         |  1 +
 arch/x86/kvm/svm/sev.c             | 27 +++++++++++++++++++++++++++
 arch/x86/kvm/svm/svm.c             |  2 ++
 arch/x86/kvm/svm/svm.h             |  2 ++
 5 files changed, 33 insertions(+)

----------------------------------------------------------------------

