From 773ccbe57 to 75a4db779
KVM mailing list update from 773ccbe57 to 75a4db779

Top 15 contributor Email domains (Based on Email Body)

     14 redhat.com
      8 google.com
      4 linux.intel.com
      3 intel.com
      2 rivosinc.com
      2 arm.com
      1 linux.ibm.com

Top 15 contributors (Based on Email Body)

     10  Paolo Abeni <pabeni@redhat.com>
      4  "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
      4  James Houghton <jthoughton@google.com>
      3  Xiaoyao Li <xiaoyao.li@intel.com>
      3  Vipin Sharma <vipinsh@google.com>
      3  Jason Wang <jasowang@redhat.com>
      2  Mark Rutland <mark.rutland@arm.com>
      2  Jesse Taube <jesse@rivosinc.com>
      1  Paolo Bonzini <pbonzini@redhat.com>
      1  David Matlack <dmatlack@google.com>
      1  Andrew Donnellan <ajd@linux.ibm.com>

===== Patch list in this time period =====


===== Patch Commit Messages ====

New:  lib: Add STR_IS_Y and STR_IS_N for checking env vars
[kvm-unit-tests PATCH v2 1/2] lib: Add STR_IS_Y and STR_IS_N for checking env vars
Author: Jesse Taube <jesse@rivosinc.com>

the line:
(s && (*s == '1' || *s == 'y' || *s == 'Y'))
is used in a few places add a macro for it and it's 'n' counterpart.

Add GET_CONFIG_OR_ENV for CONFIG values which can be overridden by
the environment.

Signed-off-by: Jesse Taube <jesse@rivosinc.com>
---
 lib/argv.h        | 13 +++++++++++++
 lib/errata.h      |  7 ++++---
 riscv/sbi-tests.h |  3 ++-
 3 files changed, 19 insertions(+), 4 deletions(-)

----------------------------------------------------------------------

New:  MAINTAINERS: Update TDX entry
[PATCH 0/3] MAINTAINERS: Update TDX entry
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>

The patchset updates the TDX entry in MAINTAINERS:

  - Add missing TDX files to the list, including KVM enabling;
  - Add Rick Edgecombe as a reviewer;
  - Update my email address.

Paolo, Sean, are you okay for TDX KVM stuff to be covered by the same
MAINTAINERS entry?

I don't see a reason why not, but I want to double-check.

Kirill A. Shutemov (3):
  MAINTAINERS: Update the file list in the TDX entry.
  MAINTAINERS: Add Rick Edgecombe as a TDX reviewer
  MAINTAINERS: Update Kirill Shutemov's email address

 .mailmap    |  1 +
 MAINTAINERS | 15 +++++++++++----
 2 files changed, 12 insertions(+), 4 deletions(-)

----------------------------------------------------------------------

New:  MAINTAINERS: Update the file list in the TDX entry.
[PATCH 1/3] MAINTAINERS: Update the file list in the TDX entry.
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>

Include files that were previously missed in the TDX entry file list.
It also includes the recently added KVM enabling.

Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
---
 MAINTAINERS | 12 +++++++++---
 1 file changed, 9 insertions(+), 3 deletions(-)

----------------------------------------------------------------------

New:  entry: Add arch_in_rcu_eqs()
[PATCH 1/2] entry: Add arch_in_rcu_eqs()
Author: Andrew Donnellan <ajd@linux.ibm.com>


All architectures have an interruptible RCU extended quiescent state
(EQS) as part of their idle sequences, where interrupts can occur
without RCU watching. Entry code must account for this and wake RCU as
necessary; the common entry code deals with this in irqentry_enter() by
treating any interrupt from an idle thread as potentially having
occurred within an EQS and waking RCU for the duration of the interrupt
via rcu_irq_enter() .. rcu_irq_exit().

Some architectures may have other interruptible EQSs which require
similar treatment. For example, on s390 it is necessary to enable
interrupts around guest entry in the middle of a period where core KVM
code has entered an EQS.

So that architectures can wake RCU in these cases, this patch adds a
new arch_in_rcu_eqs() hook to the common entry code which is checked in
addition to the existing is_idle_thread() check, with RCU woken if
either returns true. A default implementation is provided which always
returns false, which suffices for most architectures.

As no architectures currently implement arch_in_rcu_eqs(), there should
be no functional change as a result of this patch alone. A subsequent
patch will add an s390 implementation to fix a latent bug with missing
RCU wakeups.

[ajd@linux.ibm.com: rebase, fix commit message]
Signed-off-by: Mark Rutland <mark.rutland@arm.com>
Cc: Andy Lutomirski <luto@kernel.org>
Cc: Christian Borntraeger <borntraeger@linux.ibm.com>
Cc: Heiko Carstens <hca@linux.ibm.com>
Cc: Paolo Bonzini <pbonzini@redhat.com>
Cc: Paul E. McKenney <paulmck@kernel.org>
Cc: Peter Zijlstra <peterz@infradead.org>
Cc: Sven Schnelle <svens@linux.ibm.com>
Cc: Thomas Gleixner <tglx@linutronix.de>
Cc: Claudio Imbrenda <imbrenda@linux.ibm.com>
Cc: Vasily Gorbik <gor@linux.ibm.com>
Cc: Alexander Gordeev <agordeev@linux.ibm.com>
Cc: Janosch Frank <frankja@linux.ibm.com>
Reviewed-by: Christian Borntraeger <borntraeger@linux.ibm.com>
Signed-off-by: Andrew Donnellan <ajd@linux.ibm.com>
---
 include/linux/entry-common.h | 16 ++++++++++++++++
 kernel/entry/common.c        |  3 ++-
 2 files changed, 18 insertions(+), 1 deletion(-)

----------------------------------------------------------------------

New:  KVM: s390: Fix latent guest entry/exit bugs
[PATCH 0/2] KVM: s390: Fix latent guest entry/exit bugs
Author: Andrew Donnellan <ajd@linux.ibm.com>

In [0], the guest_{enter,exit}_irqoff() helpers were deprecated, in favour
of guest_timing_{enter,exit}_irqoff() and
guest_context_{enter,exit}_irqoff(). This was to fix a number of latent
guest entry/exit bugs, relating to the enabling of interrupts during an
RCU extended quiescent state, instrumentation code, and correct handling
of lockdep and tracing.

However, while arm64, mips, riscv and x86 have been migrated to the new
helpers, s390 hasn't been. There was an initial attempt at [1] to do this,
but that didn't work for reasons discussed at [2].

Since then, Claudio Imbrenda has reworked much of the interrupt handling.
Moving interrupt handling into vcpu_post_run() avoids the issues in [2],
so we can now move to the new helpers.

I've rebased Mark's patches from [1]. kvm-unit-tests, the kvm selftests,
and IBM's internal test suites pass under debug_defconfig.

These patches do introduce some overhead - in my testing, a few of the
tests in the kvm-unit-tests exittime test suite appear 6-11% slower, but
some noticeable overhead may be unavoidable (we introduce a new function
call and the irq entry/exit paths change a bit).

[0] https://lore.kernel.org/lkml/20220201132926.3301912-1-mark.rutland@arm.com/
[1] https://lore.kernel.org/all/20220119105854.3160683-7-mark.rutland@arm.com/
[2] https://lore.kernel.org/all/a4a26805-3a56-d264-0a7e-60bed1ada9f3@linux.ibm.com/
[3] https://lore.kernel.org/all/20241022120601.167009-1-imbrenda@linux.ibm.com/

Mark Rutland (2):
  entry: Add arch_in_rcu_eqs()
  KVM: s390: Rework guest entry logic

 arch/s390/include/asm/entry-common.h | 10 ++++++
 arch/s390/include/asm/kvm_host.h     |  3 ++
 arch/s390/kvm/kvm-s390.c             | 51 +++++++++++++++++++++-------
 arch/s390/kvm/vsie.c                 | 17 ++++------
 include/linux/entry-common.h         | 16 +++++++++
 kernel/entry/common.c                |  3 +-
 6 files changed, 77 insertions(+), 23 deletions(-)

----------------------------------------------------------------------

New:  x86/tdx: Fix the typo of TDX_ATTR_MIGRTABLE
[PATCH 1/2] x86/tdx: Fix the typo of TDX_ATTR_MIGRTABLE
Author: Xiaoyao Li <xiaoyao.li@intel.com>

Fix the typo of TDX_ATTR_MIGRTABLE to TDX_ATTR_MIGRATABLE.

Reviewed-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Signed-off-by: Xiaoyao Li <xiaoyao.li@intel.com>
---
 arch/x86/coco/tdx/debug.c         | 2 +-
 arch/x86/include/asm/shared/tdx.h | 4 ++--
 2 files changed, 3 insertions(+), 3 deletions(-)

----------------------------------------------------------------------

New:  TDX: Clean up the definitions of TDX ATTRIBUTES
[PATCH 0/2] TDX: Clean up the definitions of TDX ATTRIBUTES
Author: Xiaoyao Li <xiaoyao.li@intel.com>

It's a simple series. Patch 1 fixes the typo and Patch 2 removes the
redundant definitions of TD ATTRIBUTES bits.

Although some duplications were identified during the community review
of TDX KVM base support[1][2], a few slipped through unnoticed due to
the simultaneous evolution of the TD guest part.

[1] https://lore.kernel.org/all/e5387c7c-9df8-4e39-bbe9-23e8bb09e527@intel.com/
[2] https://lore.kernel.org/all/25bf543723a176bf910f27ede288f3d20f20aed1.camel@intel.com/

Xiaoyao Li (2):
  x86/tdx: Fix the typo of TDX_ATTR_MIGRTABLE
  KVM: TDX: Remove redundant definitions of TDX_TD_ATTR_*

 arch/x86/coco/tdx/debug.c         | 2 +-
 arch/x86/include/asm/shared/tdx.h | 4 ++--
 arch/x86/kvm/vmx/tdx.c            | 4 ++--
 arch/x86/kvm/vmx/tdx_arch.h       | 6 ------
 4 files changed, 5 insertions(+), 11 deletions(-)

----------------------------------------------------------------------

New:  scripts/kernel_doc.py: properly handle VIRTIO_DECLARE_FEATURES
[PATCH v7 net-next 1/9] scripts/kernel_doc.py: properly handle VIRTIO_DECLARE_FEATURES
Author: Paolo Abeni <pabeni@redhat.com>

The mentioned macro introduce by the next patch will foul kdoc;
fully expand the mentioned macro to avoid the issue.

Signed-off-by: Paolo Abeni <pabeni@redhat.com>
---
 scripts/lib/kdoc/kdoc_parser.py | 1 +
 1 file changed, 1 insertion(+)

----------------------------------------------------------------------

New:  virtio: introduce GSO over UDP tunnel
[PATCH v7 net-next 0/9] virtio: introduce GSO over UDP tunnel
Author: Paolo Abeni <pabeni@redhat.com>

Some virtualized deployments use UDP tunnel pervasively and are impacted
negatively by the lack of GSO support for such kind of traffic in the
virtual NIC driver.

The virtio_net specification recently introduced support for GSO over
UDP tunnel, this series updates the virtio implementation to support
such a feature.

Currently the kernel virtio support limits the feature space to 64,
while the virtio specification allows for a larger number of features.
Specifically the GSO-over-UDP-tunnel-related virtio features use bits
65-69.

The first four patches in this series rework the virtio and vhost
feature support to cope with up to 128 bits. The limit is set by
a define and could be easily raised in future, as needed.

This implementation choice is aimed at keeping the code churn as
limited as possible. For the same reason, only the virtio_net driver is
reworked to leverage the extended feature space; all other
virtio/vhost drivers are unaffected, but could be upgraded to support
the extended features space in a later time.

The last four patches bring in the actual GSO over UDP tunnel support.
As per specification, some additional fields are introduced into the
virtio net header to support the new offload. The presence of such
fields depends on the negotiated features.

New helpers are introduced to convert the UDP-tunneled skb metadata to
an extended virtio net header and vice versa. Such helpers are used by
the tun and virtio_net driver to cope with the newly supported offloads.

Tested with basic stream transfer with all the possible permutations of
host kernel/qemu/guest kernel with/without GSO over UDP tunnel support.

---
WRT the merge plan, this is also are available in the Git repository at
[1]:

git@github.com:pabeni/linux-devel.git virtio_udp_tunnel_07_07_2025

The first 5 patches in this series, that is, the virtio features
extension bits are also available at [2]:

git@github.com:pabeni/linux-devel.git virtio_features_extension_07_07_2025

Ideally the virtio features extension bit should go via the virtio tree
and the virtio_net/tun patches via the net-next tree. The latter have
a dependency in the first and will cause conflicts if merged via the
virtio tree, both when applied and at merge window time - inside Linus
tree.

To avoid such conflicts and duplicate commits I think the net-next
could pull from [1], while the virtio tree could pull from [2].
---
v6 -> v7:
  - avoid warning in csky build
  - rebased
v6: https://lore.kernel.org/netdev/cover.1750753211.git.pabeni@redhat.com/

v5 -> v6:
  - fix integer overflow in patch 4/9
v5: https://lore.kernel.org/netdev/cover.1750436464.git.pabeni@redhat.com/

v4 -> v5:
  - added new patch 1/9 to avoid kdoc issues
  - encapsulate guest features guessing in new tap helper
  - cleaned-up SET_FEATURES_ARRAY
  - a few checkpatch fixes
v4: https://lore.kernel.org/netdev/cover.1750176076.git.pabeni@redhat.com/

v3 -> v4:
  - vnet sockopt cleanup
  - fixed offset for UDP-tunnel related field
  - use dev->features instead of flags
v3: https://lore.kernel.org/netdev/cover.1749210083.git.pabeni@redhat.com/

v2 -> v3:
  - uint128_t -> u64[2]
  - dropped related ifdef
  - define and use vnet_hdr with tunnel layouts
v2: https://lore.kernel.org/netdev/cover.1748614223.git.pabeni@redhat.com/

v1 -> v2:
  - fix build failures
  - many comment clarification
  - changed the vhost_net ioctl API
  - fixed some hdr <> skb helper bugs
v1: https://lore.kernel.org/netdev/cover.1747822866.git.pabeni@redhat.com/

Paolo Abeni (9):
  scripts/kernel_doc.py: properly handle VIRTIO_DECLARE_FEATURES
  virtio: introduce extended features
  virtio_pci_modern: allow configuring extended features
  vhost-net: allow configuring extended features
  virtio_net: add supports for extended offloads
  net: implement virtio helpers to handle UDP GSO tunneling.
  virtio_net: enable gso over UDP tunnel support.
  tun: enable gso over UDP tunnel support.
  vhost/net: enable gso over UDP tunnel support.

 drivers/net/tun.c                      |  58 ++++++--
 drivers/net/tun_vnet.h                 | 101 +++++++++++--
 drivers/net/virtio_net.c               | 110 +++++++++++---
 drivers/vhost/net.c                    |  95 +++++++++---
 drivers/vhost/vhost.c                  |   2 +-
 drivers/vhost/vhost.h                  |   4 +-
 drivers/virtio/virtio.c                |  43 +++---
 drivers/virtio/virtio_debug.c          |  27 ++--
 drivers/virtio/virtio_pci_modern.c     |  10 +-
 drivers/virtio/virtio_pci_modern_dev.c |  69 +++++----
 include/linux/virtio.h                 |   9 +-
 include/linux/virtio_config.h          |  43 +++---
 include/linux/virtio_features.h        |  88 +++++++++++
 include/linux/virtio_net.h             | 197 ++++++++++++++++++++++++-
 include/linux/virtio_pci_modern.h      |  43 +++++-
 include/uapi/linux/if_tun.h            |   9 ++
 include/uapi/linux/vhost.h             |   7 +
 include/uapi/linux/vhost_types.h       |   5 +
 include/uapi/linux/virtio_net.h        |  33 +++++
 scripts/lib/kdoc/kdoc_parser.py        |   1 +
 20 files changed, 790 insertions(+), 164 deletions(-)

----------------------------------------------------------------------

New:  vhost: basic in order support
[PATCH net-next 1/2] vhost: basic in order support
Author: Jason Wang <jasowang@redhat.com>

This patch adds basic in order support for vhost. Two optimizations
are implemented in this patch:

1) Since driver uses descriptor in order, vhost can deduce the next
   avail ring head by counting the number of descriptors that has been
   used in next_avail_head. This eliminate the need to access the
   available ring in vhost.

2) vhost_add_used_and_singal_n() is extended to accept the number of
   batched buffers per used elem. While this increases the times of
   usersapce memory access but it helps to reduce the chance of
   used ring access of both the driver and vhost.

Vhost-net will be the first user for this.

Signed-off-by: Jason Wang <jasowang@redhat.com>
---
 drivers/vhost/net.c   |   6 ++-
 drivers/vhost/vhost.c | 121 +++++++++++++++++++++++++++++++++++-------
 drivers/vhost/vhost.h |   8 ++-
 3 files changed, 111 insertions(+), 24 deletions(-)

----------------------------------------------------------------------

New:  in order support for vhost-net
[PATCH net-next 0/2] in order support for vhost-net
Author: Jason Wang <jasowang@redhat.com>

Hi all,

This series implements VIRTIO_F_IN_ORDER support for vhost-net. This
feature is designed to improve the performance of the virtio ring by
optimizing descriptor processing.

Benchmarks show a notable improvement. Please see patch 2 for details.

Thanks

Jason Wang (2):
  vhost: basic in order support
  vhost_net: basic in_order support

 drivers/vhost/net.c   |  88 +++++++++++++++++++++---------
 drivers/vhost/vhost.c | 121 +++++++++++++++++++++++++++++++++++-------
 drivers/vhost/vhost.h |   8 ++-
 3 files changed, 170 insertions(+), 47 deletions(-)

----------------------------------------------------------------------

New:  KVM: x86/mmu: Track TDP MMU NX huge pages separately
[PATCH v5 1/7] KVM: x86/mmu: Track TDP MMU NX huge pages separately
Author: James Houghton <jthoughton@google.com>


Introduce struct kvm_possible_nx_huge_pages to track the list of
possible NX huge pages and the number of pages on the list.

When calculating how many pages to zap, we use the new counts we have
(instead of kvm->stat.nx_lpage_splits, which would be the sum of the two
new counts).

Suggested-by: Sean Christopherson <seanjc@google.com>
Suggested-by: David Matlack <dmatlack@google.com>
Signed-off-by: Vipin Sharma <vipinsh@google.com>
Co-developed-by: James Houghton <jthoughton@google.com>
Signed-off-by: James Houghton <jthoughton@google.com>
---
 arch/x86/include/asm/kvm_host.h | 43 ++++++++++++++++--------
 arch/x86/kvm/mmu/mmu.c          | 58 +++++++++++++++++++++------------
 arch/x86/kvm/mmu/mmu_internal.h |  7 ++--
 arch/x86/kvm/mmu/tdp_mmu.c      |  4 +--
 4 files changed, 75 insertions(+), 37 deletions(-)

----------------------------------------------------------------------

New:  KVM: x86/mmu: Run TDP MMU NX huge page recovery under
[PATCH v5 0/7] KVM: x86/mmu: Run TDP MMU NX huge page recovery under
Author: James Houghton <jthoughton@google.com>

Hi Sean/Paolo,

I'm finishing off Vipin's NX huge page recovery optimization for the TDP
MMU from last year. This is a respin on the series I sent a couple weeks
ago, v4. Below is a mostly unchanged cover letter from v4.

NX huge page recovery can cause guest performance jitter, originally
noticed with network tests in Windows guests. Please see Vipin's earlier
performance results[1]. Below is some new data I have collected with the
nx_huge_pages_perf_test that I've included with this series.

The NX huge page recovery for the shadow MMU is still done under the MMU
write lock, but with the TDP MMU, we can instead do it under the MMU
read lock by:

1. Tracking the possible NX huge pages for the two MMUs separately
   (patch 1).
2. Updating the NX huge page recovery routine for the TDP MMU to
    - zap SPTEs atomically, and
    - grab tdp_mmu_pages_lock to iterate over the NX huge page list
   (patch 3).

I threw in patch 4 because it seems harmless and closer to the "right"
thing to do. Feel free to drop it if you don't agree with me. :)

I'm also grabbing David's execute_perf_test[3] while I'm at it. It was
dropped before simply because it didn't apply at the time. David's test
works well as a stress test for NX huge page recovery when NX huge page
recovery is tuned to be very aggressive.

Changes since v4[4]:
- 32-bit build fixups for patch 1 and 3.
- Small variable rename in patch 3.

Changes since v3[2]:
- Dropped the move of the `sp->nx_huge_page_disallowed` check to outside
  of the tdp_mmu_pages_lock.
- Implemented Sean's array suggestion for `possible_nx_huge_pages`.
- Implemented some other cleanup suggestions from Sean.
- Made shadow MMU not take the RCU lock in NX huge page recovery.
- Added a selftest for measuring jitter.
- Added David's execute_perf_test[3].

-- Results
$ cat /sys/module/kvm/parameters/nx_huge_pages_recovery_period_ms
100
$ cat /sys/module/kvm/parameters/nx_huge_pages_recovery_ratio
4

$ ./nx_huge_pages_perf_test -b 16G -s anonymous_hugetlb_1gb
[Unpatched] Max fault latency: 8496724 cycles
[Unpatched] Max fault latency: 8404426 cycles
[ Patched ] Max fault latency: 49418 cycles
[ Patched ] Max fault latency: 51948 cycles

$ ./nx_huge_pages_perf_test -b 16G -s anonymous_hugetlb_2mb
[Unpatched] Max fault latency: 5320740 cycles
[Unpatched] Max fault latency: 5384554 cycles
[ Patched ] Max fault latency: 50052 cycles
[ Patched ] Max fault latency: 103774 cycles

$ ./nx_huge_pages_perf_test -b 16G -s anonymous_thp
[Unpatched] Max fault latency: 7625022 cycles
[Unpatched] Max fault latency: 6339934 cycles
[ Patched ] Max fault latency: 107976 cycles
[ Patched ] Max fault latency: 108386 cycles

$ ./nx_huge_pages_perf_test -b 16G -s anonymous
[Unpatched] Max fault latency: 143036 cycles
[Unpatched] Max fault latency: 287444 cycles
[ Patched ] Max fault latency: 274626 cycles
[ Patched ] Max fault latency: 303984 cycles

We can see about a 100x decrease in maximum fault latency for both
2M pages and 1G pages. This test is only timing writes to unmapped
pages that are not themselves currently undergoing NX huge page
recovery. The test only produces interesting results when NX huge page
recovery is actually occurring, so the parameters are tuned to make it
very likely for NX huge page recovery to occur in the middle of the
test.

Based on latest kvm/next.

[1]: https://lore.kernel.org/kvm/20240906204515.3276696-3-vipinsh@google.com/
[2]: https://lore.kernel.org/kvm/20240906204515.3276696-1-vipinsh@google.com/
[3]: https://lore.kernel.org/kvm/20221109185905.486172-2-dmatlack@google.com/
[4]: https://lore.kernel.org/kvm/20250616181144.2874709-1-jthoughton@google.com/

David Matlack (1):
  KVM: selftests: Introduce a selftest to measure execution performance

James Houghton (3):
  KVM: x86/mmu: Only grab RCU lock for nx hugepage recovery for TDP MMU
  KVM: selftests: Provide extra mmap flags in vm_mem_add()
  KVM: selftests: Add an NX huge pages jitter test

Vipin Sharma (3):
  KVM: x86/mmu: Track TDP MMU NX huge pages separately
  KVM: x86/mmu: Rename kvm_tdp_mmu_zap_sp() to better indicate its
    purpose
  KVM: x86/mmu: Recover TDP MMU NX huge pages using MMU read lock

 arch/x86/include/asm/kvm_host.h               |  43 +++-
 arch/x86/kvm/mmu/mmu.c                        | 180 +++++++++-----
 arch/x86/kvm/mmu/mmu_internal.h               |   7 +-
 arch/x86/kvm/mmu/tdp_mmu.c                    |  49 +++-
 arch/x86/kvm/mmu/tdp_mmu.h                    |   3 +-
 tools/testing/selftests/kvm/Makefile.kvm      |   2 +
 .../testing/selftests/kvm/execute_perf_test.c | 199 ++++++++++++++++
 .../testing/selftests/kvm/include/kvm_util.h  |   3 +-
 .../testing/selftests/kvm/include/memstress.h |   4 +
 tools/testing/selftests/kvm/lib/kvm_util.c    |  15 +-
 tools/testing/selftests/kvm/lib/memstress.c   |  25 +-
 .../kvm/x86/nx_huge_pages_perf_test.c         | 223 ++++++++++++++++++
 .../kvm/x86/private_mem_conversions_test.c    |   2 +-
 13 files changed, 656 insertions(+), 99 deletions(-)

----------------------------------------------------------------------

