From 576684175 to 4f7a4fc99
KVM mailing list update from 576684175 to 4f7a4fc99

Top 15 contributor Email domains (Based on Email Body)

     13 meta.com
      6 redhat.com
      6 google.com
      1 gmail.com
      1 bytedance.com

Top 15 contributors (Based on Email Body)

     13  Bobby Eshleman <bobbyeshleman@meta.com>
      6  Paolo Bonzini <pbonzini@redhat.com>
      3  Kevin Cheng <chengkev@google.com>
      3  Aaron Lewis <aaronlewis@google.com>
      1  "Yanfei Xu" <yanfei.xu@bytedance.com>
      1  Bobby Eshleman <bobbyeshleman@gmail.com>

===== Patch list in this time period =====


===== Patch Commit Messages ====

New:  KVM: irqchip: KVM: Reduce allocation overhead in kvm_set_irq_routing()
[PATCH] KVM: irqchip: KVM: Reduce allocation overhead in kvm_set_irq_routing()
Author: Yanfei Xu <yanfei.xu@bytedance.com>

In guests with many VFIO devices and MSI-X vectors, kvm_set_irq_routing()
becomes a high-overhead operation. Each invocation walks the entire IRQ
routing table and reallocates/frees every routing entry.

As the routing table grows on each call, entry allocation and freeing
dominate the execution time of this function. In scenarios such as VM
live migration or live upgrade, this behavior can introduce unnecessary
downtime.

Allocate memory for all routing entries in one shot using kcalloc(),
allowing them to be freed together with a single kfree() call.

Example: On a VM with 120 vCPUs and 15 VFIO devices (virtio-net), the
total number of calls to kzalloc and kfree is over 20000. With this
change, it is reduced to around 30.

Reported-by: Xiangfeng Cai <caixiangfeng@bytedance.com>
Signed-off-by: Yanfei Xu <yanfei.xu@bytedance.com>
---
 include/linux/kvm_host.h |  1 +
 virt/kvm/irqchip.c       | 21 +++++++++------------
 2 files changed, 10 insertions(+), 12 deletions(-)

----------------------------------------------------------------------

New:  vsock: add namespace support to
[PATCH RFC net-next v13 00/13] vsock: add namespace support to
Author: Bobby Eshleman <bobbyeshleman@gmail.com>

This series adds namespace support to vhost-vsock and loopback. It does
not add namespaces to any of the other guest transports (virtio-vsock,
hyperv, or vmci).

The current revision supports two modes: local and global. Local
mode is complete isolation of namespaces, while global mode is complete
sharing between namespaces of CIDs (the original behavior).

The mode is set using the parent namespace's
/proc/sys/net/vsock/child_ns_mode and inherited when a new namespace is
created. The mode of the current namespace can be queried by reading
/proc/sys/net/vsock/ns_mode. The mode can not change after the namespace
has been created.

Modes are per-netns. This allows a system to configure namespaces
independently (some may share CIDs, others are completely isolated).
This also supports future possible mixed use cases, where there may be
namespaces in global mode spinning up VMs while there are mixed mode
namespaces that provide services to the VMs, but are not allowed to
allocate from the global CID pool (this mode is not implemented in this
series).

Additionally, added tests for the new namespace features:

tools/testing/selftests/vsock/vmtest.sh
1..25
ok 1 vm_server_host_client
ok 2 vm_client_host_server
ok 3 vm_loopback
ok 4 ns_host_vsock_ns_mode_ok
ok 5 ns_host_vsock_child_ns_mode_ok
ok 6 ns_global_same_cid_fails
ok 7 ns_local_same_cid_ok
ok 8 ns_global_local_same_cid_ok
ok 9 ns_local_global_same_cid_ok
ok 10 ns_diff_global_host_connect_to_global_vm_ok
ok 11 ns_diff_global_host_connect_to_local_vm_fails
ok 12 ns_diff_global_vm_connect_to_global_host_ok
ok 13 ns_diff_global_vm_connect_to_local_host_fails
ok 14 ns_diff_local_host_connect_to_local_vm_fails
ok 15 ns_diff_local_vm_connect_to_local_host_fails
ok 16 ns_diff_global_to_local_loopback_local_fails
ok 17 ns_diff_local_to_global_loopback_fails
ok 18 ns_diff_local_to_local_loopback_fails
ok 19 ns_diff_global_to_global_loopback_ok
ok 20 ns_same_local_loopback_ok
ok 21 ns_same_local_host_connect_to_local_vm_ok
ok 22 ns_same_local_vm_connect_to_local_host_ok
ok 23 ns_delete_vm_ok
ok 24 ns_delete_host_ok
ok 25 ns_delete_both_ok
SUMMARY: PASS=25 SKIP=0 FAIL=0

Thanks again for everyone's help and reviews!

Suggested-by: Sargun Dhillon <sargun@sargun.me>
Signed-off-by: Bobby Eshleman <bobbyeshleman@gmail.com>

Changes in v13:
- add support for immutable sysfs ns_mode and inheritance from sysfs child_ns_mode
- remove passing around of net_mode, can be accessed now via
  vsock_net_mode(net) since it is immutable
- update tests for new uAPI
- add one patch to extend the kselftest timeout (it was starting to
  fail with the new tests added)
- Link to v12: https://lore.kernel.org/r/20251126-vsock-vmtest-v12-0-257ee21cd5de@meta.com

Changes in v12:
- add ns mode checking to _allow() callbacks to reject local mode for
  incompatible transports (Stefano)
- flip vhost/loopback to return true for stream_allow() and
  seqpacket_allow() in "vsock: add netns support to virtio transports"
  (Stefano)
- add VMADDR_CID_ANY + local mode documentation in af_vsock.c (Stefano)
- change "selftests/vsock: add tests for host <-> vm connectivity with
  namespaces" to skip test 29 in vsock_test for namespace local
  vsock_test calls in a host local-mode namespace. There is a
  false-positive edge case for that test encountered with the
  ->stream_allow() approach. More details in that patch.
- updated cover letter with new test output
- Link to v11: https://lore.kernel.org/r/20251120-vsock-vmtest-v11-0-55cbc80249a7@meta.com

Changes in v11:
- vmtest: add a patch to use ss in wait_for_listener functions and
  support vsock, tcp, and unix. Change all patches to use the new
  functions.
- vmtest: add a patch to re-use vm dmesg / warn counting functions
- Link to v10: https://lore.kernel.org/r/20251117-vsock-vmtest-v10-0-df08f165bf3e@meta.com

Changes in v10:
- Combine virtio common patches into one (Stefano)
- Resolve vsock_loopback virtio_transport_reset_no_sock() issue
  with info->vsk setting. This eliminates the need for skb->cb,
  so remove skb->cb patches.
- many line width 80 fixes
- Link to v9: https://lore.kernel.org/all/20251111-vsock-vmtest-v9-0-852787a37bed@meta.com

Changes in v9:
- reorder loopback patch after patch for virtio transport common code
- remove module ordering tests patch because loopback no longer depends
  on pernet ops
- major simplifications in vsock_loopback
- added a new patch for blocking local mode for guests, added test case
  to check
- add net ref tracking to vsock_loopback patch
- Link to v8: https://lore.kernel.org/r/20251023-vsock-vmtest-v8-0-dea984d02bb0@meta.com

Changes in v8:
- Break generic cleanup/refactoring patches into standalone series,
  remove those from this series
- Link to dependency: https://lore.kernel.org/all/20251022-vsock-selftests-fixes-and-improvements-v1-0-edeb179d6463@meta.com/
- Link to v7: https://lore.kernel.org/r/20251021-vsock-vmtest-v7-0-0661b7b6f081@meta.com

Changes in v7:
- fix hv_sock build
- break out vmtest patches into distinct, more well-scoped patches
- change `orig_net_mode` to `net_mode`
- many fixes and style changes in per-patch change sets (see individual
  patches for specific changes)
- optimize `virtio_vsock_skb_cb` layout
- update commit messages with more useful descriptions
- vsock_loopback: use orig_net_mode instead of current net mode
- add tests for edge cases (ns deletion, mode changing, loopback module
  load ordering)
- Link to v6: https://lore.kernel.org/r/20250916-vsock-vmtest-v6-0-064d2eb0c89d@meta.com

Changes in v6:
- define behavior when mode changes to local while socket/VM is alive
- af_vsock: clarify description of CID behavior
- af_vsock: use stronger langauge around CID rules (dont use "may")
- af_vsock: improve naming of buf/buffer
- af_vsock: improve string length checking on proc writes
- vsock_loopback: add space in struct to clarify lock protection
- vsock_loopback: do proper cleanup/unregister on vsock_loopback_exit()
- vsock_loopback: use virtio_vsock_skb_net() instead of sock_net()
- vsock_loopback: set loopback to NULL after kfree()
- vsock_loopback: use pernet_operations and remove callback mechanism
- vsock_loopback: add macros for "global" and "local"
- vsock_loopback: fix length checking
- vmtest.sh: check for namespace support in vmtest.sh
- Link to v5: https://lore.kernel.org/r/20250827-vsock-vmtest-v5-0-0ba580bede5b@meta.com

Changes in v5:
- /proc/net/vsock_ns_mode -> /proc/sys/net/vsock/ns_mode
- vsock_global_net -> vsock_global_dummy_net
- fix netns lookup in vhost_vsock to respect pid namespaces
- add callbacks for vsock_loopback to avoid circular dependency
- vmtest.sh loads vsock_loopback module
- remove vsock_net_mode_can_set()
- change vsock_net_write_mode() to return true/false based on success
- make vsock_net_mode enum instead of u8
- Link to v4: https://lore.kernel.org/r/20250805-vsock-vmtest-v4-0-059ec51ab111@meta.com

Changes in v4:
- removed RFC tag
- implemented loopback support
- renamed new tests to better reflect behavior
- completed suite of tests with permutations of ns modes and vsock_test
  as guest/host
- simplified socat bridging with unix socket instead of tcp + veth
- only use vsock_test for success case, socat for failure case (context
  in commit message)
- lots of cleanup

Changes in v3:
- add notion of "modes"
- add procfs /proc/net/vsock_ns_mode
- local and global modes only
- no /dev/vhost-vsock-netns
- vmtest.sh already merged, so new patch just adds new tests for NS
- Link to v2:
  https://lore.kernel.org/kvm/20250312-vsock-netns-v2-0-84bffa1aa97a@gmail.com

Changes in v2:
- only support vhost-vsock namespaces
- all g2h namespaces retain old behavior, only common API changes
  impacted by vhost-vsock changes
- add /dev/vhost-vsock-netns for "opt-in"
- leave /dev/vhost-vsock to old behavior
- removed netns module param
- Link to v1:
  https://lore.kernel.org/r/20200116172428.311437-1-sgarzare@redhat.com

Changes in v1:
- added 'netns' module param to vsock.ko to enable the
  network namespace support (disabled by default)
- added 'vsock_net_eq()' to check the "net" assigned to a socket
  only when 'netns' support is enabled
- Link to RFC: https://patchwork.ozlabs.org/cover/1202235/

---
Bobby Eshleman (13):
      vsock: add per-net vsock NS mode state
      vsock: add netns to vsock core
      virtio: set skb owner of virtio_transport_reset_no_sock() reply
      vsock: add netns support to virtio transports
      selftests/vsock: increase timeout to 1200
      selftests/vsock: add namespace helpers to vmtest.sh
      selftests/vsock: prepare vm management helpers for namespaces
      selftests/vsock: add vm_dmesg_{warn,oops}_count() helpers
      selftests/vsock: use ss to wait for listeners instead of /proc/net
      selftests/vsock: add tests for proc sys vsock ns_mode
      selftests/vsock: add namespace tests for CID collisions
      selftests/vsock: add tests for host <-> vm connectivity with namespaces
      selftests/vsock: add tests for namespace deletion

 MAINTAINERS                             |    1 +
 drivers/vhost/vsock.c                   |   44 +-
 include/linux/virtio_vsock.h            |    9 +-
 include/net/af_vsock.h                  |   53 +-
 include/net/net_namespace.h             |    4 +
 include/net/netns/vsock.h               |   17 +
 net/vmw_vsock/af_vsock.c                |  296 ++++++++-
 net/vmw_vsock/hyperv_transport.c        |    7 +-
 net/vmw_vsock/virtio_transport.c        |   22 +-
 net/vmw_vsock/virtio_transport_common.c |   62 +-
 net/vmw_vsock/vmci_transport.c          |   26 +-
 net/vmw_vsock/vsock_loopback.c          |   22 +-
 tools/testing/selftests/vsock/settings  |    2 +-
 tools/testing/selftests/vsock/vmtest.sh | 1055 +++++++++++++++++++++++++++++--
 14 files changed, 1487 insertions(+), 133 deletions(-)

----------------------------------------------------------------------

New:  x86/svm: Add missing svm intercepts
[kvm-unit-tests PATCH v3 1/2] x86/svm: Add missing svm intercepts
Author: Kevin Cheng <chengkev@google.com>

Some intercepts are missing from the KUT svm testing. Add all missing
intercepts and reorganize the svm intercept definition/setting/clearing.

Signed-off-by: Kevin Cheng <chengkev@google.com>
Reviewed-by: Yosry Ahmed <yosry.ahmed@linux.dev>
---
 x86/svm.c       |  18 +++++--
 x86/svm.h       |  84 ++++++++++++++++++++++++++----
 x86/svm_tests.c | 136 ++++++++++++++++++++++++------------------------
 3 files changed, 159 insertions(+), 79 deletions(-)

----------------------------------------------------------------------

New:  x86/svm: Add testing for L1 intercept bug
[kvm-unit-tests PATCH v3 0/2] x86/svm: Add testing for L1 intercept bug
Author: Kevin Cheng <chengkev@google.com>

If a feature is not advertised to L1, L1 intercepts for instructions
controlled by this feature should be ignored. Currently, the added test
fails due to a bug in nested vm exit handling where vmcb12 intercepts
are checked before vmcb02 intercepts, causing the #UD exception to never
be injected into L2 if the L1 intercept is set. This is fixed in [0]

The first patch just adds the missing intercepts needed for testing and
restructures the vmcb_control_area struct to make adding the missing
intercepts less ugly. The second patch adds the test which disables all
relevant features that have available instruction intercepts, and checks
that the #UD exception is correctly delivered despite the L1 intercept
being set.

[0] https://lore.kernel.org/all/20251205070630.4013452-1-chengkev@google.com/

v2 -> v3:
  - Added assertions to unsupported instructions test to verify that CPU
    does not support features enabling tested instructions [Yosry Ahmed]
  - Added some comments [Yosry Ahmed]
  - Changed insn_invpcid() into assembly [Yosry Ahmed]
  - Added a check to the unsupported instruction test to ensure that
    L1's vmcb is not modified by KVM [Yosry Ahmed]
  - Redacted reviewed-by tag for 2nd patch in series. I mistakingly
    added that

v2: https://lore.kernel.org/all/20251215210026.2422155-1-chengkev@google.com/

v1 -> v2:
  - Added save/restore helpers for all intercepts as suggested by Yosry
  - Reuse invpcid_safe() for added test as suggested by Yosry
  - Include '-skinit' in unittests.cfg for added test target as pointed
    out by Yosry

v1: https://lore.kernel.org/all/20251205081448.4062096-1-chengkev@google.com/

Kevin Cheng (2):
  x86/svm: Add missing svm intercepts
  x86/svm: Add unsupported instruction intercept test

 lib/x86/processor.h |   1 +
 x86/svm.c           |  18 +++-
 x86/svm.h           |  89 ++++++++++++++++--
 x86/svm_tests.c     | 224 +++++++++++++++++++++++++++++++-------------
 x86/unittests.cfg   |  11 ++-
 5 files changed, 262 insertions(+), 81 deletions(-)

----------------------------------------------------------------------

New:  vsock: add per-net vsock NS mode
[PATCH RFC net-next v13 01/13] vsock: add per-net vsock NS mode
Author: Bobby Eshleman <bobbyeshleman@gmail.com>


Add the per-net vsock NS mode state. This only adds the structure for
holding the mode and some of the functions for setting/getting and
checking the mode, but does not integrate the functionality yet.

Future patches add the uAPI and transport-specific usage of these
structures and helpers.

Signed-off-by: Bobby Eshleman <bobbyeshleman@meta.com>
---
Changes in v13:
- remove net_mode because net->vsock.mode becomes immutable, no need to
  save the mode when vsocks are created.
- add the new helpers for child_ns_mode to support ns_mode inheriting
  the mode from child_ns_mode.
- because ns_mode is immutable and child_ns_mode can be changed multiple
  times, remove the write-once lock.
- simplify vsock_net_check_mode() to no longer take mode arguments since
  the mode can be accessed via the net pointers without fear of the mode
  changing.
- add logic in vsock_net_check_mode() to infer VSOCK_NET_MODE_GLOBAL
  from NULL namespaces in order to allow only net pointers to be passed
  to vsock_net_check_mode(), while still allowing namespace-unaware
  transports to force global mode.

Changes in v10:
- change mode_locked to int (Stefano)

Changes in v9:
- use xchg(), WRITE_ONCE(), READ_ONCE() for mode and mode_locked (Stefano)
- clarify mode0/mode1 meaning in vsock_net_check_mode() comment
- remove spin lock in net->vsock (not used anymore)
- change mode from u8 to enum vsock_net_mode in vsock_net_write_mode()

Changes in v7:
- clarify vsock_net_check_mode() comments
- change to `orig_net_mode == VSOCK_NET_MODE_GLOBAL && orig_net_mode == vsk->orig_net_mode`
- remove extraneous explanation of `orig_net_mode`
- rename `written` to `mode_locked`
- rename `vsock_hdr` to `sysctl_hdr`
- change `orig_net_mode` to `net_mode`
- make vsock_net_check_mode() more generic by taking just net pointers
  and modes, instead of a vsock_sock ptr, for reuse by transports
  (e.g., vhost_vsock)

Changes in v6:
- add orig_net_mode to store mode at creation time which will be used to
  avoid breakage when namespace changes mode during socket/VM lifespan

Changes in v5:
- use /proc/sys/net/vsock/ns_mode instead of /proc/net/vsock_ns_mode
- change from net->vsock.ns_mode to net->vsock.mode
- change vsock_net_set_mode() to vsock_net_write_mode()
- vsock_net_write_mode() returns bool for write success to avoid
  need to use vsock_net_mode_can_set()
- remove vsock_net_mode_can_set()
---
 MAINTAINERS                 |  1 +
 include/net/af_vsock.h      | 42 ++++++++++++++++++++++++++++++++++++++++++
 include/net/net_namespace.h |  4 ++++
 include/net/netns/vsock.h   | 17 +++++++++++++++++
 4 files changed, 64 insertions(+)

----------------------------------------------------------------------

New:  x86, fpu/kvm: fix crash with AMX
[PATCH 0/5] x86, fpu/kvm: fix crash with AMX
Author: Paolo Bonzini <pbonzini@redhat.com>

Fix a possible host panic, due to an unexpected #NM, when a KVM guest
is using AMX features.

The guest's XFD value, which is stored in fpstate->xfd, is used for both
guest execution and host XSAVE operations.  However, the guest-configured
XFD setting can disable features that the host needs enabled to successfully
XRSTOR the guest FPU state.

The first patch replaces inline code in vcpu_enter_guest() with a new
function exported by kernel/fpu.  The new function is similar to
fpregs_lock_and_load() but operates with preemption disabled and
also restores the extra state (currently xfd_err) in struct guest_fpu.

The second patch then introduces a new xfd field in struct guest_fpu,
so that the guest's XFD setting can be swapped while leaving the host
value untouched in fpstate->xfd.

Patches 3 and 4 introduce a test.

Patch 5 makes KVM use fpregs_lock_and_load(), exporting it in place of
two lower-level functions whose other uses are now gone.

Reviews and acks are welcome (this could go in through either
the x86 or KVM trees).

Paolo


Paolo Bonzini (5):
  x86, fpu: introduce fpu_load_guest_fpstate()
  x86, fpu: separate fpstate->xfd and guest XFD
  selftests: kvm: renumber some sync points in amx_test
  selftests, kvm: try getting XFD and XSAVE state out of sync
  KVM: x86: kvm_fpu_get() is fpregs_lock_and_load()

 arch/x86/include/asm/fpu/api.h             |  7 ++--
 arch/x86/include/asm/fpu/types.h           |  7 ++++
 arch/x86/kernel/fpu/core.c                 | 38 ++++++++++-------
 arch/x86/kernel/fpu/xstate.h               | 18 ++++----
 arch/x86/kvm/fpu.h                         |  6 +--
 arch/x86/kvm/x86.c                         | 14 ++-----
 tools/testing/selftests/kvm/x86/amx_test.c | 49 +++++++++++++++-------
 7 files changed, 82 insertions(+), 57 deletions(-)

----------------------------------------------------------------------

New:  x86, fpu: introduce fpu_load_guest_fpstate()
[PATCH 1/5] x86, fpu: introduce fpu_load_guest_fpstate()
Author: Paolo Bonzini <pbonzini@redhat.com>

Create a variant of fpregs_lock_and_load() that KVM can use in its
vCPU entry code after preemption has been disabled.  While basing
it on the existing logic in vcpu_enter_guest(), ensure that
fpregs_assert_state_consistent() always runs and sprinkle a few
more assertions.

Cc: stable@vger.kernel.org
Fixes: 820a6ee944e7 ("kvm: x86: Add emulation for IA32_XFD", 2022-01-14)
Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
---
 arch/x86/include/asm/fpu/api.h |  1 +
 arch/x86/kernel/fpu/core.c     | 17 +++++++++++++++++
 arch/x86/kvm/x86.c             |  8 +-------
 3 files changed, 19 insertions(+), 7 deletions(-)

----------------------------------------------------------------------

New:  vfio: Improve DMA mapping performance for huge pages
[RFC PATCH 1/2] vfio: Improve DMA mapping performance for huge pages
Author: Aaron Lewis <aaronlewis@google.com>

Huge pages are pinned at 4K granularity.  That means when pinning a 1G
page, 2^18 pages are pinned one at a time.  This adds needless toil
which results in high latencies.

To improve this, increase the number of pages the batch is operating on
to the number of 4k pages in a huge page.  Doing that allows huge
pages to be pinned in larger chunks, reducing the number of individual
pages being pinned one at a time.

This results in a major speed up vs baseline and can be demonstrated
by the selftest "vfio_dma_mapping_perf_test".

Using this selftest and a sample profiler it is easy to see where all
the time is being spent, vfio_pin_pages_remote().  Optimizing for that
led to the gains called out below.

 Samples     Percentage  Name
 ---------------------------------------------
 30,947,392      0.209%  vfs_ioctl (ioctl.c) Inlined
 30,947,392      0.209%  vfio_fops_unl_ioctl (container.c)
 30,947,392      0.209%  vfio_iommu_type1_ioctl (vfio_iommu_type1.c)
 30,947,392      0.209%  vfio_iommu_type1_map_dma (vfio_iommu_type1.c) Inlined
 30,947,392      0.209%  vfio_dma_do_map (vfio_iommu_type1.c) Inlined
 30,947,392      0.209%  vfio_pin_map_dma (vfio_iommu_type1.c)
 30,947,392      0.209%  vfio_pin_pages_remote (vfio_iommu_type1.c)
 29,616,402      0.200%  vaddr_get_pfns (vfio_iommu_type1.c)
 29,616,402      0.200%  internal_get_user_pages_fast (gup.c)
 29,616,402      0.200%  __gup_longterm_locked (gup.c)
 28,962,361      0.195%  __get_user_pages_locked (gup.c) Inlined
 28,962,361      0.195%  __get_user_pages (gup.c)
 25,548,889      0.172%  follow_page_mask (gup.c)
 24,852,062      0.168%  follow_p4d_mask (gup.c) Inlined
 24,852,062      0.168%  follow_pud_mask (gup.c) Inlined
 17,683,506      0.119%  follow_devmap_pud (huge_memory.c)
 6,584,772       0.044%  pud_lock (mm.h) Inlined


 Results when mapping 8G of memory:
 ----------------------------------

Baseline
 - vfio_type1_iommu:      2.87ms
 - iommufd_compat_type1: 53.23ms
 - iommufd:               4.53ms

With fast huge page pinning
 - vfio_type1_iommu:      0.01ms
 - improvements to iommufd_compat_type1 and iommufd are tbd.


 Results when mapping 256G of memory:
 ----------------------------------

Baseline
 - vfio_type1_iommu:       99.36ms
 - iommufd_compat_type1: 1576.51ms
 - iommufd:               144.65ms

With fast huge page pinning:
 - vfio_type1_iommu:        0.20ms
 - improvements to iommufd_compat_type1 and iommufd are tbd.

Based on these results that is more than a 300x speed up for
vfio_type1_iommu!  E.g. 2.87ms -> 0.01ms and 99.36ms -> 0.20ms.

As of now there is only a proposal to speed up "vfio_type1_iommu".


 IOMMUFD:
 --------

More effort will be needed to see what kind of speed ups can be achieved
by optimizing iommufd.  Sample profiler results (below) show that it
isn't the GUP calls that are slowing it down like they were in the
"vfio_type1_iommu" case.  The majority of the slowdown is coming from
batch_from_pages(), and the majority of that time is being spent in
batch_add_pfn_num().

 Samples     Percentage  Name
 ---------------------------------------------
 2,710,320,547   10.22%  iommufd_fops_ioctl (main.c)
 2,710,320,547   10.22%  iommufd_ioas_map (ioas.c)
 2,710,320,547   10.22%  iopt_map_user_pages (io_pagetable.c)
 2,710,320,547   10.22%  iopt_map_common (io_pagetable.c)
 2,710,320,547   10.22%  iopt_map_pages (io_pagetable.c)
 2,710,320,547   10.22%  iopt_fill_domains_pages (io_pagetable.c) Inlined
 2,710,320,547   10.22%  iopt_area_fill_domains (pages.c)
 2,710,320,547   10.22%  pfn_reader_first (pages.c)
 2,710,320,547   10.22%  pfn_reader_next (pages.c)
 2,709,376,056   10.21%  pfn_reader_fill_span (pages.c) Inlined
   2,435,947,359   9.182%  batch_from_pages (pages.c) Inlined
     1,864,604,611  7.028%   batch_add_pfn (pages.c) Inlined
     1,864,604,611  7.028%   batch_add_pfn_num (pages.c) Inlined
   273,428,697     1.031%  pfn_reader_user_pin (pages.c)
     271,538,567     1.024%  gup_fast_fallback (gup.c)

Signed-off-by: Aaron Lewis <aaronlewis@google.com>
---
 drivers/vfio/vfio_iommu_type1.c | 37 ++++++++++++++++++++++++++-------
 1 file changed, 29 insertions(+), 8 deletions(-)

----------------------------------------------------------------------

Exist: [RFC PATCH 1/2] vfio: Improve DMA mapping performance for huge pages
 Skip: [RFC PATCH 0/2] vfio: Improve DMA mapping performance for huge pages
