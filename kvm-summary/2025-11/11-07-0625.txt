From 17982ee4c to cdf50852b
KVM mailing list update from 17982ee4c to cdf50852b

Top 15 contributor Email domains (Based on Email Body)

     25 linux.ibm.com
      9 nvidia.com
      9 arm.com
      3 google.com
      2 intel.com
      2 gmail.com
      1 sony.com
      1 linux.intel.com
      1 kernel.org

Top 15 contributors (Based on Email Body)

     25  Claudio Imbrenda <imbrenda@linux.ibm.com>
      9  Yeoreum Yun <yeoreum.yun@arm.com>
      7  Leon Romanovsky <leonro@nvidia.com>
      3  Sean Christopherson <seanjc@google.com>
      2  Vivek Kasireddy <vivek.kasireddy@intel.com>
      2  Uros Bizjak <ubizjak@gmail.com>
      2  Jason Gunthorpe <jgg@nvidia.com>
      1  Sukrit Bhatnagar <Sukrit.Bhatnagar@sony.com>
      1  Leon Romanovsky <leon@kernel.org>
      1  Binbin Wu <binbin.wu@linux.intel.com>

===== Patch list in this time period =====


===== Patch Commit Messages ====

New:  KVM: x86: Use "checked" versions of get_user() and put_user()
[PATCH] KVM: x86: Use "checked" versions of get_user() and put_user()
Author: Sean Christopherson <seanjc@google.com>

Use the normal, checked versions for get_user() and put_user() instead of
the double-underscore versions that omit range checks, as the checked
versions are actually measurably faster on modern CPUs (12%+ on Intel,
25%+ on AMD).

The performance hit on the unchecked versions is almost entirely due to
the added LFENCE on CPUs where LFENCE is serializing (which is effectively
all modern CPUs), which was added by commit 304ec1b05031 ("x86/uaccess:
Use __uaccess_begin_nospec() and uaccess_try_nospec").  The small
optimizations done by commit b19b74bc99b1 ("x86/mm: Rework address range
check in get_user() and put_user()") likely shave a few cycles off, but
the bulk of the extra latency comes from the LFENCE.

Don't bother trying to open-code an equivalent for performance reasons, as
the loss of inlining (e.g. see commit ea6f043fc984 ("x86: Make __get_user()
generate an out-of-line call") is largely a non-factor (ignoring setups
where RET is something entirely different),

As measured across tens of millions of calls of guest PTE reads in
FNAME(walk_addr_generic):

              __get_user()  get_user()  open-coded  open-coded, no LFENCE
Intel (EMR)           75.1        67.6        75.3                   65.5
AMD (Turin)           68.1        51.1        67.5                   49.3

Reported-by: Linus Torvalds <torvalds@linux-foundation.org>
Closes: https://lore.kernel.org/all/CAHk-=wimh_3jM9Xe8Zx0rpuf8CPDu6DkRCGb44azk0Sz5yqSnw@mail.gmail.com
Cc: Borislav Petkov <bp@alien8.de>
Signed-off-by: Sean Christopherson <seanjc@google.com>
---
 arch/x86/kvm/hyperv.c          | 2 +-
 arch/x86/kvm/mmu/paging_tmpl.h | 2 +-
 2 files changed, 2 insertions(+), 2 deletions(-)

----------------------------------------------------------------------

New:  KVM: VMX: Make loaded_vmcs_clear() static in vmx.c
[PATCH] KVM: VMX: Make loaded_vmcs_clear() static in vmx.c
Author: Sean Christopherson <seanjc@google.com>

Make loaded_vmcs_clear() local to vmx.c as there are no longer any
external callers.

No functional change intended.

Signed-off-by: Sean Christopherson <seanjc@google.com>
---
 arch/x86/kvm/vmx/vmx.c | 2 +-
 arch/x86/kvm/vmx/vmx.h | 1 -
 2 files changed, 1 insertion(+), 2 deletions(-)

----------------------------------------------------------------------

New:  KVM: x86: Enforce use of EXPORT_SYMBOL_FOR_KVM_INTERNAL
[PATCH] KVM: x86: Enforce use of EXPORT_SYMBOL_FOR_KVM_INTERNAL
Author: Sean Christopherson <seanjc@google.com>

Add a (gnarly) inline "script" in the Makefile to fail the build if there
is EXPORT_SYMBOL_GPL or EXPORT_SYMBOL usage in virt/kvm or arch/x86/kvm
beyond the known-good/expected exports for other modules.  Remembering to
use EXPORT_SYMBOL_FOR_KVM_INTERNAL is surprisingly difficult, and hoping
to detect "bad" exports via code review is not a robust long-term strategy.

Jump through a pile of hoops to coerce make into printing a human-friendly
error message, with the offending files+lines cleanly separated.

E.g. where <srctree> is the resolution of $(srctree), i.e. '.' for in-tree
builds, and the absolute path for out-of-tree-builds:

  <srctree>/arch/x86/kvm/Makefile:97: *** ERROR ***
  found 2 unwanted occurrences of EXPORT_SYMBOL_GPL:
    <srctree>/arch/x86/kvm/x86.c:686:EXPORT_SYMBOL_GPL(__kvm_set_user_return_msr);
    <srctree>/arch/x86/kvm/x86.c:703:EXPORT_SYMBOL_GPL(kvm_set_user_return_msr);
  in directories:
    <srctree>/arch/x86/kvm
    <srctree>/virt/kvm
  Use EXPORT_SYMBOL_FOR_KVM_INTERNAL, not EXPORT_SYMBOL_GPL.  Stop.

and

  <srctree>/arch/x86/kvm/Makefile:98: *** ERROR ***
  found 1 unwanted occurrences of EXPORT_SYMBOL:
    <srctree>/arch/x86/kvm/x86.c:709:EXPORT_SYMBOL(kvm_get_user_return_msr);
  in directories:
    <srctree>/arch/x86/kvm
    <srctree>/virt/kvm
  Use EXPORT_SYMBOL_FOR_KVM_INTERNAL, not EXPORT_SYMBOL.  Stop.

Put the enforcement in x86's Makefile even though the rule itself applies
to virt/kvm, as putting the enforcement in virt/kvm/Makefile.kvm would
effectively require exempting every architecture except x86.  PPC is the
only other architecture with sub-modules, and PPC hasn't been switched to
use EXPORT_SYMBOL_FOR_KVM_INTERNAL (and given its nearly-orphaned state,
likely never will).  And for KVM architectures without sub-modules, that
means that, barring truly spurious exports, the exports are intended for
non-KVM usage and thus shouldn't be using EXPORT_SYMBOL_FOR_KVM_INTERNAL.

Cc: Chao Gao <chao.gao@intel.com>
Signed-off-by: Sean Christopherson <seanjc@google.com>
---
 arch/x86/kvm/Makefile | 56 +++++++++++++++++++++++++++++++++++++++++++
 1 file changed, 56 insertions(+)

----------------------------------------------------------------------

New:  KVM: s390: Refactor pgste lock and unlock functions
[PATCH v3 01/23] KVM: s390: Refactor pgste lock and unlock functions
Author: Claudio Imbrenda <imbrenda@linux.ibm.com>

Move the pgste lock and unlock functions back into mm/pgtable.c and
duplicate them in mm/gmap_helpers.c to avoid function name collisions
later on.

Signed-off-by: Claudio Imbrenda <imbrenda@linux.ibm.com>
---
 arch/s390/include/asm/pgtable.h | 22 ----------------------
 arch/s390/mm/gmap_helpers.c     | 23 ++++++++++++++++++++++-
 arch/s390/mm/pgtable.c          | 23 ++++++++++++++++++++++-
 3 files changed, 44 insertions(+), 24 deletions(-)

----------------------------------------------------------------------

New:  KVM: s390: gmap rewrite, the real deal
[PATCH v3 00/23] KVM: s390: gmap rewrite, the real deal
Author: Claudio Imbrenda <imbrenda@linux.ibm.com>

This series is the last big series of the gmap rewrite. It introduces
the new code and actually uses it. The old code is then removed.

The insertions/deletions balance is negative both for this series, and
for the whole rewrite, also considering all the preparatory patches.

KVM on s390 will now use the mmu_notifier, like most other
architectures. The gmap address space is now completely separate from
userspace; no level of the page tables is shared between guest mapping
and userspace.

One of the biggest advantages is that the page size of userspace is
completely independent of the page size used by the guest. Userspace
can mix normal pages, THPs, hugetlbfs, and more.

Patches 1 to 6 are mostly preparations; introducing some new bits and
functions, and moving code around.

Patch 7 to 16 is the meat of the new gmap code; page table management
functions and gmap management. This is the code that will be used to
manage guest memory.

Patch 18 is unfortunately big; the existing code is converted to use
the new gmap and all references to the old gmap are removed. This needs
to be done all at once, unfortunately, hence the size of the patch.

Patch 19 and 20 remove all the now unused code.

Patch 21 and 22 allow for 1M pages to be used to back guests, and add
some more functions that are useful for testing.

Patch 23 fixes storage key manipulation functions, which would
otherwise be broken by the new code.


v2->v3:
* Add lots of small comments and cosmetic fixes
* Rename some functions to improve clarity
* Remove unused helper functions and macros
* Rename inline asm constraints labels to make them more understandable
* Refactor the code to pre-allocate the page tables (using custom
  caches) when sleeping is allowed, use the cached pages when holding
  spinlocks and handle gracefully allocation failures (i.e. retry
  instead of killing the guest)
* Refactor the code for fault handling; it's now in a separate file,
  and it takes a callback that can be optionally called when all the
  relevant locks are still held
* Use assembler mnemonics instead of manually specifying the opcode
  where appropriate
* Remove the LEVEL_* enum, and use TABLE_TYPE_* macros instead;
  introduce new TABLE_TYPE_PAGE_TABLE
* Remove usage of cpu_has_idte() since it is being removed from the
  kernel
* Improve storage key handling and PGSTE locking
* Introduce struct guest_fault to represent the state of a guest fault
  that is being resolved
* Minor CMMA fixes


Claudio Imbrenda (23):
  KVM: s390: Refactor pgste lock and unlock functions
  KVM: s390: add P bit in table entry bitfields, move union vaddress
  s390: Move sske_frame() to a header
  KVM: s390: Add gmap_helper_set_unused()
  KVM: s390: Enable KVM_GENERIC_MMU_NOTIFIER
  KVM: s390: Rename some functions in gaccess.c
  KVM: s390: KVM-specific bitfields and helper functions
  KVM: s390: KVM page table management functions: allocation
  KVM: s390: KVM page table management functions: clear and replace
  KVM: s390: KVM page table management functions: walks
  KVM: s390: KVM page table management functions: storage keys
  KVM: s390: KVM page table management functions: lifecycle management
  KVM: s390: KVM page table management functions: CMMA
  KVM: s390: New gmap code
  KVM: s390: Add helper functions for fault handling
  KVM: s390: Add some helper functions needed for vSIE
  KVM: s390: Stop using CONFIG_PGSTE
  KVM: s390: Switch to new gmap
  KVM: s390: Remove gmap from s390/mm
  KVM: S390: Remove PGSTE code from linux/s390 mm
  KVM: s390: Enable 1M pages for gmap
  KVM: s390: Storage key manipulation IOCTL
  KVM: s390: Fix storage key memop IOCTLs

 MAINTAINERS                          |    2 -
 arch/s390/Kconfig                    |    3 -
 arch/s390/include/asm/dat-bits.h     |   32 +-
 arch/s390/include/asm/gmap.h         |  174 --
 arch/s390/include/asm/gmap_helpers.h |    1 +
 arch/s390/include/asm/kvm_host.h     |    5 +
 arch/s390/include/asm/mmu.h          |   13 -
 arch/s390/include/asm/mmu_context.h  |    6 +-
 arch/s390/include/asm/page.h         |    4 -
 arch/s390/include/asm/pgalloc.h      |    4 -
 arch/s390/include/asm/pgtable.h      |  163 +-
 arch/s390/include/asm/tlb.h          |    3 -
 arch/s390/include/asm/uaccess.h      |   70 +-
 arch/s390/kvm/Kconfig                |    3 +-
 arch/s390/kvm/Makefile               |    3 +-
 arch/s390/kvm/dat.c                  | 1362 ++++++++++++++
 arch/s390/kvm/dat.h                  |  971 ++++++++++
 arch/s390/kvm/diag.c                 |    2 +-
 arch/s390/kvm/faultin.c              |  148 ++
 arch/s390/kvm/faultin.h              |   92 +
 arch/s390/kvm/gaccess.c              |  929 +++++-----
 arch/s390/kvm/gaccess.h              |   20 +-
 arch/s390/kvm/gmap-vsie.c            |  141 --
 arch/s390/kvm/gmap.c                 | 1125 ++++++++++++
 arch/s390/kvm/gmap.h                 |  159 ++
 arch/s390/kvm/intercept.c            |   15 +-
 arch/s390/kvm/interrupt.c            |    2 +-
 arch/s390/kvm/kvm-s390.c             |  926 ++++------
 arch/s390/kvm/kvm-s390.h             |   28 +-
 arch/s390/kvm/priv.c                 |  207 +--
 arch/s390/kvm/pv.c                   |   67 +-
 arch/s390/kvm/vsie.c                 |  117 +-
 arch/s390/lib/uaccess.c              |  184 +-
 arch/s390/mm/Makefile                |    1 -
 arch/s390/mm/fault.c                 |    4 +-
 arch/s390/mm/gmap.c                  | 2453 --------------------------
 arch/s390/mm/gmap_helpers.c          |   87 +-
 arch/s390/mm/hugetlbpage.c           |   24 -
 arch/s390/mm/page-states.c           |    1 +
 arch/s390/mm/pageattr.c              |    7 -
 arch/s390/mm/pgalloc.c               |   24 -
 arch/s390/mm/pgtable.c               |  818 +--------
 include/uapi/linux/kvm.h             |   10 +
 mm/khugepaged.c                      |    9 -
 44 files changed, 5116 insertions(+), 5303 deletions(-)

----------------------------------------------------------------------

New:  KVM: s390: Fix gmap_helper_zap_one_page() again
[PATCH v1 1/1] KVM: s390: Fix gmap_helper_zap_one_page() again
Author: Claudio Imbrenda <imbrenda@linux.ibm.com>

A few checks were missing in gmap_helper_zap_one_page(), which can lead
to memory corruption in the guest under specific circumstances.

Add the missing checks.

Fixes: 5deafa27d9ae ("KVM: s390: Fix to clear PTE when discarding a swapped page")
Reported-by: Marc Hartmayer <mhartmay@linux.ibm.com>
Signed-off-by: Claudio Imbrenda <imbrenda@linux.ibm.com>
---
 arch/s390/mm/gmap_helpers.c | 9 +++++++--
 1 file changed, 7 insertions(+), 2 deletions(-)

----------------------------------------------------------------------

New:  PCI/P2PDMA: Separate the mmap() support from the core logic
[PATCH v7 01/11] PCI/P2PDMA: Separate the mmap() support from the core logic
Author: Leon Romanovsky <leon@kernel.org>


Currently the P2PDMA code requires a pgmap and a struct page to
function. The was serving three important purposes:

 - DMA API compatibility, where scatterlist required a struct page as
   input

 - Life cycle management, the percpu_ref is used to prevent UAF during
   device hot unplug

 - A way to get the P2P provider data through the pci_p2pdma_pagemap

The DMA API now has a new flow, and has gained phys_addr_t support, so
it no longer needs struct pages to perform P2P mapping.

Lifecycle management can be delegated to the user, DMABUF for instance
has a suitable invalidation protocol that does not require struct page.

Finding the P2P provider data can also be managed by the caller
without need to look it up from the phys_addr.

Split the P2PDMA code into two layers. The optional upper layer,
effectively, provides a way to mmap() P2P memory into a VMA by
providing struct page, pgmap, a genalloc and sysfs.

The lower layer provides the actual P2P infrastructure and is wrapped
up in a new struct p2pdma_provider. Rework the mmap layer to use new
p2pdma_provider based APIs.

Drivers that do not want to put P2P memory into VMA's can allocate a
struct p2pdma_provider after probe() starts and free it before
remove() completes. When DMA mapping the driver must convey the struct
p2pdma_provider to the DMA mapping code along with a phys_addr of the
MMIO BAR slice to map. The driver must ensure that no DMA mapping
outlives the lifetime of the struct p2pdma_provider.

The intended target of this new API layer is DMABUF. There is usually
only a single p2pdma_provider for a DMABUF exporter. Most drivers can
establish the p2pdma_provider during probe, access the single instance
during DMABUF attach and use that to drive the DMA mapping.

DMABUF provides an invalidation mechanism that can guarantee all DMA
is halted and the DMA mappings are undone prior to destroying the
struct p2pdma_provider. This ensures there is no UAF through DMABUFs
that are lingering past driver removal.

The new p2pdma_provider layer cannot be used to create P2P memory that
can be mapped into VMA's, be used with pin_user_pages(), O_DIRECT, and
so on. These use cases must still use the mmap() layer. The
p2pdma_provider layer is principally for DMABUF-like use cases where
DMABUF natively manages the life cycle and access instead of
vmas/pin_user_pages()/struct page.

In addition, remove the bus_off field from pci_p2pdma_map_state since
it duplicates information already available in the pgmap structure.
The bus_offset is only used in one location (pci_p2pdma_bus_addr_map)
and is always identical to pgmap->bus_offset.

Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
Tested-by: Alex Mastro <amastro@fb.com>
Tested-by: Nicolin Chen <nicolinc@nvidia.com>
Signed-off-by: Leon Romanovsky <leonro@nvidia.com>
---
 drivers/pci/p2pdma.c       | 43 +++++++++++++++++++++++--------------------
 include/linux/pci-p2pdma.h | 19 ++++++++++++++-----
 2 files changed, 37 insertions(+), 25 deletions(-)

----------------------------------------------------------------------

New:  vfio/pci: Allow MMIO regions to be exported through dma-buf
[PATCH v7 00/11] vfio/pci: Allow MMIO regions to be exported through dma-buf
Author: Leon Romanovsky <leon@kernel.org>

Changelog:=0D
v7:=0D
 * Dropped restore_revoke flag and added vfio_pci_dma_buf_move=0D
   to reverse loop.=0D
 * Fixed spelling errors in documentation patch.=0D
 * Rebased on top of v6.18-rc3.=0D
 * Added include to stddef.h to vfio.h, to keep uapi header file independen=
t.=0D
v6: https://patch.msgid.link/20251102-dmabuf-vfio-v6-0-d773cff0db9f@nvidia.=
com=0D
 * Fixed wrong error check from pcim_p2pdma_init().=0D
 * Documented pcim_p2pdma_provider() function.=0D
 * Improved commit messages.=0D
 * Added VFIO DMA-BUF selftest, not sent yet.=0D
 * Added __counted_by(nr_ranges) annotation to struct vfio_device_feature_d=
ma_buf.=0D
 * Fixed error unwind when dma_buf_fd() fails.=0D
 * Document latest changes to p2pmem.=0D
 * Removed EXPORT_SYMBOL_GPL from pci_p2pdma_map_type.=0D
 * Moved DMA mapping logic to DMA-BUF.=0D
 * Removed types patch to avoid dependencies between subsystems.=0D
 * Moved vfio_pci_dma_buf_move() in err_undo block.=0D
 * Added nvgrace patch.=0D
v5: https://lore.kernel.org/all/cover.1760368250.git.leon@kernel.org=0D
 * Rebased on top of v6.18-rc1.=0D
 * Added more validation logic to make sure that DMA-BUF length doesn't=0D
   overflow in various scenarios.=0D
 * Hide kernel config from the users.=0D
 * Fixed type conversion issue. DMA ranges are exposed with u64 length,=0D
   but DMA-BUF uses "unsigned int" as a length for SG entries.=0D
 * Added check to prevent from VFIO drivers which reports BAR size=0D
   different from PCI, do not use DMA-BUF functionality.=0D
v4: https://lore.kernel.org/all/cover.1759070796.git.leon@kernel.org=0D
 * Split pcim_p2pdma_provider() to two functions, one that initializes=0D
   array of providers and another to return right provider pointer.=0D
v3: https://lore.kernel.org/all/cover.1758804980.git.leon@kernel.org=0D
 * Changed pcim_p2pdma_enable() to be pcim_p2pdma_provider().=0D
 * Cache provider in vfio_pci_dma_buf struct instead of BAR index.=0D
 * Removed misleading comment from pcim_p2pdma_provider().=0D
 * Moved MMIO check to be in pcim_p2pdma_provider().=0D
v2: https://lore.kernel.org/all/cover.1757589589.git.leon@kernel.org/=0D
 * Added extra patch which adds new CONFIG, so next patches can reuse=0D
 * it.=0D
 * Squashed "PCI/P2PDMA: Remove redundant bus_offset from map state"=0D
   into the other patch.=0D
 * Fixed revoke calls to be aligned with true->false semantics.=0D
 * Extended p2pdma_providers to be per-BAR and not global to whole=0D
 * device.=0D
 * Fixed possible race between dmabuf states and revoke.=0D
 * Moved revoke to PCI BAR zap block.=0D
v1: https://lore.kernel.org/all/cover.1754311439.git.leon@kernel.org=0D
 * Changed commit messages.=0D
 * Reused DMA_ATTR_MMIO attribute.=0D
 * Returned support for multiple DMA ranges per-dMABUF.=0D
v0: https://lore.kernel.org/all/cover.1753274085.git.leonro@nvidia.com=0D
=0D
---------------------------------------------------------------------------=
=0D
Based on "[PATCH v6 00/16] dma-mapping: migrate to physical address-based A=
PI"=0D
https://lore.kernel.org/all/cover.1757423202.git.leonro@nvidia.com/ series.=
=0D
---------------------------------------------------------------------------=
=0D
=0D
This series extends the VFIO PCI subsystem to support exporting MMIO=0D
regions from PCI device BARs as dma-buf objects, enabling safe sharing of=0D
non-struct page memory with controlled lifetime management. This allows RDM=
A=0D
and other subsystems to import dma-buf FDs and build them into memory regio=
ns=0D
for PCI P2P operations.=0D
=0D
The series supports a use case for SPDK where a NVMe device will be=0D
owned by SPDK through VFIO but interacting with a RDMA device. The RDMA=0D
device may directly access the NVMe CMB or directly manipulate the NVMe=0D
device's doorbell using PCI P2P.=0D
=0D
However, as a general mechanism, it can support many other scenarios with=0D
VFIO. This dmabuf approach can be usable by iommufd as well for generic=0D
and safe P2P mappings.=0D
=0D
In addition to the SPDK use-case mentioned above, the capability added=0D
in this patch series can also be useful when a buffer (located in device=0D
memory such as VRAM) needs to be shared between any two dGPU devices or=0D
instances (assuming one of them is bound to VFIO PCI) as long as they=0D
are P2P DMA compatible.=0D
=0D
The implementation provides a revocable attachment mechanism using dma-buf=
=0D
move operations. MMIO regions are normally pinned as BARs don't change=0D
physical addresses, but access is revoked when the VFIO device is closed=0D
or a PCI reset is issued. This ensures kernel self-defense against=0D
potentially hostile userspace.=0D
=0D
The series includes significant refactoring of the PCI P2PDMA subsystem=0D
to separate core P2P functionality from memory allocation features,=0D
making it more modular and suitable for VFIO use cases that don't need=0D
struct page support.=0D
=0D
-----------------------------------------------------------------------=0D
The series is based originally on=0D
https://lore.kernel.org/all/20250307052248.405803-1-vivek.kasireddy@intel.c=
om/=0D
but heavily rewritten to be based on DMA physical API.=0D
-----------------------------------------------------------------------=0D
The WIP branch can be found here:=0D
https://git.kernel.org/pub/scm/linux/kernel/git/leon/linux-rdma.git/log/?h=
=3Ddmabuf-vfio-v7=0D
=0D
Thanks=0D
=0D
---=0D
Jason Gunthorpe (2):=0D
      PCI/P2PDMA: Document DMABUF model=0D
      vfio/nvgrace: Support get_dmabuf_phys=0D
=0D
Leon Romanovsky (7):=0D
      PCI/P2PDMA: Separate the mmap() support from the core logic=0D
      PCI/P2PDMA: Simplify bus address mapping API=0D
      PCI/P2PDMA: Refactor to separate core P2P functionality from memory a=
llocation=0D
      PCI/P2PDMA: Provide an access to pci_p2pdma_map_type() function=0D
      dma-buf: provide phys_vec to scatter-gather mapping routine=0D
      vfio/pci: Enable peer-to-peer DMA transactions by default=0D
      vfio/pci: Add dma-buf export support for MMIO regions=0D
=0D
Vivek Kasireddy (2):=0D
      vfio: Export vfio device get and put registration helpers=0D
      vfio/pci: Share the core device pointer while invoking feature functi=
ons=0D
=0D
 Documentation/driver-api/pci/p2pdma.rst |  95 +++++++---=0D
 block/blk-mq-dma.c                      |   2 +-=0D
 drivers/dma-buf/dma-buf.c               | 235 ++++++++++++++++++++++++=0D
 drivers/iommu/dma-iommu.c               |   4 +-=0D
 drivers/pci/p2pdma.c                    | 182 +++++++++++++-----=0D
 drivers/vfio/pci/Kconfig                |   3 +=0D
 drivers/vfio/pci/Makefile               |   1 +=0D
 drivers/vfio/pci/nvgrace-gpu/main.c     |  56 ++++++=0D
 drivers/vfio/pci/vfio_pci.c             |   5 +=0D
 drivers/vfio/pci/vfio_pci_config.c      |  22 ++-=0D
 drivers/vfio/pci/vfio_pci_core.c        |  53 ++++--=0D
 drivers/vfio/pci/vfio_pci_dmabuf.c      | 315 ++++++++++++++++++++++++++++=
++++=0D
 drivers/vfio/pci/vfio_pci_priv.h        |  23 +++=0D
 drivers/vfio/vfio_main.c                |   2 +=0D
 include/linux/dma-buf.h                 |  18 ++=0D
 include/linux/pci-p2pdma.h              | 120 +++++++-----=0D
 include/linux/vfio.h                    |   2 +=0D
 include/linux/vfio_pci_core.h           |  42 +++++=0D
 include/uapi/linux/vfio.h               |  28 +++=0D
 kernel/dma/direct.c                     |   4 +-=0D
 mm/hmm.c                                |   2 +-=0D
 21 files changed, 1074 insertions(+), 140 deletions(-)=0D

----------------------------------------------------------------------

New:  KVM: x86: Add a help to dedup loading guest/host XCR0 and XSS
[PATCH] KVM: x86: Add a help to dedup loading guest/host XCR0 and XSS
Author: Binbin Wu <binbin.wu@linux.intel.com>

Add and use a helper, kvm_load_xfeatures(), to dedup the code that loads
guest/host xfeatures by passing XCR0 and XSS values accordingly.

No functional change intended.

Signed-off-by: Binbin Wu <binbin.wu@linux.intel.com>
---
The patch is based on the patch series
"KVM: x86: Cleanup #MC and XCR0/XSS/PKRU handling" [1], which is applied on 
top of kvm-x86 next branch(commit a996dd2a5e1ec54dcf7d7b93915ea3f97e14e68a).

[1] https://lore.kernel.org/all/20251030224246.3456492-1-seanjc@google.com
---
 arch/x86/kvm/x86.c | 25 +++++--------------------
 1 file changed, 5 insertions(+), 20 deletions(-)

----------------------------------------------------------------------

New:  support FEAT_LSUI
[PATCH v11 0/9] support FEAT_LSUI
Author: Yeoreum Yun <yeoreum.yun@arm.com>

Since Armv9.6, FEAT_LSUI supplies the load/store instructions for
previleged level to access to access user memory without clearing
PSTATE.PAN bit.

This patchset support FEAT_LSUI and applies in futex atomic operation
and user_swpX emulation where can replace from ldxr/st{l}xr
pair implmentation with clearing PSTATE.PAN bit to correspondant
load/store unprevileged atomic operation without clearing PSTATE.PAN bit.


Patch Sequences
================

Patch #1 adds cpufeature for FEAT_LSUI

Patch #2-#3 expose FEAT_LSUI to guest

Patch #4 adds Kconfig for FEAT_LSUI

Patch #5-#6 support futex atomic-op with FEAT_LSUI

Patch #7-#9 support user_swpX emulation with FEAT_LSUI


Patch History
==============
from v10 to v11:
  - use cast instruction to emulate deprecated swpb instruction
  - https://lore.kernel.org/all/20251103163224.818353-1-yeoreum.yun@arm.com/

from v9 to v10:
  - apply FEAT_LSUI to user_swpX emulation.
  - add test coverage for LSUI bit in ID_AA64ISAR3_EL1
  - rebase to v6.18-rc4
  - https://lore.kernel.org/all/20250922102244.2068414-1-yeoreum.yun@arm.com/

from v8 to v9:
  - refotoring __lsui_cmpxchg64()
  - rebase to v6.17-rc7
  - https://lore.kernel.org/all/20250917110838.917281-1-yeoreum.yun@arm.com/

from v7 to v8:
  - implements futex_atomic_eor() and futex_atomic_cmpxchg() with casalt
    with C helper.
  - Drop the small optimisation on ll/sc futex_atomic_set operation.
  - modify some commit message.
  - https://lore.kernel.org/all/20250816151929.197589-1-yeoreum.yun@arm.com/

from v6 to v7:
  - wrap FEAT_LSUI with CONFIG_AS_HAS_LSUI in cpufeature
  - remove unnecessary addition of indentation.
  - remove unnecessary mte_tco_enable()/disable() on LSUI operation.
  - https://lore.kernel.org/all/20250811163635.1562145-1-yeoreum.yun@arm.com/

from v5 to v6:
  - rebase to v6.17-rc1
  - https://lore.kernel.org/all/20250722121956.1509403-1-yeoreum.yun@arm.com/

from v4 to v5:
  - remove futex_ll_sc.h futext_lsui and lsui.h and move them to futex.h
  - reorganize the patches.
  - https://lore.kernel.org/all/20250721083618.2743569-1-yeoreum.yun@arm.com/

from v3 to v4:
  - rebase to v6.16-rc7
  - modify some patch's title.
  - https://lore.kernel.org/all/20250617183635.1266015-1-yeoreum.yun@arm.com/

from v2 to v3:
  - expose FEAT_LUSI to guest
  - add help section for LUSI Kconfig
  - https://lore.kernel.org/all/20250611151154.46362-1-yeoreum.yun@arm.com/

from v1 to v2:
  - remove empty v9.6 menu entry
  - locate HAS_LUSI in cpucaps in order
  - https://lore.kernel.org/all/20250611104916.10636-1-yeoreum.yun@arm.com/

Yeoreum Yun (9):
  arm64: cpufeature: add FEAT_LSUI
  KVM: arm64: expose FEAT_LSUI to guest
  KVM: arm64: kselftest: set_id_regs: add test for FEAT_LSUI
  arm64: Kconfig: Detect toolchain support for LSUI
  arm64: futex: refactor futex atomic operation
  arm64: futex: support futex with FEAT_LSUI
  arm64: separate common LSUI definitions into lsui.h
  arm64: armv8_deprecated: convert user_swpX to inline function
  arm64: armv8_deprecated: apply FEAT_LSUI for swpX emulation.

 arch/arm64/Kconfig                            |   5 +
 arch/arm64/include/asm/futex.h                | 291 +++++++++++++++---
 arch/arm64/include/asm/lsui.h                 |  25 ++
 arch/arm64/kernel/armv8_deprecated.c          | 111 +++++--
 arch/arm64/kernel/cpufeature.c                |  10 +
 arch/arm64/kvm/sys_regs.c                     |   3 +-
 arch/arm64/tools/cpucaps                      |   1 +
 .../testing/selftests/kvm/arm64/set_id_regs.c |   1 +
 8 files changed, 381 insertions(+), 66 deletions(-)

----------------------------------------------------------------------

New:  KVM: VMX: Fix check for valid GVA on an EPT violation
[PATCH] KVM: VMX: Fix check for valid GVA on an EPT violation
Author: Sukrit Bhatnagar <Sukrit.Bhatnagar@sony.com>

On an EPT violation, bit 7 of the exit qualification is set if the
guest linear-address is valid. The derived page fault error code
should not be checked for this bit.

Fixes: f3009482512e ("KVM: VMX: Set PFERR_GUEST_{FINAL,PAGE}_MASK if and only if the GVA is valid")
Signed-off-by: Sukrit Bhatnagar <Sukrit.Bhatnagar@sony.com>
---
 arch/x86/kvm/vmx/common.h | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

----------------------------------------------------------------------

New:  is context switched between
[PATCH] KVM: SVM: Ensure SPEC_CTRL[63:32] is context switched between
Author: Sean Christopherson <seanjc@google.com>


SPEC_CTRL is an MSR, i.e. a 64-bit value, but the VMRUN assembly code
assumes bits 63:32 are always zero.  The bug is _currently_ benign because
neither KVM nor the kernel support setting any of bits 63:32, but it's
still a bug that needs to be fixed.

Signed-off-by: Uros Bizjak <ubizjak@gmail.com>
Suggested-by: Sean Christopherson <seanjc@google.com>
Co-developed-by: Sean Christopherson <seanjc@google.com>
Signed-off-by: Sean Christopherson <seanjc@google.com>
---
 arch/x86/kvm/svm/vmenter.S | 44 +++++++++++++++++++++++++++++++-------
 1 file changed, 36 insertions(+), 8 deletions(-)

----------------------------------------------------------------------

