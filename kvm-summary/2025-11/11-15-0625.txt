From fa31e1d9a to 305602255
KVM mailing list update from fa31e1d9a to 305602255

Top 15 contributor Email domains (Based on Email Body)

     40 google.com
     17 intel.com
     11 redhat.com
     10 grsecurity.net
      6 kernel.org
      4 amd.com
      2 amazon.com=0A=
      1 linux.intel.com
      1 amazon.de
      1 amazon.co.uk

Top 15 contributors (Based on Email Body)

     39  Sean Christopherson <seanjc@google.com>
     14  Chao Gao <chao.gao@intel.com>
     11  Paolo Bonzini <pbonzini@redhat.com>
     10  Mathias Krause <minipli@grsecurity.net>
      5  Mark Brown <broonie@kernel.org>
      4  Michael Roth <michael.roth@amd.com>
      2  Yang Weijiang <weijiang.yang@intel.com>
      2  Nikita Kalyazin <kalyazin@amazon.com>=0A=
      1  Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
      1  Maximilian Dittgen <mdittgen@amazon.de>
      1  Marc Zyngier <maz@kernel.org>
      1  "Kalyazin, Nikita" <kalyazin@amazon.co.uk>
      1  "Chang S. Bae" <chang.seok.bae@intel.com>
      1  Brendan Jackman <jackmanb@google.com>

===== Patch list in this time period =====


===== Patch Commit Messages ====

New:  x86: cet: Pass virtual addresses to invlpg
[kvm-unit-tests PATCH v4 01/18] x86: cet: Pass virtual addresses to invlpg
Author: Sean Christopherson <seanjc@google.com>


Correct the parameter passed to invlpg.

The invlpg instruction should take a virtual address instead of a physical
address when flushing TLBs. Using shstk_phys results in TLBs associated
with the virtual address (shstk_virt) not being flushed, and the virtual
address may not be treated as a shadow stack address if there is a stale
TLB. So, subsequent shadow stack accesses to shstk_virt may cause a #PF,
which terminates the test unexpectedly.

Signed-off-by: Yang Weijiang <weijiang.yang@intel.com>
Signed-off-by: Chao Gao <chao.gao@intel.com>
Signed-off-by: Mathias Krause <minipli@grsecurity.net>
Signed-off-by: Sean Christopherson <seanjc@google.com>
---
 x86/cet.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

----------------------------------------------------------------------

New:  x86: Improve CET tests
[kvm-unit-tests PATCH v4 00/18] x86: Improve CET tests
Author: Sean Christopherson <seanjc@google.com>

Hopefully the last version of this particular CET series.  Mathias, I owe you
like five beers for root causing and fixing all the gnarly edge cases.

v4:
 - Fixup the argumentes for the vmx_cet_test. [Mathias]
 - Drop "_test" from the vmx_cet config to match the other VMX testcases.
 - Enable NOTRACK instead of dodging jmp tables in exception_mnemonic() [Mathias].
 - Reset IBT state after (intentional) #CP. [Mathias]
 - Fix a changelog typo. [Mathias]
 - Document that ljmpq isn't supported on AMD. [Mathias]
 - Use ljmpl to make the 32-bit JMP FAR more obvious. [Mathias]

v3:
 - https://lore.kernel.org/all/20251114001258.1717007-1-seanjc@google.com
 - Run the test if only one of SHSTK or IBT is supported (e.g. to test
   SHSTK on AMD).
 - Rename the test from "intel_cet" to just "cet".
 - Add an endbr64 in the user_mode trampoline (the test was getting false
   passes without ever reaching cet_shstk_far_ret() due to getting the
   expected #CP).
 - Add testcases to verify KVM rejects emulation as expected.
 - Add a comment explaining the SHSTK PTE magic (I forgot about the magic
   and spent a long time trying to figure out how the user_mode trampoline
   was succeeding if the SHSTK wasn't writable, *sigh*)


Chao Gao (7):
  x86: cet: Remove unnecessary memory zeroing for shadow stack
  x86: cet: Directly check for #CP exception in run_in_user()
  x86: cet: Validate #CP error code
  x86: cet: Use report_skip()
  x86: cet: Drop unnecessary casting
  x86: cet: Validate writing unaligned values to SSP MSR causes #GP
  x86: cet: Validate CET states during VMX transitions

Mathias Krause (5):
  x86: cet: Make shadow stack less fragile
  x86: cet: Simplify IBT test
  x86: cet: Use symbolic values for the #CP error codes
  x86: cet: Test far returns too
  x86: Avoid top-most page for vmalloc on x86-64

Sean Christopherson (5):
  x86: cet: Run SHSTK and IBT tests as appropriate if either feature is
    supported
  x86: cet: Drop the "intel_" prefix from the CET testcase
  x86: cet: Enable NOTRACK handling for IBT tests
  x86: cet: Reset IBT tracker state on #CP violations
  x86: cet: Add testcases to verify KVM rejects emulation of CET
    instructions

Yang Weijiang (1):
  x86: cet: Pass virtual addresses to invlpg

 lib/x86/msr.h      |   1 +
 lib/x86/usermode.c |  16 ++-
 lib/x86/usermode.h |  13 +-
 lib/x86/vm.c       |   2 +
 x86/cet.c          | 308 ++++++++++++++++++++++++++++++++++++---------
 x86/lam.c          |  10 +-
 x86/unittests.cfg  |  10 +-
 x86/vmx.h          |   8 +-
 x86/vmx_tests.c    |  81 ++++++++++++
 9 files changed, 375 insertions(+), 74 deletions(-)

----------------------------------------------------------------------

New:  KVM: selftests: Use GUEST_ASSERT_EQ() to check exit codes in
[PATCH] KVM: selftests: Use GUEST_ASSERT_EQ() to check exit codes in
Author: Sean Christopherson <seanjc@google.com>

Use GUEST_ASSERT_EQ() instead of GUEST_ASSERT(x == SVM_EXIT_<code>) in the
Hyper-V SVM test so that the test prints the actual vs. expected values on
failure.  E.g. instead of printing:

  vmcb->control.exit_code == SVM_EXIT_VMMCALL

print:

 0x7c != 0x81 (vmcb->control.exit_code != SVM_EXIT_VMMCALL)

Cc: Yosry Ahmed <yosry.ahmed@linux.dev>
Signed-off-by: Sean Christopherson <seanjc@google.com>
---
 .../testing/selftests/kvm/x86/hyperv_svm_test.c  | 16 ++++++++--------
 1 file changed, 8 insertions(+), 8 deletions(-)

----------------------------------------------------------------------

New:  KVM: selftests: arm64: Report set_id_reg reads of
[PATCH v2 1/4] KVM: selftests: arm64: Report set_id_reg reads of
Author: Mark Brown <broonie@kernel.org>

Currently when we run guest code to validate that the values we wrote to
the registers are seen by the guest we assert that these values match using
a KVM selftests level assert, resulting in unclear diagnostics if the test
fails. Replace this assert with reporting a kselftest test per register.

In order to support getting the names of the registers we repaint the array
of ID_ registers to store the names and open code the rest.

Signed-off-by: Mark Brown <broonie@kernel.org>
---
 tools/testing/selftests/kvm/arm64/set_id_regs.c | 74 +++++++++++++++++++------
 1 file changed, 57 insertions(+), 17 deletions(-)

----------------------------------------------------------------------

New:  KVM: selftests: arm64: Improve diagnostics from
[PATCH v2 0/4] KVM: selftests: arm64: Improve diagnostics from
Author: Mark Brown <broonie@kernel.org>

While debugging issues related to aarch64 only systems I ran into
speedbumps due to the lack of detail in the results reported when the
guest register read and reset value preservation tests were run, they
generated an immediately fatal assert without indicating which register
was being tested. Update these tests to report a result per register,
making it much easier to see what the problem being reported is.

A similar, though less severe, issue exists with the validation of the
individual bitfields in registers due to the use of immediately fatal
asserts. Update those asserts to be standard kselftest reports.

Finally we have a fix for spurious errors on some NV systems.

Signed-off-by: Mark Brown <broonie@kernel.org>
---
Changes in v2:
- Add a fix for spurious failures with 64 bit only guests.
- Link to v1: https://patch.msgid.link/20251030-kvm-arm64-set-id-regs-aarch64-v1-0-96fe0d2b178e@kernel.org

---
Mark Brown (4):
      KVM: selftests: arm64: Report set_id_reg reads of test registers as tests
      KVM: selftests: arm64: Report register reset tests individually
      KVM: selftests: arm64: Make set_id_regs bitfield validatity checks non-fatal
      KVM: selftests: arm64: Skip all 32 bit IDs when set_id_regs is aarch64 only

 tools/testing/selftests/kvm/arm64/set_id_regs.c | 150 ++++++++++++++++++------
 1 file changed, 111 insertions(+), 39 deletions(-)

----------------------------------------------------------------------

New:  KVM: guest_memfd: add generic population via write
[PATCH v7 1/2] KVM: guest_memfd: add generic population via write
Author: Kalyazin, Nikita <kalyazin@amazon.co.uk>

=0A=
On systems that support shared guest memory, write() is useful, for=0A=
example, for population of the initial image.  Even though the same can=0A=
also be achieved via userspace mapping and memcpying from userspace,=0A=
write() provides a more performant option because it does not need to=0A=
set user page tables and it does not cause a page fault for every page=0A=
like memcpy would.  Note that memcpy cannot be accelerated via=0A=
MADV_POPULATE_WRITE as it is not supported by guest_memfd and relies on=0A=
GUP.=0A=
=0A=
Populating 512MiB of guest_memfd on a x86 machine:=0A=
 - via memcpy: 436 ms=0A=
 - via write:  202 ms (-54%)=0A=
=0A=
Only PAGE_ALIGNED offset and len are allowed.  Even though non-aligned=0A=
writes are technically possible, when in-place conversion support is=0A=
implemented [1], the restriction makes handling of mixed shared/private=0A=
huge pages simpler.  write() will only be allowed to populate shared=0A=
pages.=0A=
=0A=
When direct map removal is implemented [2]=0A=
 - write() will not be allowed to access pages that have already been=0A=
   removed from direct map=0A=
 - on completion, write() will remove the populated pages from direct=0A=
   map=0A=
=0A=
While it is technically possible to implement read() syscall on systems=0A=
with shared guest memory, it is not supported as there is currently no=0A=
use case for it.=0A=
=0A=
[1] https://lore.kernel.org/kvm/cover.1760731772.git.ackerleytng@google.com=
=0A=
[2] https://lore.kernel.org/kvm/20250924151101.2225820-1-patrick.roy@campus=
.lmu.de=0A=
=0A=
Signed-off-by: Nikita Kalyazin <kalyazin@amazon.com>=0A=
---=0A=
 Documentation/virt/kvm/api.rst |  2 ++=0A=
 include/linux/kvm_host.h       |  2 +-=0A=
 include/uapi/linux/kvm.h       |  1 +=0A=
 virt/kvm/guest_memfd.c         | 52 ++++++++++++++++++++++++++++++++++=0A=
 4 files changed, 56 insertions(+), 1 deletion(-)=0A=

----------------------------------------------------------------------

New:  KVM: guest_memfd: use write for population
[PATCH v7 0/2] KVM: guest_memfd: use write for population
Author: Kalyazin, Nikita <kalyazin@amazon.co.uk>

On systems that support shared guest memory, write() is useful, for=0A=
example, for population of the initial image.  Even though the same can=0A=
also be achieved via userspace mapping and memcpying from userspace,=0A=
write() provides a more performant option because it does not need to=0A=
set user page tables and it does not cause a page fault for every page=0A=
like memcpy would.  Note that memcpy cannot be accelerated via=0A=
MADV_POPULATE_WRITE as it is not supported by guest_memfd and relies on=0A=
GUP.=0A=
=0A=
Populating 512MiB of guest_memfd on a x86 machine:=0A=
 - via memcpy: 436 ms=0A=
 - via write:  202 ms (-54%)=0A=
=0A=
Only PAGE_ALIGNED offset and len are allowed.  Even though non-aligned=0A=
writes are technically possible, when in-place conversion support is=0A=
implemented [1], the restriction makes handling of mixed shared/private=0A=
huge pages simpler.  write() will only be allowed to populate shared=0A=
pages.=0A=
=0A=
When direct map removal is implemented [2]=0A=
 - write() will not be allowed to access pages that have already=0A=
   been removed from direct map=0A=
 - on completion, write() will remove the populated pages from=0A=
   direct map=0A=
=0A=
While it is technically possible to implement read() syscall on systems=0A=
with shared guest memory, it is not supported as there is currently no=0A=
use case for it.=0A=
=0A=
[1]=0A=
https://lore.kernel.org/kvm/cover.1760731772.git.ackerleytng@google.com=0A=
[2]=0A=
https://lore.kernel.org/kvm/20250924151101.2225820-1-patrick.roy@campus.lmu=
.de=0A=
=0A=
Nikita Kalyazin (2):=0A=
  KVM: guest_memfd: add generic population via write=0A=
  KVM: selftests: update guest_memfd write tests=0A=
=0A=
 Documentation/virt/kvm/api.rst                |  2 +=0A=
 include/linux/kvm_host.h                      |  2 +-=0A=
 include/uapi/linux/kvm.h                      |  1 +=0A=
 .../testing/selftests/kvm/guest_memfd_test.c  | 58 +++++++++++++++++--=0A=
 virt/kvm/guest_memfd.c                        | 52 +++++++++++++++++=0A=
 5 files changed, 108 insertions(+), 7 deletions(-)=0A=

----------------------------------------------------------------------

New:  KVM: selftests: Add SYNC after guest ITS setup in vgic_lpi_stress
[PATCH] KVM: selftests: Add SYNC after guest ITS setup in vgic_lpi_stress
Author: Maximilian Dittgen <mdittgen@amazon.de>

vgic_lpi_stress sends MAPTI and MAPC commands during guest GIC
setup to map interrupt events to ITT entries and collection IDs
to redistributors, respectively.

Theoretically, we have no guarantee that the ITS will
finish handling these mapping commands before the selftest
calls KVM_SIGNAL_MSI to inject LPIs to the guest. If LPIs
are injected before ITS mapping completes, the ITS cannot
properly pass the interrupt on to the redistributor.

In practice, KVM processes ITS commands synchronously, so
SYNC calls are functionally unnecessary and ignored in
vgic_its_handle_command().

However, selftests should test based on ARM specification and
be blind to KVM-specific implementation optimizations. Thus,
we must update the test to be architecturally compliant and
logically correct.

Fix by adding a SYNC command to the selftests ITS library,
then calling SYNC after ITS mapping to ensure mapping
completes before signal_lpi() writes to GITS_TRANSLATER.

This patch depends on commit a24f7afce048 ("KVM: selftests:
fix MAPC RDbase target formatting in vgic_lpi_stress"), which
is queued in kvmarm/fixes.

Signed-off-by: Maximilian Dittgen <mdittgen@amazon.de>
---
Validated by the following debug logging to the GITS_CMD_SYNC handler
in vgic_its_handle_command():

        kvm_info("ITS SYNC command: %016llx %016llx %016llx %016llx\n",
            its_cmd[0], its_cmd[1], its_cmd[2], its_cmd[3]);

Initialized a selftest guest with 4 vCPUs by:

        ./vgic_lpi_stress -v 4

Confirmed that an ITS SYNC was successfully called for all 4 vCPUs:

        kvm [5094]: ITS SYNC command: 0000000000000005 0000000000000000 0000000000000000 0000000000000000
        kvm [5094]: ITS SYNC command: 0000000000000005 0000000000000000 0000000000010000 0000000000000000
        kvm [5094]: ITS SYNC command: 0000000000000005 0000000000000000 0000000000020000 0000000000000000
        kvm [5094]: ITS SYNC command: 0000000000000005 0000000000000000 0000000000030000 0000000000000000
---
 tools/testing/selftests/kvm/arm64/vgic_lpi_stress.c   |  4 ++++
 .../testing/selftests/kvm/include/arm64/gic_v3_its.h  |  1 +
 tools/testing/selftests/kvm/lib/arm64/gic_v3_its.c    | 11 +++++++++++
 3 files changed, 16 insertions(+)

----------------------------------------------------------------------

New:  KVM: arm64: GICv3: Don't advertise ICH_HCR_EL2.En==1 when no vgic is configured
[PATCH] KVM: arm64: GICv3: Don't advertise ICH_HCR_EL2.En==1 when no vgic is configured
Author: Marc Zyngier <maz@kernel.org>

Configuring GICv3 to deal with the lack of GIC in the guest relies
on not setting ICH_HCR_EL2.En in the shadow register, as this is
an indication of the fact that we want to trap all system registers
to report an UNDEF in the guest.

Make sure we leave vgic_hcr untouched in this case.

Reported-by: Mark Brown <broonie@kernel.org>
Closes: https://lore.kernel.org/r/72e1e8b5-e397-4dc5-9cd6-a32b6af3d739@sirena.org.uk
Fixes: 877324a1b5415 ("KVM: arm64: Revamp vgic maintenance interrupt configuration")
Signed-off-by: Marc Zyngier <maz@kernel.org>
---
 arch/arm64/kvm/vgic/vgic-v3.c | 3 +++
 1 file changed, 3 insertions(+)

----------------------------------------------------------------------

New:  KVM: emulate: add MOVNTDQA
[PATCH 01/10] KVM: emulate: add MOVNTDQA
Author: Paolo Bonzini <pbonzini@redhat.com>

MOVNTDQA is a simple MOV instruction, in fact it has the same
characteristics as 0F E7 (MOVNTDQ) other than the aligned-address
requirement.

Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
---
 arch/x86/kvm/emulate.c | 13 +++++++++----
 1 file changed, 9 insertions(+), 4 deletions(-)

----------------------------------------------------------------------

New:  KVM: emulate: enable AVX moves
[PATCH 00/10] KVM: emulate: enable AVX moves
Author: Paolo Bonzini <pbonzini@redhat.com>

Over a year ago, Keith Busch posted an RFC patch to enable VMOVDQA
and VMOVDQU instructions in the KVM emulator.  The reason to do so
is that people are using QEMU to emulate fancy devices whose drivers
use those instructions with BARs that, on real hardware, would
presumably support write combining.  These same people obviously
would appreciate being able to use KVM instead of emulation, hence
the request.

The original patch was not bad at all, but missed a few details:

- checking in XCR0 if AVX is enabled (which also protects against
  *hosts* with AVX disabled)

- 32-bit support

- clearing the high bytes of AVX registers if VEX.L=0

- checking some invalid prefix combinations

The ugly parts are in patch 7, which has to juggle the fact that the
same instruction can decode to SSE and AVX, and we only know which are
valid after all the groups are handled.

While at it I also included a small refactoring taken out of the
APX series, by Chang S. Bae, some cleanups, and an extra MOVNTDQ
instruction.

Paolo

Chang S. Bae (1):
  KVM: x86: Refactor REX prefix handling in instruction emulation

Paolo Bonzini (9):
  KVM: emulate: add MOVNTDQA
  KVM: emulate: move Src2Shift up one bit
  KVM: emulate: improve formatting of flags table
  KVM: emulate: move op_prefix to struct x86_emulate_ctxt
  KVM: emulate: share common register decoding code
  KVM: emulate: add get_xcr callback
  KVM: emulate: add AVX support to register fetch and writeback
  KVM: emulate: decode VEX prefix
  KVM: emulate: enable AVX moves

 arch/x86/kvm/emulate.c     | 320 ++++++++++++++++++++++++++-----------
 arch/x86/kvm/fpu.h         |  62 +++++++
 arch/x86/kvm/kvm_emulate.h |  20 ++-
 arch/x86/kvm/x86.c         |   9 ++
 4 files changed, 311 insertions(+), 100 deletions(-)

----------------------------------------------------------------------

New:  x86/run_in_user: Add an "end branch"
[kvm-unit-tests PATCH v3 01/17] x86/run_in_user: Add an "end branch"
Author: Sean Christopherson <seanjc@google.com>

Add an endbr64 at the user_mode "entry point" so that run_in_user() can be
used when CET's Indirect Branch Tracking is enabled.

Signed-off-by: Sean Christopherson <seanjc@google.com>
---
 lib/x86/usermode.c | 3 +++
 1 file changed, 3 insertions(+)

----------------------------------------------------------------------

Exist: [kvm-unit-tests PATCH v4 00/18] x86: Improve CET tests
 Skip: [kvm-unit-tests PATCH v3 00/17] x86: Improve CET tests
New:  KVM: VMX: Use on-stack copy of @flags in __vmx_vcpu_run()
[PATCH v5 1/9] KVM: VMX: Use on-stack copy of @flags in __vmx_vcpu_run()
Author: Sean Christopherson <seanjc@google.com>

When testing for VMLAUNCH vs. VMRESUME, use the copy of @flags from the
stack instead of first moving it to EBX, and then propagating
VMX_RUN_VMRESUME to RFLAGS.CF (because RBX is clobbered with the guest
value prior to the conditional branch to VMLAUNCH).  Stashing information
in RFLAGS is gross, especially with the writer and reader being bifurcated
by yet more gnarly assembly code.

Opportunistically drop the SHIFT macros as they existed purely to allow
the VM-Enter flow to use Bit Test.

Suggested-by: Borislav Petkov <bp@alien8.de>
Signed-off-by: Sean Christopherson <seanjc@google.com>
---
 arch/x86/kvm/vmx/run_flags.h | 10 +++-------
 arch/x86/kvm/vmx/vmenter.S   | 13 ++++---------
 2 files changed, 7 insertions(+), 16 deletions(-)

----------------------------------------------------------------------

New:  x86/bugs: KVM: L1TF and MMIO Stale Data cleanups
[PATCH v5 0/9] x86/bugs: KVM: L1TF and MMIO Stale Data cleanups
Author: Sean Christopherson <seanjc@google.com>

Clean up KVM's handling of L1TF and MMIO Stale data, as the code has bit
rotted a bit and is harder than it should be to understand, and has a few
warts.

TL;DR:

 - Unify L1TF flushing under per-CPU variable
 - Bury L1TF L1D flushing under CONFIG_CPU_MITIGATIONS=y
 - Move MMIO Stale Data into asm, and do VERW at most once per VM-Enter

To allow VMX to use ALTERNATIVE_2 to select slightly different flows for doing
VERW, tweak the low lever macros in nospec-branch.h to define the instruction
sequence, and then wrap it with __stringify() as needed.

As before, the non-VMX code is lightly tested (but there's far less chance
for breakage there).  For the VMX code, I verified the KVM side of things by
hacking the code to force/clear various mitigations, and using ud2 to confirm
the right path got selected.

v5:
 - Collect reviews and acks.
 - Add/improve comments for various macros and flows. [Everyone]
 - s/CLEAR_CPU_BUFFERS_SEQ/VERW [Pawan, Boris]
 - Use the on-stack copy of @flags instead of stashing information in
   RFLAGS' arithmetic flags. [Boris]
 - Fix typos (hopefully). [Boris]

v4:
 - https://lore.kernel.org/all/20251031003040.3491385-1-seanjc@google.com
 - Drop the patch to fallback to handling the MMIO mitigation if
   vmx_l1d_flush() doesn't flush, and instead use Pawan's approach of
   decoupling the two entirely.
 - Replace the static branch with X86_FEATURE_CLEAR_CPU_BUF_MMIO so that
   it can be referenced in ALTERNATIVE macros.
 - Decouple X86_FEATURE_CLEAR_CPU_BUF_VM from X86_FEATURE_CLEAR_CPU_BUF_MMIO
   (though they still interact and can both be set)

v3:
 - https://lore.kernel.org/all/20251016200417.97003-1-seanjc@google.com
 - [Pawan's series] https://lore.kernel.org/all/20251029-verw-vm-v1-0-babf9b961519@linux.intel.com
 - Put the "raw" variant in KVM, dress it up with KVM's "request" terminology,
   and add a comment explaining why _KVM_ knows its usage doesn't need to
   disable virtualization.
 - Add the prep patches.

v2:
 - https://lore.kernel.org/all/20251015-b4-l1tf-percpu-v2-1-6d7a8d3d40e9@google.com
 - Moved the bit back to irq_stat
 - Fixed DEBUG_PREEMPT issues by adding a _raw variant

v1: https://lore.kernel.org/r/20251013-b4-l1tf-percpu-v1-1-d65c5366ea1a@google.com

Brendan Jackman (1):
  KVM: x86: Unify L1TF flushing under per-CPU variable

Pawan Gupta (1):
  x86/bugs: Use VM_CLEAR_CPU_BUFFERS in VMX as well

Sean Christopherson (7):
  KVM: VMX: Use on-stack copy of @flags in __vmx_vcpu_run()
  x86/bugs: Decouple ALTERNATIVE usage from VERW macro definition
  x86/bugs: Use an x86 feature to track the MMIO Stale Data mitigation
  KVM: VMX: Handle MMIO Stale Data in VM-Enter assembly via
    ALTERNATIVES_2
  x86/bugs: KVM: Move VM_CLEAR_CPU_BUFFERS into SVM as
    SVM_CLEAR_CPU_BUFFERS
  KVM: VMX: Bundle all L1 data cache flush mitigation code together
  KVM: VMX: Disable L1TF L1 data cache flush if CONFIG_CPU_MITIGATIONS=n

 arch/x86/include/asm/cpufeatures.h   |   5 +
 arch/x86/include/asm/hardirq.h       |   4 +-
 arch/x86/include/asm/kvm_host.h      |   3 -
 arch/x86/include/asm/nospec-branch.h |  25 ++-
 arch/x86/kernel/cpu/bugs.c           |  22 +--
 arch/x86/kvm/mmu/mmu.c               |   2 +-
 arch/x86/kvm/mmu/spte.c              |   2 +-
 arch/x86/kvm/svm/vmenter.S           |   6 +-
 arch/x86/kvm/vmx/nested.c            |   2 +-
 arch/x86/kvm/vmx/run_flags.h         |  10 +-
 arch/x86/kvm/vmx/vmenter.S           |  29 ++--
 arch/x86/kvm/vmx/vmx.c               | 235 ++++++++++++++-------------
 arch/x86/kvm/x86.c                   |   6 +-
 arch/x86/kvm/x86.h                   |  14 ++
 14 files changed, 193 insertions(+), 172 deletions(-)

----------------------------------------------------------------------

New:  KVM: guest_memfd: Elaborate on how release() vs. get_pfn() is
[PATCH] KVM: guest_memfd: Elaborate on how release() vs. get_pfn() is
Author: Sean Christopherson <seanjc@google.com>

Add more context and information to the comment in kvm_gmem_release() that
explains why there's no synchronization on RCU _or_ kvm->srcu.  Point (b)
from commit 67b43038ce14 ("KVM: guest_memfd: Remove RCU-protected attribute
from slot->gmem.file")

      b) kvm->srcu ensures that kvm_gmem_unbind() and freeing of a memslot
         occur after the memslot is no longer visible to kvm_gmem_get_pfn().

is especially difficult to fully grok, particularly in light of commit
ae431059e75d ("KVM: guest_memfd: Remove bindings on memslot deletion when
gmem is dying"), which addressed a race between unbind() and release().

No functional change intended.

Cc: Yan Zhao <yan.y.zhao@intel.com>
Cc: Vishal Annapurve <vannapurve@google.com>
Signed-off-by: Sean Christopherson <seanjc@google.com>
---
 virt/kvm/guest_memfd.c | 20 ++++++++++++++------
 1 file changed, 14 insertions(+), 6 deletions(-)

----------------------------------------------------------------------

New:  KVM: SVM: Serialize updates to global OS-Visible
[PATCH 1/5] KVM: SVM: Serialize updates to global OS-Visible
Author: Sean Christopherson <seanjc@google.com>

Guard writes to the global osvw_status and osvw_len variables with a
spinlock to ensure enabling virtualization on multiple CPUs in parallel
doesn't effectively drop any writes due to writing back stale data.  Don't
bother taking the lock when the boot CPU doesn't support the feature, as
that check is constant for all CPUs, i.e. racing writes will always write
the same value (zero).

Note, the bug was inadvertently "fixed" by commit 9a798b1337af ("KVM:
Register cpuhp and syscore callbacks when enabling hardware"), which
effectively serialized calls to enable virtualization due to how the cpuhp
framework "brings up" CPU.  But KVM shouldn't rely on the mechanics of
cphup to provide serialization.

Signed-off-by: Sean Christopherson <seanjc@google.com>
---
 arch/x86/kvm/svm/svm.c | 10 +++++++---
 1 file changed, 7 insertions(+), 3 deletions(-)

----------------------------------------------------------------------

New:  KVM: SVM: Fix and clean up OSVW handling
[PATCH 0/5] KVM: SVM: Fix and clean up OSVW handling
Author: Sean Christopherson <seanjc@google.com>

Fix a long-standing bug where KVM could clobber its OS-visible workarounds
handling (not that anyone would notice), and then clean up the code to make
it easier understand and maintain (I didn't even know what "osvw" stood for
until I ran into this code when trying to moving actual SVM pieces of
svm_enable_virtualization_cpu() out of KVM (for TDX purposes)).

Tested by running in a VM and generating unique per-vCPU MSR values in the
host (see below), and verifying KVM ended up with the right values.

diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index c9c2aa6f4705..d8b8eff733d8 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -4631,12 +4631,20 @@ int kvm_get_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
        case MSR_AMD64_OSVW_ID_LENGTH:
                if (!guest_cpu_cap_has(vcpu, X86_FEATURE_OSVW))
                        return 1;
+
+               if (vcpu->vcpu_idx == 64)
+                       return 1;
+
                msr_info->data = vcpu->arch.osvw.length;
+               if (vcpu->vcpu_idx < 64)
+                       msr_info->data = max(vcpu->vcpu_idx, 8);
                break;
        case MSR_AMD64_OSVW_STATUS:
                if (!guest_cpu_cap_has(vcpu, X86_FEATURE_OSVW))
                        return 1;
                msr_info->data = vcpu->arch.osvw.status;
+               if (vcpu->vcpu_idx < 64)
+                       msr_info->data |= BIT_ULL(vcpu->vcpu_idx);
                break;
        case MSR_PLATFORM_INFO:
                if (!msr_info->host_initiated &&

Sean Christopherson (5):
  KVM: SVM: Serialize updates to global OS-Visible Workarounds variables
  KVM: SVM: Skip OSVW MSR reads if KVM is treating all errata as present
  KVM: SVM: Extract OS-visible workarounds setup to helper function
  KVM: SVM: Skip OSVW variable updates if current CPU's errata are a
    subset
  KVM: SVM: Skip OSVW MSR reads if current CPU doesn't support the
    feature

 arch/x86/kvm/svm/svm.c | 72 ++++++++++++++++++++++++++----------------
 1 file changed, 44 insertions(+), 28 deletions(-)

----------------------------------------------------------------------

New:  KVM: guest_memfd: Remove preparation tracking
[PATCH 1/3] KVM: guest_memfd: Remove preparation tracking
Author: Michael Roth <michael.roth@amd.com>

guest_memfd currently uses the folio uptodate flag to track:

  1) whether or not a page has been cleared before initial usage
  2) whether or not the architecture hooks have been issued to put the
     page in a private state as defined by the architecture

In practice, 2) is only actually being tracked for SEV-SNP VMs, and
there do not seem to be any plans/reasons that would suggest this will
change in the future, so this additional tracking/complexity is not
really providing any general benefit to guest_memfd users. Future plans
around in-place conversion and hugepage support, where the per-folio
uptodate flag is planned to be used purely to track the initial clearing
of folios, whereas conversion operations could trigger multiple
transitions between 'prepared' and 'unprepared' and thus need separate
tracking, will make the burden of tracking this information within
guest_memfd even more complex, since preparation generally happens
during fault time, on the "read-side" of any global locks that might
protect state tracked by guest_memfd, and so may require more complex
locking schemes to allow for concurrent handling of page faults for
multiple vCPUs where the "preparedness" state tracked by guest_memfd
might need to be updated as part of handling the fault.

Instead of keeping this current/future complexity within guest_memfd for
what is essentially just SEV-SNP, just drop the tracking for 2) and have
the arch-specific preparation hooks get triggered unconditionally on
every fault so the arch-specific hooks can check the preparation state
directly and decide whether or not a folio still needs additional
preparation. In the case of SEV-SNP, the preparation state is already
checked again via the preparation hooks to avoid double-preparation, so
nothing extra needs to be done to update the handling of things there.

Signed-off-by: Michael Roth <michael.roth@amd.com>
---
 virt/kvm/guest_memfd.c | 47 ++++++++++++++----------------------------
 1 file changed, 15 insertions(+), 32 deletions(-)

----------------------------------------------------------------------

New:  KVM: guest_memfd: Rework preparation/population flows in prep for in-place conversion
[PATCH RFC 0/3] KVM: guest_memfd: Rework preparation/population flows in prep for in-place conversion
Author: Michael Roth <michael.roth@amd.com>

This patchset is also available at:

  https://github.com/AMDESE/linux/tree/gmem-populate-rework-rfc1

and is based on top of kvm-x86/next (kvm-x86-next-2025.11.07)


Overview
--------

Yan previously posted a series[1] that reworked kvm_gmem_populate() to deal
with potential locking issues that might arise once in-place conversion
support[2] is added for guest_memfd. To quote Yan's original summary of the
issues:

  (1)
  In Michael's series "KVM: gmem: 2MB THP support and preparedness tracking
  changes" [4], kvm_gmem_get_pfn() was modified to rely on the filemap
  invalidation lock for protecting its preparedness tracking. Similarly, the
  in-place conversion version of guest_memfd series by Ackerly also requires
  kvm_gmem_get_pfn() to acquire filemap invalidation lock [5].
  
  kvm_gmem_get_pfn
      filemap_invalidate_lock_shared(file_inode(file)->i_mapping);
  
  However, since kvm_gmem_get_pfn() is called by kvm_tdp_map_page(), which is
  in turn invoked within kvm_gmem_populate() in TDX, a deadlock occurs on the
  filemap invalidation lock.
  
  (2)
  Moreover, in step 2, get_user_pages_fast() may acquire mm->mmap_lock,
  resulting in the following lock sequence in tdx_vcpu_init_mem_region():
  - filemap invalidation lock --> mm->mmap_lock
  
  However, in future code, the shared filemap invalidation lock will be held
  in kvm_gmem_fault_shared() (see [6]), leading to the lock sequence:
  - mm->mmap_lock --> filemap invalidation lock
  
  This creates an AB-BA deadlock issue.

Sean has since then addressed (1) with his series[3] that avoids relying on
calling kvm_gmem_get_pfn() within the TDX post-populate callback to re-fetch
the PFN that was passed to it.

This series aims to address (2), which is still outstanding, and does so based
heavily on Sean's suggested approach[4] of hoisting the get_user_pages_fast()
out of the TDX post-populate callback so that it can be called prior to taking
the filemap invalidate lock so that the ABBA deadlock is no longer possible.

It additionally removes 'preparation' tracking from guest_memfd, which would
similarly complicate locking considerations in the context of in-place
conversion (and even moreso in the context of hugepage support). This has
been discussed during both the guest_memfd calls and PUCK calls, and so far
no strong objections have been given, so hopefully that particular change
isn't too controversial.


Some items worth noting/discussing
----------------------------------

(A) Unlike TDX, which has always enforced that the source address used to
    populate the contents of gmem pages via kvm_gmem_populate() is
    page-aligned, SNP explicitly allowed for this. This unfortunately means
    that instead of a simple 1:1 correspondance between source/target pages,
    post-populate callbacks need to be able to handle straddling multiple
    source pages to populate a single target page within guest_memfd, which
    complicates the handling. While the changes to the SNP post-populate
    callback in patch #3 are not horrendous, they certainly are not ideal.
    However, architectures that never allowed a non-page-aligned source
    address can essentially ignore the src_pages/src_offset considerations
    and simply assume/enforce src_offset is 0, and that src_pages[0] is the
    source struct page of relevance for each call.

    That said, it would be possible to have SNP copy unaligned pages into
    an intermediate set of bounce-buffer pages before passing them to some
    variant of kvm_gmem_populate() that skips the GUP and just works directly
    with the kernel-allocated bounce pages, but there is a performance hit
    there, and potentially some additional complexity with the interfaces to
    handle the different flow, so it's not clear if the trade-off is worth
    it.

    Another potential approach would be to take advantage of the fact that
    all *known* VMM implementations of SNP do use page-aligned source
    addresses, so it *may* be justifiable to retroactively enforce this as
    a requirement so that the post-populate callbacks can be simplified
    accordingly.

(B) While one of the aims of this rework is to implement things such that
    a separate source address can still be passed to kvm_gmem_populate()
    even though the gmem pages can be populated in-place from userspace
    beforehand, issues still arise if the source address itself has the
    KVM_MEMORY_ATTRIBUTE_PRIVATE attribute set, e.g. if source/target
    addresses are the same page. One line of reasoning would be to
    conclude that KVM_MEMORY_ATTRIBUTE_PRIVATE implies that it cannot
    be used as the source of a GUP/copy_from_user(), and thus cases like
    source==target are naturally disallowed. Thus userspace has no choice
    but to populate pages in-place *prior* to setting the
    KVM_MEMORY_ATTRIBUTE_PRIVATE attribute (as kvm_gmem_populate()
    requires), and passing in NULL for the source such that the GUP can
    be skipped (otherwise, it will trigger the shared memory fault path,
    which will then SIGBUS because it will see that it is faulting in
    pages for which KVM_MEMORY_ATTRIBUTE_PRIVATE is set).

    While workable, this would at the very least involve documentation
    updates to KVM_TDX_INIT_MEM_REGION/KVM_SEV_SNP_LAUNCH_UPDATE to cover
    these soon-to-be-possible scenarios. Ira posted a patch separately
    that demonstrates how a NULL source could be safely handled within
    the TDX post-populate callback[5].

    
Known issues / TODO
-------------------

- Compile-tested only for the TDX bits (testing/feedback welcome!)


Thanks,

Mike


[1] https://lore.kernel.org/kvm/20250703062641.3247-1-yan.y.zhao@intel.com/
[2] https://lore.kernel.org/kvm/cover.1760731772.git.ackerleytng@google.com/
[3] https://lore.kernel.org/kvm/20251030200951.3402865-1-seanjc@google.com/
[4] https://lore.kernel.org/kvm/aHEwT4X0RcfZzHlt@google.com/
[5] https://lore.kernel.org/kvm/20251105-tdx-init-in-place-v1-1-1196b67d0423@intel.com/


----------------------------------------------------------------
Michael Roth (3):
      KVM: guest_memfd: Remove preparation tracking
      KVM: TDX: Document alignment requirements for KVM_TDX_INIT_MEM_REGION
      KVM: guest_memfd: GUP source pages prior to populating guest memory

 Documentation/virt/kvm/x86/intel-tdx.rst |  2 +-
 arch/x86/kvm/svm/sev.c                   | 40 +++++++++-----
 arch/x86/kvm/vmx/tdx.c                   | 20 +++----
 include/linux/kvm_host.h                 |  3 +-
 virt/kvm/guest_memfd.c                   | 89 ++++++++++++++++++--------------
 5 files changed, 88 insertions(+), 66 deletions(-)

----------------------------------------------------------------------

New:  KVM: nSVM: Clear exit_code_hi in VMCB when synthesizing
[PATCH 1/9] KVM: nSVM: Clear exit_code_hi in VMCB when synthesizing
Author: Sean Christopherson <seanjc@google.com>

Explicitly clear exit_code_hi in the VMCB when synthesizing "normal"
nested VM-Exits, as the full exit code is a 64-bit value (spoiler alert),
and all exit codes for non-failing VMRUN use only bits 31:0.

Cc: Jim Mattson <jmattson@google.com>
Cc: Yosry Ahmed <yosry.ahmed@linux.dev>
Cc: stable@vger.kernel.org
Signed-off-by: Sean Christopherson <seanjc@google.com>
---
 arch/x86/kvm/svm/svm.c | 2 ++
 arch/x86/kvm/svm/svm.h | 7 ++++---
 2 files changed, 6 insertions(+), 3 deletions(-)

----------------------------------------------------------------------

New:  KVM: SVM: Fix (hilarious) exit_code bugs
[PATCH 0/9] KVM: SVM: Fix (hilarious) exit_code bugs
Author: Sean Christopherson <seanjc@google.com>

Hyper-V folks, y'all are getting Cc'd because of a change in
include/hyperv/hvgdk.h to ensure HV_SVM_EXITCODE_ENL is an unsigned value.
AFAICT, only KVM consumes that macro.  That said, any insight you can provide
on relevant Hyper-V behavior would be appreciated :-)


Fix bugs in SVM that mostly impact nested SVM where KVM treats exit codes
as 32-bit values instead of 64-bit values.  I have no idea how KVM ended up
with such an egregious flaw, as the blame trail goes all the way back to
commit 6aa8b732ca01 ("[PATCH] kvm: userspace interface").  Maybe there was
pre-production hardware or something?

I'm also fairly surprised no one has noticed, as at least Xen treats exit
codes as 64-bit values.  Maybe the only people that run hypervisor tests on
top of KVM are also running KVM, or similarly buggy tests?  /shrug

The most dangerous aspect of the mess is that simply fixing KVM would likely
break KVM-on-KVM setups if only L1 is patched.  To try and avoid such
breakage while also fixing KVM, I opted to have KVM retain its checks on
only bits 31:0 if KVM is running as a VM (as detected by
X86_FEATURE_HYPERVISOR).

I stumbled on this when trying to resolve a array_index_nospec() build failure
on 32-bit kernels (array_index_nospec() requires the index to fit in an
"unsigned long").

Oh, and I have KUT changes to detect the nSVM bugs.

Because of the potential for breakage, I tagged only the nSVM fixes for
stable@.  E.g. I almost botched things by sending this as two separate
series, which would have create a window where svm_invoke_exit_handler()
would process a 64-bit code when running KVM-on-KVM and thus break if L0
KVM left gargage in bits 63:32.

Sean Christopherson (9):
  KVM: nSVM: Clear exit_code_hi in VMCB when synthesizing nested
    VM-Exits
  KVM: nSVM: Set exit_code_hi to -1 when synthesizing SVM_EXIT_ERR
    (failed VMRUN)
  KVM: SVM: Add a helper to detect VMRUN failures
  KVM: SVM: Open code handling of unexpected exits in
    svm_invoke_exit_handler()
  KVM: SVM: Check for an unexpected VM-Exit after RETPOLINE "fast"
    handling
  KVM: SVM: Filter out 64-bit exit codes when invoking exit handlers on
    bare metal
  KVM: SVM: Treat exit_code as an unsigned 64-bit value through all of
    KVM
  KVM: SVM: Limit incorrect check on SVM_EXIT_ERR to running as a VM
  KVM: SVM: Harden exit_code against being used in Spectre-like attacks

 arch/x86/include/asm/svm.h      |  3 +-
 arch/x86/include/uapi/asm/svm.h | 32 ++++++++++-----------
 arch/x86/kvm/svm/hyperv.c       |  1 -
 arch/x86/kvm/svm/nested.c       | 29 +++++++------------
 arch/x86/kvm/svm/sev.c          | 36 ++++++++----------------
 arch/x86/kvm/svm/svm.c          | 49 +++++++++++++++++++--------------
 arch/x86/kvm/svm/svm.h          | 17 ++++++++----
 arch/x86/kvm/trace.h            |  2 +-
 include/hyperv/hvgdk.h          |  2 +-
 9 files changed, 82 insertions(+), 89 deletions(-)

----------------------------------------------------------------------

