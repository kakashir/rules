From b50ef847a to 679b71a1a
KVM mailing list update from b50ef847a to 679b71a1a

Top 15 contributor Email domains (Based on Email Body)

     23 google.com
     14 nvidia.com
     10 linux.intel.com
      6 kernel.org
      4 linux.ibm.com
      3 intel.com
      2 tu-dortmund.de
      2 suse.com
      1 vates.tech
      1 redhat.com
      1 nutanix.com
      1 fb.com

Top 15 contributors (Based on Email Body)

     14  Ankit Agrawal <ankita@nvidia.com>
     13  David Matlack <dmatlack@google.com>
     10  Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
      9  Vipin Sharma <vipinsh@google.com>
      6  Marc Zyngier <maz@kernel.org>
      3  Xiaoyao Li <xiaoyao.li@intel.com>
      3  Andrew Donnellan <ajd@linux.ibm.com>
      2  Simon Schippers <simon.schippers@tu-dortmund.de>
      2  Juergen Gross <jgross@suse.com>
      1  "Thomas Courrege" <thomas.courrege@vates.tech>
      1  Stefano Garzarella <sgarzare@redhat.com>
      1  Sean Christopherson <seanjc@google.com>
      1  Khushit Shah <khushit.shah@nutanix.com>
      1  Heiko Carstens <hca@linux.ibm.com>
      1  Alex Mastro <amastro@fb.com>

===== Patch list in this time period =====


===== Patch Commit Messages ====

New:  x86/bhi: x86/vmscape: Move LFENCE out of
[PATCH v5 1/9] x86/bhi: x86/vmscape: Move LFENCE out of
Author: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>

Currently, BHB clearing sequence is followed by an LFENCE to prevent
transient execution of subsequent indirect branches prematurely. However,
LFENCE barrier could be unnecessary in certain cases. For example, when
kernel is using BHI_DIS_S mitigation, and BHB clearing is only needed for
userspace. In such cases, LFENCE is redundant because ring transitions
would provide the necessary serialization.

Below is a quick recap of BHI mitigation options:

  On Alder Lake and newer

  - BHI_DIS_S: Hardware control to mitigate BHI in ring0. This has low
	       performance overhead.
  - Long loop: Alternatively, longer version of BHB clearing sequence
	       can be used to mitigate BHI. It can also be used to mitigate
	       BHI variant of VMSCAPE. This is not yet implemented in
	       Linux.

  On older CPUs

  - Short loop: Clears BHB at kernel entry and VMexit. The "Long loop" is
		effective on older CPUs as well, but should be avoided
		because of unnecessary overhead.

On Alder Lake and newer CPUs, eIBRS isolates the indirect targets between
guest and host. But when affected by the BHI variant of VMSCAPE, a guest's
branch history may still influence indirect branches in userspace. This
also means the big hammer IBPB could be replaced with a cheaper option that
clears the BHB at exit-to-userspace after a VMexit.

In preparation for adding the support for BHB sequence (without LFENCE) on
newer CPUs, move the LFENCE to the caller side after clear_bhb_loop() is
executed. This allows callers to decide whether they need the LFENCE or
not. This does adds a few extra bytes to the call sites, but it obviates
the need for multiple variants of clear_bhb_loop().

Suggested-by: Dave Hansen <dave.hansen@linux.intel.com>
Reviewed-by: Nikolay Borisov <nik.borisov@suse.com>
Signed-off-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
---
 arch/x86/entry/entry_64.S            | 5 ++++-
 arch/x86/include/asm/nospec-branch.h | 4 ++--
 arch/x86/net/bpf_jit_comp.c          | 2 ++
 3 files changed, 8 insertions(+), 3 deletions(-)

----------------------------------------------------------------------

New:  VMSCAPE optimization for BHI variant
[PATCH v5 0/9] VMSCAPE optimization for BHI variant
Author: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>

v5:
- For BHI seq, limit runtime-patching to loop counts only (Dave).
  Dropped 2 patches that moved the BHB seq to a macro.
- Remove redundant switch cases in vmscape_select_mitigation() (Nikolay).
- Improve commit message (Nikolay).
- Collected tags.

Thank you Dave and Nikolay for great suggestions and the review.

v4: https://lore.kernel.org/r/20251119-vmscape-bhb-v4-0-1adad4e69ddc@linux.intel.com
- Move LFENCE to the callsite, out of clear_bhb_loop(). (Dave)
- Make clear_bhb_loop() work for larger BHB. (Dave)
  This now uses hardware enumeration to determine the BHB size to clear.
- Use write_ibpb() instead of indirect_branch_prediction_barrier() when
  IBPB is known to be available. (Dave)
- Use static_call() to simplify mitigation at exit-to-userspace. (Dave)
- Refactor vmscape_select_mitigation(). (Dave)
- Fix vmscape=on which was wrongly behaving as AUTO. (Dave)
- Split the patches. (Dave)
  - Patch 1-4 prepares for making the sequence flexible for VMSCAPE use.
  - Patch 5 trivial rename of variable.
  - Patch 6-8 prepares for deploying BHB mitigation for VMSCAPE.
  - Patch 9 deploys the mitigation.
  - Patch 10-11 fixes ON Vs AUTO mode.

v3: https://lore.kernel.org/r/20251027-vmscape-bhb-v3-0-5793c2534e93@linux.intel.com
- s/x86_pred_flush_pending/x86_predictor_flush_exit_to_user/ (Sean).
- Removed IBPB & BHB-clear mutual exclusion at exit-to-userspace.
- Collected tags.

v2: https://lore.kernel.org/r/20251015-vmscape-bhb-v2-0-91cbdd9c3a96@linux.intel.com
- Added check for IBPB feature in vmscape_select_mitigation(). (David)
- s/vmscape=auto/vmscape=on/ (David)
- Added patch to remove LFENCE from VMSCAPE BHB-clear sequence.
- Rebased to v6.18-rc1.

v1: https://lore.kernel.org/r/20250924-vmscape-bhb-v1-0-da51f0e1934d@linux.intel.com

Hi All,

These patches aim to improve the performance of a recent mitigation for
VMSCAPE[1] vulnerability. This improvement is relevant for BHI variant of
VMSCAPE that affect Alder Lake and newer processors.

The current mitigation approach uses IBPB on kvm-exit-to-userspace for all
affected range of CPUs. This is an overkill for CPUs that are only affected
by the BHI variant. On such CPUs clearing the branch history is sufficient
for VMSCAPE, and also more apt as the underlying issue is due to poisoned
branch history.

Below is the iPerf data for transfer between guest and host, comparing IBPB
and BHB-clear mitigation. BHB-clear shows performance improvement over IBPB
in most cases.

Platform: Emerald Rapids
Baseline: vmscape=off
Target: IBPB at VMexit-to-userspace Vs the new BHB-clear at
	VMexit-to-userspace mitigation (both compared against baseline).

(pN = N parallel connections)

| iPerf user-net | IBPB    | BHB Clear |
|----------------|---------|-----------|
| UDP 1-vCPU_p1  | -12.5%  |   1.3%    |
| TCP 1-vCPU_p1  | -10.4%  |  -1.5%    |
| TCP 1-vCPU_p1  | -7.5%   |  -3.0%    |
| UDP 4-vCPU_p16 | -3.7%   |  -3.7%    |
| TCP 4-vCPU_p4  | -2.9%   |  -1.4%    |
| UDP 4-vCPU_p4  | -0.6%   |   0.0%    |
| TCP 4-vCPU_p4  |  3.5%   |   0.0%    |

| iPerf bridge-net | IBPB    | BHB Clear |
|------------------|---------|-----------|
| UDP 1-vCPU_p1    | -9.4%   |  -0.4%    |
| TCP 1-vCPU_p1    | -3.9%   |  -0.5%    |
| UDP 4-vCPU_p16   | -2.2%   |  -3.8%    |
| TCP 4-vCPU_p4    | -1.0%   |  -1.0%    |
| TCP 4-vCPU_p4    |  0.5%   |   0.5%    |
| UDP 4-vCPU_p4    |  0.0%   |   0.9%    |
| TCP 1-vCPU_p1    |  0.0%   |   0.9%    |

| iPerf vhost-net | IBPB    | BHB Clear |
|-----------------|---------|-----------|
| UDP 1-vCPU_p1   | -4.3%   |   1.0%    |
| TCP 1-vCPU_p1   | -3.8%   |  -0.5%    |
| TCP 1-vCPU_p1   | -2.7%   |  -0.7%    |
| UDP 4-vCPU_p16  | -0.7%   |  -2.2%    |
| TCP 4-vCPU_p4   | -0.4%   |   0.8%    |
| UDP 4-vCPU_p4   |  0.4%   |  -0.7%    |
| TCP 4-vCPU_p4   |  0.0%   |   0.6%    |

[1] https://comsec.ethz.ch/research/microarch/vmscape-exposing-and-exploiting-incomplete-branch-predictor-isolation-in-cloud-environments/

---
Pawan Gupta (9):
      x86/bhi: x86/vmscape: Move LFENCE out of clear_bhb_loop()
      x86/bhi: Make clear_bhb_loop() effective on newer CPUs
      x86/vmscape: Rename x86_ibpb_exit_to_user to x86_predictor_flush_exit_to_user
      x86/vmscape: Move mitigation selection to a switch()
      x86/vmscape: Use write_ibpb() instead of indirect_branch_prediction_barrier()
      x86/vmscape: Use static_call() for predictor flush
      x86/vmscape: Deploy BHB clearing mitigation
      x86/vmscape: Fix conflicting attack-vector controls with =force
      x86/vmscape: Add cmdline vmscape=on to override attack vector controls

 Documentation/admin-guide/hw-vuln/vmscape.rst   |  8 ++++
 Documentation/admin-guide/kernel-parameters.txt |  4 +-
 arch/x86/Kconfig                                |  1 +
 arch/x86/entry/entry_64.S                       | 13 +++--
 arch/x86/include/asm/cpufeatures.h              |  2 +-
 arch/x86/include/asm/entry-common.h             |  9 ++--
 arch/x86/include/asm/nospec-branch.h            | 11 +++--
 arch/x86/kernel/cpu/bugs.c                      | 64 +++++++++++++++++++------
 arch/x86/kvm/x86.c                              |  4 +-
 arch/x86/net/bpf_jit_comp.c                     |  2 +
 10 files changed, 89 insertions(+), 29 deletions(-)

----------------------------------------------------------------------

New:  liveupdate: luo_flb: Prevent retrieve() after finish()
[PATCH 01/21] liveupdate: luo_flb: Prevent retrieve() after finish()
Author: David Matlack <dmatlack@google.com>

Prevent an incoming FLB from being retrieved after its finish() callback
has been run. An incoming FLB should only retrieved once, and once its
finish() callback has run the data has been freed and cannot be (and
should not be) retrieved again.

Fixes: e8c57e582167 ("liveupdate: luo_flb: Introduce File-Lifecycle-Bound global state")
Signed-off-by: David Matlack <dmatlack@google.com>
---
 include/linux/liveupdate.h  | 3 +++
 kernel/liveupdate/luo_flb.c | 4 ++++
 2 files changed, 7 insertions(+)

----------------------------------------------------------------------

New:  vfio/pci: Base support to preserve a VFIO device file
[PATCH 00/21] vfio/pci: Base support to preserve a VFIO device file
Author: David Matlack <dmatlack@google.com>

This series adds the base support to preserve a VFIO device file across
a Live Update. "Base support" means that this allows userspace to
safetly preserve a VFIO device file with LIVEUPDATE_SESSION_PRESERVE_FD
and retrieve a preserved VFIO device file with
LIVEUPDATE_SESSION_RETRIEVE_FD, but the device itself is not preserved
in a fully running state across Live Update.

This series unblocks 2 parallel but related streams of work:

 - iommufd preservation across Live Update. This work spans iommufd,
   the IOMMU subsystem, and IOMMU drivers [1]

 - Preservation of VFIO device state across Live Update (config space,
   BAR addresses, power state, SR-IOV state, etc.). This work spans both
   VFIO and the core PCI subsystem.

While we need all of the above to fully preserve a VFIO device across a
Live Update without disrupting the workload on the device, this series
aims to be functional and safe enough to merge as the first incremental
step toward that goal.

Areas for Discussion
--------------------

BDF Stability across Live Update

  The PCI support for tracking preserved devices across a Live Update to
  prevent auto-probing relies on PCI segment numbers and BDFs remaining
  stable. For now I have disallowed VFs, as the BDFs assigned to VFs can
  vary depending on how the kernel chooses to allocate bus numbers. For
  non-VFs I am wondering if there is any more needed to ensure BDF
  stability across Live Update.

  While we would like to support many different systems and
  configurations in due time (including preserving VFs), I'd like to
  keep this first serses constrained to simple use-cases.

FLB Locking

  I don't see a way to properly synchronize pci_flb_finish() with
  pci_liveupdate_incoming_is_preserved() since the incoming FLB mutex is
  dropped by liveupdate_flb_get_incoming() when it returns the pointer
  to the object, and taking pci_flb_incoming_lock in pci_flb_finish()
  could result in a deadlock due to reversing the lock ordering.

FLB Retrieving

  The first patch of this series includes a fix to prevent an FLB from
  being retrieved again it is finished. I am wondering if this is the
  right approach or if subsystems are expected to stop calling
  liveupdate_flb_get_incoming() after an FLB is finished.

Testing
-------

The patches at the end of this series provide comprehensive selftests
for the new code added by this series. The selftests have been validated
in both a VM environment using a virtio-net PCIe device, and in a
baremetal environment on an Intel EMR server with an Intel DSA device.

Here is an example of how to run the new selftests:

vfio_pci_liveupdate_uapi_test:

  $ tools/testing/selftests/vfio/scripts/setup.sh 0000:00:04.0
  $ tools/testing/selftests/vfio/vfio_pci_liveupdate_uapi_test 0000:00:04.0
  $ tools/testing/selftests/vfio/scripts/cleanup.sh

vfio_pci_liveupdate_kexec_test:

  $ tools/testing/selftests/vfio/scripts/setup.sh 0000:00:04.0
  $ tools/testing/selftests/vfio/vfio_pci_liveupdate_kexec_test --stage 1 0000:00:04.0
  $ kexec [...]  # NOTE: distro-dependent

  $ tools/testing/selftests/vfio/scripts/setup.sh 0000:00:04.0
  $ tools/testing/selftests/vfio/vfio_pci_liveupdate_kexec_test --stage 2 0000:00:04.0
  $ tools/testing/selftests/vfio/scripts/cleanup.sh

Dependencies
------------

This series was constructed on top of several in-flight series and on
top of mm-nonmm-unstable [2].

  +-- This series
  |
  +-- [PATCH v2 00/18] vfio: selftests: Support for multi-device tests
  |    https://lore.kernel.org/kvm/20251112192232.442761-1-dmatlack@google.com/
  |
  +-- [PATCH v3 0/4] vfio: selftests: update DMA mapping tests to use queried IOVA ranges
  |   https://lore.kernel.org/kvm/20251111-iova-ranges-v3-0-7960244642c5@fb.com/
  |
  +-- [PATCH v8 0/2] Live Update: File-Lifecycle-Bound (FLB) State
  |   https://lore.kernel.org/linux-mm/20251125225006.3722394-1-pasha.tatashin@soleen.com/
  |
  +-- [PATCH v8 00/18] Live Update Orchestrator
  |   https://lore.kernel.org/linux-mm/20251125165850.3389713-1-pasha.tatashin@soleen.com/
  |

To simplify checking out the code, this series can be found on GitHub:

  https://github.com/dmatlack/linux/tree/liveupdate/vfio/cdev/v1

Changelog
---------

v1:
 - Rebase series on top of LUOv8 and VFIO selftests improvements
 - Drop commits to preserve config space fields across Live Update.
   These changes require changes to the PCI layer. For exmaple,
   preserving rbars could lead to an inconsistent device state until
   device BARs addresses are preserved across Live Update.
 - Drop commits to preserve Bus Master Enable on the device. There's no
   reason to preserve this until iommufd preservation is fully working.
   Furthermore, preserving Bus Master Enable could lead to memory
   corruption when the device if the device is bound to the default
   identity-map domain after Live Update.
 - Drop commits to preserve saved PCI state. This work is not needed
   until we are ready to preserve the device's config space, and
   requires more thought to make the PCI state data layout ABI-friendly.
 - Add support to skip auto-probing devices that are preserved by VFIO
   to avoid them getting bound to a different driver by the next kernel.
 - Restrict device preservation further (no VFs, no intel-graphics).
 - Various refactoring and small edits to improve readability and
   eliminate code duplication.

rfc: https://lore.kernel.org/kvm/20251018000713.677779-1-vipinsh@google.com/

Cc: Saeed Mahameed <saeedm@nvidia.com>
Cc: Adithya Jayachandran <ajayachandra@nvidia.com>
Cc: Jason Gunthorpe <jgg@nvidia.com>
Cc: Parav Pandit <parav@nvidia.com>
Cc: Leon Romanovsky <leonro@nvidia.com>
Cc: William Tu <witu@nvidia.com>
Cc: Jacob Pan <jacob.pan@linux.microsoft.com>
Cc: Lukas Wunner <lukas@wunner.de>
Cc: Pasha Tatashin <pasha.tatashin@soleen.com>
Cc: Mike Rapoport <rppt@kernel.org>
Cc: Pratyush Yadav <pratyush@kernel.org>
Cc: Samiullah Khawaja <skhawaja@google.com>
Cc: Chris Li <chrisl@kernel.org>
Cc: Josh Hilke <jrhilke@google.com>
Cc: David Rientjes <rientjes@google.com>

[1] https://lore.kernel.org/linux-iommu/20250928190624.3735830-1-skhawaja@google.com/
[2] https://git.kernel.org/pub/scm/linux/kernel/git/akpm/mm.git/log/?h=mm-nonmm-unstable

David Matlack (12):
  liveupdate: luo_flb: Prevent retrieve() after finish()
  PCI: Add API to track PCI devices preserved across Live Update
  PCI: Require driver_override for incoming Live Update preserved
    devices
  vfio/pci: Notify PCI subsystem about devices preserved across Live
    Update
  vfio: Enforce preserved devices are retrieved via
    LIVEUPDATE_SESSION_RETRIEVE_FD
  vfio/pci: Store Live Update state in struct vfio_pci_core_device
  vfio: selftests: Add Makefile support for TEST_GEN_PROGS_EXTENDED
  vfio: selftests: Add vfio_pci_liveupdate_uapi_test
  vfio: selftests: Expose iommu_modes to tests
  vfio: selftests: Expose low-level helper routines for setting up
    struct vfio_pci_device
  vfio: selftests: Verify that opening VFIO device fails during Live
    Update
  vfio: selftests: Add continuous DMA to vfio_pci_liveupdate_kexec_test

Vipin Sharma (9):
  vfio/pci: Register a file handler with Live Update Orchestrator
  vfio/pci: Preserve vfio-pci device files across Live Update
  vfio/pci: Retrieve preserved device files after Live Update
  vfio/pci: Skip reset of preserved device after Live Update
  selftests/liveupdate: Move luo_test_utils.* into a reusable library
  selftests/liveupdate: Add helpers to preserve/retrieve FDs
  vfio: selftests: Build liveupdate library in VFIO selftests
  vfio: selftests: Initialize vfio_pci_device using a VFIO cdev FD
  vfio: selftests: Add vfio_pci_liveupdate_kexec_test

 MAINTAINERS                                   |   1 +
 drivers/pci/Makefile                          |   1 +
 drivers/pci/liveupdate.c                      | 248 ++++++++++++++++
 drivers/pci/pci-driver.c                      |  12 +-
 drivers/vfio/device_cdev.c                    |  25 +-
 drivers/vfio/group.c                          |   9 +
 drivers/vfio/pci/Makefile                     |   1 +
 drivers/vfio/pci/vfio_pci.c                   |  11 +-
 drivers/vfio/pci/vfio_pci_core.c              |  23 +-
 drivers/vfio/pci/vfio_pci_liveupdate.c        | 278 ++++++++++++++++++
 drivers/vfio/pci/vfio_pci_priv.h              |  16 +
 drivers/vfio/vfio.h                           |  13 -
 drivers/vfio/vfio_main.c                      |  22 +-
 include/linux/kho/abi/pci.h                   |  53 ++++
 include/linux/kho/abi/vfio_pci.h              |  45 +++
 include/linux/liveupdate.h                    |   3 +
 include/linux/pci.h                           |  38 +++
 include/linux/vfio.h                          |  51 ++++
 include/linux/vfio_pci_core.h                 |   7 +
 kernel/liveupdate/luo_flb.c                   |   4 +
 tools/testing/selftests/liveupdate/.gitignore |   1 +
 tools/testing/selftests/liveupdate/Makefile   |  14 +-
 .../include/libliveupdate.h}                  |  11 +-
 .../selftests/liveupdate/lib/libliveupdate.mk |  20 ++
 .../{luo_test_utils.c => lib/liveupdate.c}    |  43 ++-
 .../selftests/liveupdate/luo_kexec_simple.c   |   2 +-
 .../selftests/liveupdate/luo_multi_session.c  |   2 +-
 tools/testing/selftests/vfio/Makefile         |  23 +-
 .../vfio/lib/include/libvfio/iommu.h          |   2 +
 .../lib/include/libvfio/vfio_pci_device.h     |   8 +
 tools/testing/selftests/vfio/lib/iommu.c      |   4 +-
 .../selftests/vfio/lib/vfio_pci_device.c      |  60 +++-
 .../vfio/vfio_pci_liveupdate_kexec_test.c     | 255 ++++++++++++++++
 .../vfio/vfio_pci_liveupdate_uapi_test.c      |  93 ++++++
 34 files changed, 1313 insertions(+), 86 deletions(-)

----------------------------------------------------------------------

New:  vfio: refactor vfio_pci_mmap_huge_fault function
[PATCH v8 1/6] vfio: refactor vfio_pci_mmap_huge_fault function
Author: ankita <ankita@nvidia.com>


Refactor vfio_pci_mmap_huge_fault to take out the implementation
to map the VMA to the PTE/PMD/PUD as a separate function.

Export the new function to be used by nvgrace-gpu module.

Move the alignment check code to verify that pfn and VMA VA is
aligned to the page order to the header file and make it inline.

No functional change is intended.

Cc: Shameer Kolothum <skolothumtho@nvidia.com>
Cc: Alex Williamson <alex@shazbot.org>
Cc: Jason Gunthorpe <jgg@ziepe.ca>
Reviewed-by: Shameer Kolothum <skolothumtho@nvidia.com>
Signed-off-by: Ankit Agrawal <ankita@nvidia.com>
---
 drivers/vfio/pci/vfio_pci_core.c | 54 ++++++++++++++++----------------
 include/linux/vfio_pci_core.h    | 13 ++++++++
 2 files changed, 40 insertions(+), 27 deletions(-)

----------------------------------------------------------------------

New:  vfio/nvgrace-gpu: Support huge PFNMAP and wait for GPU ready post reset
[PATCH v8 0/6] vfio/nvgrace-gpu: Support huge PFNMAP and wait for GPU ready post reset
Author: ankita <ankita@nvidia.com>


NVIDIA's Grace based system have large GPU device memory. The device
memory is mapped as VM_PFNMAP in the VMM VMA. The nvgrace-gpu
module could make use of the huge PFNMAP support added in mm [1].

To achieve this, nvgrace-gpu module is updated to implement huge_fault ops.
The implementation establishes mapping according to the order request.
Note that if the PFN or the VMA address is unaligned to the order, the
mapping fallbacks to the PTE level.

Secondly, it is expected that the mapping not be re-established until
the GPU is ready post reset. Presence of the mappings during that time
could potentially leads to harmless corrected RAS events to be logged if
the CPU attempts to do speculative reads on the GPU memory on the Grace
systems.

It can take several seconds for the GPU to be ready. So it is desirable
that the time overlaps as much of the VM startup as possible to reduce
impact on the VM bootup time. The GPU readiness state is thus checked
on the first fault/huge_fault request which amortizes the GPU readiness
time. The GPU readiness is checked through BAR0 registers as is done
at the device probe.

Patch 1 Refactor vfio_pci_mmap_huge_fault and export the code to map
at the various levels.

Patch 2 implements support for huge pfnmap.

Patch 3 vfio_pci_core_mmap cleanup.

Patch 4 split the code to check the device readiness.

Patch 5 reset_done handler implementation

Patch 6 Ensures that the GPU is ready before re-establishing the mapping
after reset.

Applied over 6.18-rc6.

Link: https://lore.kernel.org/all/20240826204353.2228736-1-peterx@redhat.com/ [1]

Changelog:
[v8]
- Fix bug to vfio_pci_core_disable on error path in 6/6
  (Thanks Shameer Kolothum)
- Code cleanup the huge_fault functions in 1/6, 2/6, 4/6, 6/6
  (Thanks Alex Williamson)
- Commit message fix in 5/6, 6/6 (Thanks Alex Williamson)
- Collected Reviewed-by. (Thanks Shameer Kolothum)
Link: https://lore.kernel.org/all/20251126052627.43335-1-ankita@nvidia.com/ [v7]
- Code cleanup to use ALIGN functions and move the mapping sanity check
  to the vfio header file. (Thanks Alex Williamson, Zhi Wang)
- Added comments for reset_done implementation. (Thanks Alex Williamson)
- Collected Reviewed-by. (Thanks Zhi Wang)
- Miscellaneous cleanup (Thanks Alex Williamson, Zhi Wang)
Link: https://lore.kernel.org/all/20251125173013.39511-1-ankita@nvidia.com/ [v6]
- Updated the vfio_pci_vmf_insert_pfn function to add more common code.
  (Thanks Shameer Kolothum)
- Added missing userspace offset for pgoff calculation for huge pfnmap.
  (Thanks Shameer Kolothum)
- Removed alignment for GPU memory. (Thanks Jason Gunthorpe)
- Collected Reviewed-by. (Thanks Shameer Kolothum)
- Miscellaneous cleanup, log messages fixup
  (Thanks Alex Williamson, Jason Gunthorpe, Shameer Kolothum)
Link: https://lore.kernel.org/all/20251124115926.119027-1-ankita@nvidia.com/ [v5]
- Updated gpu_mem_mapped with reset_done flag for clearer semantics. (6/7)
  (Thanks Alex Williamson)
- Renamed vfio_pci_map_pfn to vfio_pci_vmf_insert_pfn. (2/7)
  (Thanks Alex Williamson)
- Updated to hold memory_lock across the vmf_insert_pfn and the
  read/write access of the device. (7/7) (Thanks Alex Williamson)
- Used scoped_guard to simplify critical region. (1/7, 7/7)
[v4]
- Implemented reset_done handler to set gpu_mem_mapped flag. Cleaned up
  FLR detection path (Thanks Alex Williamson)
- Moved the premap check of the device readiness to a new function.
  Added locking to avoid races. (Thanks Alex Williamson)
- vfio_pci_core_mmap cleanup.
- Added ioremap to BAR0 during open.
Link: https://lore.kernel.org/all/20251121141141.3175-1-ankita@nvidia.com/ [v3]
- Moved the code for BAR mapping to a separate function.
- Added BAR0 mapping during open. Ensures BAR0 is mapped when registers
  are checked. (Thanks Alex Williamson, Jason Gunthorpe for suggestion)
- Added check for GPU readiness on nvgrace_gpu_map_device_mem. (Thanks
  Alex Williamson for the suggestion.
Link: https://lore.kernel.org/all/20251118074422.58081-1-ankita@nvidia.com/ [v2]
- Fixed build kernel warning
- subject text changes
- Rebased to 6.18-rc6.
Link: https://lore.kernel.org/all/20251117124159.3560-1-ankita@nvidia.com/ [v1]

Signed-off-by: Ankit Agrawal <ankita@nvidia.com>

Ankit Agrawal (6):
  vfio: refactor vfio_pci_mmap_huge_fault function
  vfio/nvgrace-gpu: Add support for huge pfnmap
  vfio: use vfio_pci_core_setup_barmap to map bar in mmap
  vfio/nvgrace-gpu: split the code to wait for GPU ready
  vfio/nvgrace-gpu: Inform devmem unmapped after reset
  vfio/nvgrace-gpu: wait for the GPU mem to be ready

 drivers/vfio/pci/nvgrace-gpu/main.c | 208 ++++++++++++++++++++++------
 drivers/vfio/pci/vfio_pci_core.c    |  69 ++++-----
 include/linux/vfio_pci_core.h       |  13 ++
 3 files changed, 209 insertions(+), 81 deletions(-)

----------------------------------------------------------------------

New:  x86/paravirt: Replace io_delay() hook with a bool
[PATCH 1/5] x86/paravirt: Replace io_delay() hook with a bool
Author: Juergen Gross <jgross@suse.com>

The io_delay() paravirt hook is in no way performance critical and all
users setting it to a different function than native_io_delay() are
using an empty function as replacement.

This enables to replace the hook with a bool indicating whether
native_io_delay() should be called.

Signed-off-by: Juergen Gross <jgross@suse.com>
---
 arch/x86/include/asm/io.h             |  7 +++++--
 arch/x86/include/asm/paravirt.h       | 11 +----------
 arch/x86/include/asm/paravirt_types.h |  3 +--
 arch/x86/kernel/cpu/vmware.c          |  2 +-
 arch/x86/kernel/kvm.c                 |  8 +-------
 arch/x86/kernel/paravirt.c            |  3 +--
 arch/x86/xen/enlighten_pv.c           |  6 +-----
 7 files changed, 11 insertions(+), 29 deletions(-)

----------------------------------------------------------------------

New:  x86: Cleanups around slow_down_io()
[PATCH 0/5] x86: Cleanups around slow_down_io()
Author: Juergen Gross <jgross@suse.com>

While looking at paravirt cleanups I stumbled over slow_down_io() and
the related REALLY_SLOW_IO define.

Especially REALLY_SLOW_IO is a mess, which is proven by 2 completely
wrong use cases.

Do several cleanups, resulting in a deletion of REALLY_SLOW_IO and the
io_delay() paravirt function hook.

Patches 2 and 3 are not changing any functionality, but maybe they
should? As the potential bug has been present for more than a decade
now, I went with just deleting the useless "#define REALLY_SLOW_IO".
The alternative would be to do something similar as in patch 5.

Juergen Gross (5):
  x86/paravirt: Replace io_delay() hook with a bool
  hwmon/lm78: Drop REALLY_SLOW_IO setting
  hwmon/w83781d: Drop REALLY_SLOW_IO setting
  block/floppy: Don't use REALLY_SLOW_IO for delays
  x86/io: Remove REALLY_SLOW_IO handling

 arch/x86/include/asm/floppy.h         | 27 ++++++++++++++++++++++-----
 arch/x86/include/asm/io.h             | 12 +++++-------
 arch/x86/include/asm/paravirt.h       | 11 +----------
 arch/x86/include/asm/paravirt_types.h |  3 +--
 arch/x86/kernel/cpu/vmware.c          |  2 +-
 arch/x86/kernel/kvm.c                 |  8 +-------
 arch/x86/kernel/paravirt.c            |  3 +--
 arch/x86/xen/enlighten_pv.c           |  6 +-----
 drivers/block/floppy.c                |  2 --
 drivers/hwmon/lm78.c                  |  5 +++--
 drivers/hwmon/w83781d.c               |  5 +++--
 11 files changed, 39 insertions(+), 45 deletions(-)

----------------------------------------------------------------------

New:  KVM: arm64: Add support for FEAT_IDST
[PATCH v2 0/5] KVM: arm64: Add support for FEAT_IDST
Author: Marc Zyngier <maz@kernel.org>

FEAT_IDST appeared in ARMv8.4, and allows ID registers to be trapped
if they are not implemented. This only concerns 3 registers (GMID_EL1,
CCSIDR2_EL1 and SMIDR_EL1), which are part of features that may not be
exposed to the guest even if present on the host.

For these registers, the HW should report them with EC=0x18, even if
the feature isn't implemented.

Add support for this feature by handling these registers in a specific
way and implementing GMID_EL1 support in the process. A very basic
selftest checks that these registers behave as expected.

* From v1: [1]

  - Fixed commit message in patch #4 (Ben)
  - Collected RB, with thanks (Joey)

[1] https://lore.kernel.org/r/20251120133202.2037803-1-maz@kernel.org

Marc Zyngier (5):
  KVM: arm64: Add routing/handling for GMID_EL1
  KVM: arm64: Force trap of GMID_EL1 when the guest doesn't have MTE
  KVM: arm64: Add a generic synchronous exception injection primitive
  KVM: arm64: Report optional ID register traps with a 0x18 syndrome
  KVM: arm64: selftests: Add a test for FEAT_IDST

 arch/arm64/include/asm/kvm_emulate.h          |   1 +
 arch/arm64/kvm/emulate-nested.c               |   8 ++
 arch/arm64/kvm/inject_fault.c                 |  10 +-
 arch/arm64/kvm/sys_regs.c                     |  17 ++-
 tools/testing/selftests/kvm/Makefile.kvm      |   1 +
 .../testing/selftests/kvm/arm64/idreg-idst.c  | 117 ++++++++++++++++++
 6 files changed, 149 insertions(+), 5 deletions(-)

----------------------------------------------------------------------

New:  KVM: arm64: Add routing/handling for GMID_EL1
[PATCH v2 1/5] KVM: arm64: Add routing/handling for GMID_EL1
Author: Marc Zyngier <maz@kernel.org>

HCR_EL2.TID5 is currently ignored by the trap routing infrastructure,
and we currently don't handle GMID_EL1 either (the only register trapped
by TID5).

Wire both the trap bit and a default UNDEF handler.

Signed-off-by: Marc Zyngier <maz@kernel.org>
---
 arch/arm64/kvm/emulate-nested.c | 8 ++++++++
 arch/arm64/kvm/sys_regs.c       | 1 +
 2 files changed, 9 insertions(+)

----------------------------------------------------------------------

New:  vhost/vsock: improve RCU read sections around vhost_vsock_get()
[PATCH] vhost/vsock: improve RCU read sections around vhost_vsock_get()
Author: Stefano Garzarella <sgarzare@redhat.com>


vhost_vsock_get() uses hash_for_each_possible_rcu() to find the
`vhost_vsock` associated with the `guest_cid`. hash_for_each_possible_rcu()
should only be called within an RCU read section, as mentioned in the
following comment in include/linux/rculist.h:

/**
 * hlist_for_each_entry_rcu - iterate over rcu list of given type
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the hlist_node within the struct.
 * @cond:	optional lockdep expression if called from non-RCU protection.
 *
 * This list-traversal primitive may safely run concurrently with
 * the _rcu list-mutation primitives such as hlist_add_head_rcu()
 * as long as the traversal is guarded by rcu_read_lock().
 */

Currently, all calls to vhost_vsock_get() are between rcu_read_lock()
and rcu_read_unlock() except for calls in vhost_vsock_set_cid() and
vhost_vsock_reset_orphans(). In both cases, the current code is safe,
but we can make improvements to make it more robust.

About vhost_vsock_set_cid(), when building the kernel with
CONFIG_PROVE_RCU_LIST enabled, we get the following RCU warning when the
user space issues `ioctl(dev, VHOST_VSOCK_SET_GUEST_CID, ...)` :

  WARNING: suspicious RCU usage
  6.18.0-rc7 #62 Not tainted
  -----------------------------
  drivers/vhost/vsock.c:74 RCU-list traversed in non-reader section!!

  other info that might help us debug this:

  rcu_scheduler_active = 2, debug_locks = 1
  1 lock held by rpc-libvirtd/3443:
   #0: ffffffffc05032a8 (vhost_vsock_mutex){+.+.}-{4:4}, at: vhost_vsock_dev_ioctl+0x2ff/0x530 [vhost_vsock]

  stack backtrace:
  CPU: 2 UID: 0 PID: 3443 Comm: rpc-libvirtd Not tainted 6.18.0-rc7 #62 PREEMPT(none)
  Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.17.0-7.fc42 06/10/2025
  Call Trace:
   <TASK>
   dump_stack_lvl+0x75/0xb0
   dump_stack+0x14/0x1a
   lockdep_rcu_suspicious.cold+0x4e/0x97
   vhost_vsock_get+0x8f/0xa0 [vhost_vsock]
   vhost_vsock_dev_ioctl+0x307/0x530 [vhost_vsock]
   __x64_sys_ioctl+0x4f2/0xa00
   x64_sys_call+0xed0/0x1da0
   do_syscall_64+0x73/0xfa0
   entry_SYSCALL_64_after_hwframe+0x76/0x7e
   ...
   </TASK>

This is not a real problem, because the vhost_vsock_get() caller, i.e.
vhost_vsock_set_cid(), holds the `vhost_vsock_mutex` used by the hash
table writers. Anyway, to prevent that warning, add lockdep_is_held()
condition to hash_for_each_possible_rcu() to verify that either the
caller is in an RCU read section or `vhost_vsock_mutex` is held when
CONFIG_PROVE_RCU_LIST is enabled; and also clarify the comment for
vhost_vsock_get() to better describe the locking requirements and the
scope of the returned pointer validity.

About vhost_vsock_reset_orphans(), currently this function is only
called via vsock_for_each_connected_socket(), which holds the
`vsock_table_lock` spinlock (which is also an RCU read-side critical
section). However, add an explicit RCU read lock there to make the code
more robust and explicit about the RCU requirements, and to prevent
issues if the calling context changes in the future or if
vhost_vsock_reset_orphans() is called from other contexts.

Fixes: 834e772c8db0 ("vhost/vsock: fix use-after-free in network stack callers")
Cc: stefanha@redhat.com
Signed-off-by: Stefano Garzarella <sgarzare@redhat.com>
---
 drivers/vhost/vsock.c | 15 +++++++++++----
 1 file changed, 11 insertions(+), 4 deletions(-)

----------------------------------------------------------------------

New:  x86/split_lock: Don't try to handle user split lock in TDX guest
[PATCH 1/2] x86/split_lock: Don't try to handle user split lock in TDX guest
Author: Xiaoyao Li <xiaoyao.li@intel.com>

When the host enables split lock detection feature, the split lock from
guests (normal or TDX) triggers #AC. The #AC caused by split lock access
within a normal guest triggers a VM Exit and is handled in the host.
The #AC caused by split lock access within a TDX guest does not trigger
a VM Exit and instead it's delivered to the guest self.

The default "warning" mode of handling split lock depends on being able
to temporarily disable detection to recover from the split lock event.
But the MSR that disables detection is not accessible to a guest.

This means that TDX guests today can not disable the feature or use
the "warning" mode (which is the default). But, they can use the "fatal"
mode.

Force TDX guests to use the "fatal" mode.

Signed-off-by: Xiaoyao Li <xiaoyao.li@intel.com>
---
 arch/x86/kernel/cpu/bus_lock.c | 17 ++++++++++++++++-
 1 file changed, 16 insertions(+), 1 deletion(-)

----------------------------------------------------------------------

New:  x86/split_lock: Fix and enhancement for TDX guest
[PATCH 0/2] x86/split_lock: Fix and enhancement for TDX guest
Author: Xiaoyao Li <xiaoyao.li@intel.com>

Running a split lock test[1] inside a TDX guest under KVM triggers the
warning below. The test hangs but can be terminated.

  x86/split lock detection: #AC: split_lock/1176 took a split_lock trap at address: 0x5630b30921f9
  unchecked MSR access error: WRMSR to 0x33 (tried to write 0x0000000000000000) at rIP: 0xffffffff812a061f (native_write_msr+0xf/0x30)
  Call Trace:
  handle_user_split_lock
  exc_alignment_check
  asm_exc_alignment_check

It turns out that split lock detection is enabled (by the host) when the
TDX vCPU is running, and #AC is not intercepted but delivered directly to
the TDX guest. The default "warning" mode of split lock #AC handler in
the guest tries to handle the split lock by temporarily disabling
detection. However, the MSR that disables detection is not accessible to
a guest.

Patch 1 forces the TDX guest to always treat the split lock #AC as the
"fatal" mode. This prevents the TDX guest from attempting invalid MSR
writes.

Patch 2 enhances the sld_state_show() to indicate that the TDX guest can
receive #AC on split locks depending on the host's split lock detection
configuration.

Note that all the split lock issues on TDX guests are due to the
non-architectural behavior of TDX: a TDX guest can receive #AC even
though the split lock detection feature is not available and the
relevant MSR is not accessible.

One option is to make the behavior architectural for TDX guests by not
delivering the (unexpected) #AC to the TDX guest and letting the host
handle it instead. This is exactly how KVM handles split lock #AC for
normal VMs. This option also has the advantage that the TDX guest can
survive from split locks when the host mode is not fatal.

However, this option cannot replace current patches because it changes
the behavior of current TDX and would need to be opted-in by the host VMM
for compatibility. More importantly, it would be a new feature available
only in newer TDX modules, which means all existing TDX modules
cannot benefit from it.

We list the option here as an open to solicit feedback and determine
whether to pursue adding such feature to TDX module.

[1] https://github.com/xiaoyaoli-intel/splitlock/blob/main/splitlock.c

Xiaoyao Li (2):
  x86/split_lock: Don't try to handle user split lock in TDX guest
  x86/split_lock: Describe #AC handling in TDX guest kernel log

 arch/x86/kernel/cpu/bus_lock.c | 20 +++++++++++++++++++-
 1 file changed, 19 insertions(+), 1 deletion(-)

----------------------------------------------------------------------

New:  target/i386/kvm: Configure proper KVM SEOIB behavior
[PATCH] target/i386/kvm: Configure proper KVM SEOIB behavior
Author: Khushit Shah <khushit.shah@nutanix.com>

Historically, KVM always advertised x2APIC Suppress EOI Broadcast
(SEOIB) support in split-irqchip mode, This is incorrect for userspace
IOAPIC implementations without an EOI register (e.g. version 0x11).
Furthermore, KVM did not actually honor guest suppression requests and
continued to broadcast LAPIC EOIs to userspace IOAPIC. This can cause
interrupt storms in guests that rely on Directed EOI semantics
(notably Windows with Credential Guard, which experiences boot hangs).

KVM is adding two new x2APIC API flags to control this behavior:
  - KVM_X2APIC_API_DISABLE_IGNORE_SUPPRESS_EOI_BROADCAST_QUIRK
  - KVM_X2APIC_API_DISABLE_SUPPRESS_EOI_BROADCAST
[https://patchwork.kernel.org/project/kvm/patch/20251125180557.2022311-1-khushit.shah@nutanix.com/]

Wire those flags into QEMU via a new machine-level state variable
(kvm_lapic_seoib_state), which models three possible policies:

  - SEOIB_STATE_QUIRKED:
        Legacy behavior. SEOIB advertised but LAPIC EOIs are
        broadcasted even when guest turns on SEOIB. This is the default
        for backward compatibility.

  - SEOIB_STATE_RESPECTED:
        SEOIB advertised and suppression honored.

  - SEOIB_STATE_NOT_ADVERTISED:
        SEOIB not advertised (required for IOAPIC v0x11).

For new VMs using split-irqchip, QEMU selects a policy based on the
userspace IOAPIC version and programs KVM accordingly during
x86_cpus_init(). If KVM does not support the new API, QEMU falls back
to the quirked behavior with a warning.

SEOIB state is migrated only when non-quirked. Legacy VMs remain in QUIRKED
mode and behave exactly as before. Older VMs that migrate into a newer
QEMU version will also be able to migrate back to an older QEMU version,
as they always stay in the QUIRKED state. VMs powered on with new QEMU and
a new kernel that use a non-quirked SEOIB state will not be able to migrate
to older QEMU versions or older kernels. The state is applied on the
destination in x86_seoib_post_load() to ensure correct KVM configuration
before VM execution resumes.

Additional changes:
  - Add qemu_will_load_snapshot() to detect loadvm scenarios
  - Move IOAPIC_VER_DEF to header for use in x86-common.c
  - Add get_ioapic_version_from_globals() helper
  - Add trace events (kvm_lapic_seoib_*) for debugging

Signed-off-by: Khushit Shah <khushit.shah@nutanix.com>
---
 hw/i386/x86-common.c         | 98 ++++++++++++++++++++++++++++++++++++
 hw/i386/x86.c                |  1 +
 hw/intc/ioapic.c             |  2 -
 include/hw/i386/x86.h        | 12 +++++
 include/hw/intc/ioapic.h     |  2 +
 include/system/system.h      |  1 +
 system/vl.c                  |  5 ++
 target/i386/kvm/kvm.c        | 46 +++++++++++++++++
 target/i386/kvm/kvm_i386.h   | 12 +++++
 target/i386/kvm/trace-events |  4 ++
 10 files changed, 181 insertions(+), 2 deletions(-)

----------------------------------------------------------------------

New:  tun/tap & vhost-net: netdev queue flow
[PATCH net-next v6 0/8] tun/tap & vhost-net: netdev queue flow
Author: Simon Schippers <simon.schippers@tu-dortmund.de>


----------------------------------------------------------------------

New:  KVM: s390: Add signal_exits counter
[PATCH v2 1/3] KVM: s390: Add signal_exits counter
Author: Andrew Donnellan <ajd@linux.ibm.com>

Add a signal_exits counter for s390, as exists on arm64, loongarch, mips,
powerpc, riscv and x86.

This is used by kvm_handle_signal_exit(), which we will use when we
later enable CONFIG_VIRT_XFER_TO_GUEST_WORK.

Signed-off-by: Andrew Donnellan <ajd@linux.ibm.com>
---
 arch/s390/include/asm/kvm_host.h | 1 +
 arch/s390/kvm/kvm-s390.c         | 4 +++-
 2 files changed, 4 insertions(+), 1 deletion(-)

----------------------------------------------------------------------

New:  KVM: s390: Use generic VIRT_XFER_TO_GUEST_WORK
[PATCH v2 0/3] KVM: s390: Use generic VIRT_XFER_TO_GUEST_WORK
Author: Andrew Donnellan <ajd@linux.ibm.com>

This series enables VIRT_XFER_TO_GUEST_WORK on s390.

This requires:

  1) adding a signal_exits stats counter, which is used by
     kvm_handle_signal_exit()
  2) moving the point where interrupts are enabled and disabled in the
     guest entry path, so that interrupts aren't enabled until after the
     __TI_sie flag is set
  3) enabling VIRT_XFER_TO_GUEST_WORK and adding the appropriate calls to
     check for and handle outstanding work in __vcpu_run() and the VSIE
     path.

With this series applied, the kvm-unit-tests suite passes on both the host
and an L1 guest with nested KVM enabled, and benchmarks done using the
exittime tests from kvm-unit-tests show that the impact on entry path
performance is generally small enough to be noise (in my tests, around
+/-3%, running directly in an LPAR and in a L1 KVM guest).

Thanks to Heiko for feedback and guidance on this.

Signed-off-by: Andrew Donnellan <ajd@linux.ibm.com>
---
Changes in v2:
- if work is handled, recheck for outstanding work with interrupts
  disabled before entering guest (Heiko)
- Link to v1: https://lore.kernel.org/r/20251125-s390-kvm-xfer-to-guest-work-v1-0-091281a34611@linux.ibm.com

---
Andrew Donnellan (2):
      KVM: s390: Add signal_exits counter
      KVM: s390: Use generic VIRT_XFER_TO_GUEST_WORK functions

Heiko Carstens (1):
      KVM: s390: Enable and disable interrupts in entry code

 arch/s390/include/asm/kvm_host.h   |  1 +
 arch/s390/include/asm/stacktrace.h |  1 +
 arch/s390/kernel/asm-offsets.c     |  1 +
 arch/s390/kernel/entry.S           |  2 ++
 arch/s390/kvm/Kconfig              |  1 +
 arch/s390/kvm/kvm-s390.c           | 34 +++++++++++++++++++++-------------
 arch/s390/kvm/vsie.c               | 18 +++++++++++++-----
 7 files changed, 40 insertions(+), 18 deletions(-)

----------------------------------------------------------------------

Exist: [PATCH v8 1/6] vfio: refactor vfio_pci_mmap_huge_fault function
 Skip: [PATCH v7 1/6] vfio: refactor vfio_pci_mmap_huge_fault function
Exist: [PATCH v8 0/6] vfio/nvgrace-gpu: Support huge PFNMAP and wait for GPU ready post reset
 Skip: [PATCH v7 0/6] vfio/nvgrace-gpu: Support huge PFNMAP and wait for GPU ready post reset
New:  dma-buf: fix integer overflow in fill_sg_entry() for
[PATCH] dma-buf: fix integer overflow in fill_sg_entry() for
Author: Alex Mastro <amastro@fb.com>

fill_sg_entry() splits large DMA buffers into multiple scatter-gather
entries, each holding up to UINT_MAX bytes. When calculating the DMA
address for entries beyond the second one, the expression (i * UINT_MAX)
causes integer overflow due to 32-bit arithmetic.

This manifests when the input arg length >= 8 GiB results in looping for
i >= 2.

Fix by casting i to dma_addr_t before multiplication.

Fixes: 3aa31a8bb11e ("dma-buf: provide phys_vec to scatter-gather mapping routine")
Signed-off-by: Alex Mastro <amastro@fb.com>
---
More color about how I discovered this in [1] for the commit at [2]:

[1] https://lore.kernel.org/all/aSZHO6otK0Heh+Qj@devgpu015.cco6.facebook.com
[2] https://lore.kernel.org/all/20251120-dmabuf-vfio-v9-6-d7f71607f371@nvidia.com
---
 drivers/dma-buf/dma-buf-mapping.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

----------------------------------------------------------------------

