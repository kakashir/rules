From 4ac921739 to 0ce085f8c
KVM mailing list update from 4ac921739 to 0ce085f8c

Top 15 contributor Email domains (Based on Email Body)

      7 nvidia.com
      5 google.com
      1 redhat.com
      1 linux.alibaba.com
      1 amazon.co.uk

Top 15 contributors (Based on Email Body)

      6  Nicolin Chen <nicolinc@nvidia.com>
      5  Sean Christopherson <seanjc@google.com>
      1  Ryosuke Yasuoka <ryasuoka@redhat.com>
      1  Fred Griffoul <fgriffo@amazon.co.uk>
      1  Fangyu Yu <fangyu.yu@linux.alibaba.com>
      1  Ankit Agrawal <ankita@nvidia.com>

===== Patch list in this time period =====


===== Patch Commit Messages ====

New:  x86/kvm: Avoid freeing stack-allocated node in kvm_async_pf_queue_task
[PATCH] x86/kvm: Avoid freeing stack-allocated node in kvm_async_pf_queue_task
Author: Ryosuke Yasuoka <ryasuoka@redhat.com>

kvm_async_pf_queue_task() can incorrectly remove a node allocated on the
stack of kvm_async_pf_task_wait_schedule(). This occurs when a task
request a PF while another task's PF request with the same token is
still pending. Currently, kvm_async_pf_queue_task() assumes that any
entry in the list is a dummy entry and tries to kfree(). To fix this,
add a dummy flag to the node structure and the function should check
this flag and kfree() only if it is a dummy entry.

Signed-off-by: Ryosuke Yasuoka <ryasuoka@redhat.com>
---
 arch/x86/kernel/kvm.c | 13 +++++++++++--
 1 file changed, 11 insertions(+), 2 deletions(-)

----------------------------------------------------------------------

New:  RISC-V: KVM: Allow to downgrade HGATP mode via SATP mode
[PATCH] RISC-V: KVM: Allow to downgrade HGATP mode via SATP mode
Author: fangyu.yu <fangyu.yu@linux.alibaba.com>


Currently, HGATP mode uses the maximum value detected by the hardware
but often such a wide GPA is unnecessary, just as a host sometimes
doesn't need sv57.
It's likely that no additional parameters (like no5lvl and no4lvl) are
needed, aligning HGATP mode to SATP mode should meet the requirements
of most scenarios.

Signed-off-by: Fangyu Yu <fangyu.yu@linux.alibaba.com>
---
 arch/riscv/kvm/gstage.c | 7 ++++---
 1 file changed, 4 insertions(+), 3 deletions(-)

----------------------------------------------------------------------

New:  Disable ATS via iommu during PCI resets
[PATCH v7 0/5] Disable ATS via iommu during PCI resets
Author: Nicolin Chen <nicolinc@nvidia.com>

Hi all,

PCIe permits a device to ignore ATS invalidation TLPs while processing a
reset. This creates a problem visible to the OS where an ATS invalidation
command will time out: e.g. an SVA domain will have no coordination with a
reset event and can racily issue ATS invalidations to a resetting device.

The OS should do something to mitigate this as we do not want production
systems to be reporting critical ATS failures, especially in a hypervisor
environment. Broadly, OS could arrange to ignore the timeouts, block page
table mutations to prevent invalidations, or disable and block ATS.

The PCIe spec in sec 10.3.1 IMPLEMENTATION NOTE recommends to disable and
block ATS before initiating a Function Level Reset. It also mentions that
other reset methods could have the same vulnerability as well.

Provide a callback from the PCI subsystem that will enclose the reset and
have the iommu core temporarily change domains to group->blocking_domain,
so IOMMU drivers would fence any incoming ATS queries, synchronously stop
issuing new ATS invalidations, and wait for existing ATS invalidations to
complete. Doing this can avoid any ATS invaliation timeouts.

When a device is resetting, any new domain attachment has to be rejected,
until the reset is finished, to prevent ATS activity from being activated
between the two callback functions. Introduce a new resetting_domain, and
reject a concurrent __iommu_attach_device/set_group_pasid().

Finally, call these pci_dev_reset_iommu/done() functions in the PCI reset
functions.

This is on Github:
https://github.com/nicolinc/iommufd/commits/iommu_dev_reset-v7

Changelog
v7
 * Rebase on Joerg's next tree
 * Add Reviewed-by from Kevin
 * [iommu] Fix inline functions when !CONFIG_IOMMU_API
v6
 https://lore.kernel.org/all/cover.1763512374.git.nicolinc@nvidia.com/
 * Add Reviewed-by from Baolu and Kevin
 * Revise inline comments, kdocs, commit messages, uAPI
 * [iommu] s/iommu_dev_reset/pci_dev_reset_iommu/g for PCI exclusively
 * [iommu] Disallow iommu group sibling devices to attach concurrently
 * [pci] Drop unnecessary initializations to "ret" and "rc"
 * [pci] Improve pci_err message unpon a prepare() failure
 * [pci] Move pci_ats_supported() check inside the IOMMU callbacks
 * [pci] Apply callbacks to pci_reset_bus_function() that was missed
v5
 https://lore.kernel.org/all/cover.1762835355.git.nicolinc@nvidia.com/
 * Rebase on Joerg's next tree
 * [iommu] Skip in shared iommu_group cases
 * [iommu] Pass in default_domain to iommu_setup_dma_ops
 * [iommu] Add kdocs to iommu_get_domain_for_dev_locked()
 * [iommu] s/get_domain_for_dev_locked/driver_get_domain_for_dev
 * [iommu] Replace per-gdev pending_reset with per-group resetting_domain
v4
 https://lore.kernel.org/all/cover.1756682135.git.nicolinc@nvidia.com/
 * Add Reviewed-by from Baolu
 * [iommu] Use guard(mutex)
 * [iommu] Update kdocs for typos and revisings
 * [iommu] Skip two corner cases (alias and SRIOV)
 * [iommu] Rework attach_dev to pass in old domain pointer
 * [iommu] Reject concurrent attach_dev/set_dev_pasid for compatibility
           concern
 * [smmuv3] Drop the old_domain depedency in its release_dev callback
 * [pci] Add pci_reset_iommu_prepare/_done() wrappers checking ATS cap
v3
 https://lore.kernel.org/all/cover.1754952762.git.nicolinc@nvidia.com/
 * Add Reviewed-by from Jason
 * [iommu] Add a fast return in iommu_deferred_attach()
 * [iommu] Update kdocs, inline comments, and commit logs
 * [iommu] Use group->blocking_domain v.s. ops->blocked_domain
 * [iommu] Drop require_direct, iommu_group_get(), and xa_lock()
 * [iommu] Set the pending_reset flag after RID/PASID domain setups
 * [iommu] Do not bypass PASID domains when RID domain is already the
           blocking_domain
 * [iommu] Add iommu_get_domain_for_dev_locked to correctly return the
           blocking_domain
v2
 https://lore.kernel.org/all/cover.1751096303.git.nicolinc@nvidia.com/
 * [iommu] Update kdocs, inline comments, and commit logs
 * [iommu] Replace long-holding group->mutex with a pending_reset flag
 * [pci] Abort reset routines if iommu_dev_reset_prepare() fails
 * [pci] Apply the same vulnerability fix to other reset functions
v1
 https://lore.kernel.org/all/cover.1749494161.git.nicolinc@nvidia.com/

Thanks
Nicolin

Nicolin Chen (5):
  iommu: Lock group->mutex in iommu_deferred_attach()
  iommu: Tidy domain for iommu_setup_dma_ops()
  iommu: Add iommu_driver_get_domain_for_dev() helper
  iommu: Introduce pci_dev_reset_iommu_prepare/done()
  PCI: Suspend iommu function prior to resetting a device

 drivers/iommu/dma-iommu.h                   |   5 +-
 include/linux/iommu.h                       |  14 ++
 include/uapi/linux/vfio.h                   |   4 +
 drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3.c |   5 +-
 drivers/iommu/dma-iommu.c                   |   4 +-
 drivers/iommu/iommu.c                       | 220 +++++++++++++++++++-
 drivers/pci/pci-acpi.c                      |  13 +-
 drivers/pci/pci.c                           |  65 +++++-
 drivers/pci/quirks.c                        |  19 +-
 9 files changed, 326 insertions(+), 23 deletions(-)

----------------------------------------------------------------------

New:  iommu: Lock group->mutex in iommu_deferred_attach()
[PATCH v7 1/5] iommu: Lock group->mutex in iommu_deferred_attach()
Author: Nicolin Chen <nicolinc@nvidia.com>

The iommu_deferred_attach() function invokes __iommu_attach_device(), but
doesn't hold the group->mutex like other __iommu_attach_device() callers.

Though there is no pratical bug being triggered so far, it would be better
to apply the same locking to this __iommu_attach_device(), since the IOMMU
drivers nowaday are more aware of the group->mutex -- some of them use the
iommu_group_mutex_assert() function that could be potentially in the path
of an attach_dev callback function invoked by the __iommu_attach_device().

Worth mentioning that the iommu_deferred_attach() will soon need to check
group->resetting_domain that must be locked also.

Thus, grab the mutex to guard __iommu_attach_device() like other callers.

Reviewed-by: Jason Gunthorpe <jgg@nvidia.com>
Reviewed-by: Kevin Tian <kevin.tian@intel.com>
Reviewed-by: Lu Baolu <baolu.lu@linux.intel.com>
Signed-off-by: Nicolin Chen <nicolinc@nvidia.com>
---
 drivers/iommu/iommu.c | 13 ++++++++++---
 1 file changed, 10 insertions(+), 3 deletions(-)

----------------------------------------------------------------------

New:  KVM: Use vCPU specific memslots in __kvm_vcpu_map()
[PATCH v3 1/5] KVM: Use vCPU specific memslots in __kvm_vcpu_map()
Author: Sean Christopherson <seanjc@google.com>

When establishing a "host access map", lookup the gfn in the vCPU specific
memslots, as the intent is that the mapping will be established for the
current vCPU context.  Specifically, using __kvm_vcpu_map() in x86's SMM
context should create mappings based on the SMM memslots, not the non-SMM
memslots.

Luckily, the bug is benign as x86 is the only architecture with multiple
memslot address spaces, and all of x86's usage is limited to non-SMM.  The
calls in (or reachable by) {svm,vmx}_enter_smm() are made before
enter_smm() sets HF_SMM_MASK, and the calls in {svm,vmx}_leave_smm() are
made after emulator_leave_smm() clears HF_SMM_MASK.

Note, kvm_vcpu_unmap() uses the vCPU specific memslots, only the map() side
of things is broken.

Fixes: 357a18ad230f ("KVM: Kill kvm_map_gfn() / kvm_unmap_gfn() and gfn_to_pfn_cache")
Signed-off-by: Sean Christopherson <seanjc@google.com>
---
 virt/kvm/kvm_main.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

----------------------------------------------------------------------

New:  KVM: nVMX: Mark APIC page dirty on VM-Exit
[PATCH v3 0/5] KVM: nVMX: Mark APIC page dirty on VM-Exit
Author: Sean Christopherson <seanjc@google.com>

Extended version of Fred's patch to mark the APIC access page dirty on
VM-Exit (KVM already marks it dirty when it's unmapped).

v3:
 - Fix a benign memslots bug in __kvm_vcpu_map().
 - Mark vmcs12 pages dirty if and only if they're mapped (out-of-band).
 - Don't mark the APIC access page dirty when deliver nested posted IRQ.

v2: https://lore.kernel.org/all/20250910085156.1419090-1-griffoul@gmail.com

Fred Griffoul (1):
  KVM: nVMX: Mark APIC access page dirty when syncing vmcs12 pages

Sean Christopherson (4):
  KVM: Use vCPU specific memslots in __kvm_vcpu_map()
  KVM: x86: Mark vmcs12 pages as dirty if and only if they're mapped
  KVM: nVMX: Precisely mark vAPIC and PID maps dirty when delivering
    nested PI
  KVM: VMX: Move nested_mark_vmcs12_pages_dirty() to vmx.c, and rename

 arch/x86/kvm/vmx/nested.c | 25 ++-----------------------
 arch/x86/kvm/vmx/vmx.c    | 11 ++++++++++-
 include/linux/kvm_host.h  |  9 ++++++++-
 virt/kvm/kvm_main.c       |  2 +-
 4 files changed, 21 insertions(+), 26 deletions(-)

----------------------------------------------------------------------

