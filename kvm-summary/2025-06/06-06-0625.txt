From 65aa4c890 to bb83a944b
KVM mailing list update from 65aa4c890 to bb83a944b

Top 15 contributor Email domains (Based on Email Body)

     40 google.com
     14 ventanamicro.com
      4 oracle.com
      3 linux.ibm.com
      2 rivosinc.com
      1 zhaoxin.com

Top 15 contributors (Based on Email Body)

     18  Sean Christopherson <seanjc@google.com>
     17  Fuad Tabba <tabba@google.com>
     14  Anup Patel <apatel@ventanamicro.com>
      4  Liam Merwick <liam.merwick@oracle.com>
      3  Dionna Glaze <dionnaglaze@google.com>
      3  Christoph Schlameuss <schlameuss@linux.ibm.com>
      2  Jesse Taube <jesse@rivosinc.com>
      2  Ackerley Tng <ackerleytng@google.com>
      1  EwanHai <ewanhai-oc@zhaoxin.com>

===== Patch list in this time period =====


===== Patch Commit Messages ====

New:  KVM: x86: Drop pending_smi vs. INIT_RECEIVED check when
[PATCH 1/4] KVM: x86: Drop pending_smi vs. INIT_RECEIVED check when
Author: Sean Christopherson <seanjc@google.com>

Allow userspace to set a vCPU's mp_state to INIT_RECEIVED in conjunction
with a pending SMI, as rejecting that combination could result in KVM
disallowing reflecting the output from KVM_GET_VCPU_EVENTS back into KVM
via KVM_SET_VCPU_EVENTS.

At the time the check was added, smi_pending could only be set in the
context of KVM_RUN, with the vCPU in the RUNNABLE state.  I.e. it was
impossible for KVM to save vCPU state such that userspace could see a
pending SMI for a vCPU in WFS.

That no longer holds true now that KVM processes requested SMIs during
KVM_GET_VCPU_EVENTS, e.g. if a vCPU receives an SMI while in WFS, and
then userspace saves vCPU state.

Note, this may partially re-open the user-triggerable WARN that was mostly
closed by commit 28bf28887976 ("KVM: x86: fix user triggerable warning in
kvm_apic_accept_events()"), but that WARN can already be triggered in
several other ways, e.g. if userspace stuffs VMXON=1 after putting the
vCPU into WFS.  That issue will be addressed in an upcoming commit, in a
more robust fashion (hopefully).

Fixes: 1f7becf1b7e2 ("KVM: x86: get smi pending status correctly")
Signed-off-by: Sean Christopherson <seanjc@google.com>
---
 arch/x86/kvm/x86.c | 5 ++---
 1 file changed, 2 insertions(+), 3 deletions(-)

----------------------------------------------------------------------

New:  KVM: x86: Fix WFS vs. pending SMI WARN
[PATCH 0/4] KVM: x86: Fix WFS vs. pending SMI WARN
Author: Sean Christopherson <seanjc@google.com>

Fix a user-triggerable WARN that syzkaller found by stuffing INIT_RECEIVED,
a.k.a. WFS, and then putting the vCPU into VMX Root Mode (post-VMXON).  Use
the same approach KVM uses for dealing with "impossible" emulation when
running a !URG guest, and simply wait until KVM_RUN to detect that the vCPU
has architecturally impossible state.

Sean Christopherson (4):
  KVM: x86: Drop pending_smi vs. INIT_RECEIVED check when setting
    MP_STATE
  KVM: x86: WARN and reject KVM_RUN if vCPU's MP_STATE is SIPI_RECEIVED
  KVM: x86: Move INIT_RECEIVED vs. INIT/SIPI blocked check to KVM_RUN
  KVM: x86: Refactor handling of SIPI_RECEIVED when setting MP_STATE

 arch/x86/kvm/x86.c | 49 ++++++++++++++++++++++++++++------------------
 1 file changed, 30 insertions(+), 19 deletions(-)

----------------------------------------------------------------------

New:  x86/msr: Treat PRED_CMD as support if CPU
[kvm-unit-tests PATCH 1/3] x86/msr: Treat PRED_CMD as support if CPU
Author: Sean Christopherson <seanjc@google.com>

The PRED_CMD MSR also exists if the CPU supports SBPB.

Signed-off-by: Sean Christopherson <seanjc@google.com>
---
 lib/x86/processor.h | 1 +
 x86/msr.c           | 3 ++-
 2 files changed, 3 insertions(+), 1 deletion(-)

----------------------------------------------------------------------

New:  x86/msr: Add SPEC_CTRL coverage
[kvm-unit-tests PATCH 0/3] x86/msr: Add SPEC_CTRL coverage
Author: Sean Christopherson <seanjc@google.com>

Add test coverage for SPEC_CTRL, which detects the bug pointed by Chao[1]
when running on hosts with V_SPEC_CTRL.

Note, this applies on top of the X86_FEATURE_XXX cleanup[2].

[1] https://lore.kernel.org/all/aEE4BEHAHdhNTGoG@intel.com
[2] https://lore.kernel.org/all/20250529221929.3807680-1-seanjc@google.com

Sean Christopherson (3):
  x86/msr: Treat PRED_CMD as support if CPU has SBPB
  x86/msr: Add a testcase to verify SPEC_CTRL exists (or not) as
    expected
  x86/msr: Add an "msr64" test configuration to validate negative cases

 lib/x86/msr.h       |  8 ++++++--
 lib/x86/processor.h | 10 ++++++++--
 x86/msr.c           | 34 +++++++++++++++++++++++++++++++---
 x86/unittests.cfg   |  7 +++++++
 4 files changed, 52 insertions(+), 7 deletions(-)

----------------------------------------------------------------------

New:  x86: nSVM: Actually report missed MSR
[kvm-unit-tests PATCH v2 1/8] x86: nSVM: Actually report missed MSR
Author: Sean Christopherson <seanjc@google.com>

Report an error if KVM fails to generate a nested VM-Exit due to MSR
interception instead of eating the #GP and printing a "warning".  Printing
an innocuous message on failure makes the test completely worthless.

Signed-off-by: Sean Christopherson <seanjc@google.com>
---
 x86/svm_tests.c | 76 ++++++++++++++++++-------------------------------
 1 file changed, 27 insertions(+), 49 deletions(-)

----------------------------------------------------------------------

New:  x86/svm: Make nSVM MSR test useful
[kvm-unit-tests PATCH v2 0/8] x86/svm: Make nSVM MSR test useful
Author: Sean Christopherson <seanjc@google.com>

Fix the nSVM MSR interception test to actually detect failures, and expand
its coverage to test:

 - Out-of-range MSRs
 - With all other bits in the MSPRM clear
 - {RD,WR}MSR with interception *disabled*
 - That L1 and L2 see the same value for RDMSR when interception is disabled

v1: https://lore.kernel.org/all/20250529215713.3802116-1-seanjc@google.com

Sean Christopherson (8):
  x86: nSVM: Actually report missed MSR intercepts as failures
  x86: nSVM: Test MSRs just outside the ranges of the MSR Permissions
    Map
  x86: nSVM: Clean up variable types and names in test_msr_intercept()
  x86: Expand the suite of bitops to cover all set/clear operations
  x86: nVMX: Use set_bit() instead of test_and_set_bit() when return is
    ignored
  x86: nSVM: Set MSRPM bit on-demand when testing interception
  x86: nSVM: Verify disabling {RD,WR}MSR interception behaves as
    expected
  x86: nSVM: Verify L1 and L2 see same MSR value when interception is
    disabled

 lib/x86/asm/bitops.h |  86 +++++++++++++++-
 lib/x86/processor.h  |  12 ---
 x86/svm_tests.c      | 240 +++++++++++++++++++++++++++++++++----------
 x86/vmx_tests.c      |   4 +-
 4 files changed, 269 insertions(+), 73 deletions(-)

----------------------------------------------------------------------

New:  riscv: Add RV_INSN_LEN to processor.h
[kvm-unit-tests PATCH v3 1/2] riscv: Add RV_INSN_LEN to processor.h
Author: Jesse Taube <jesse@rivosinc.com>

When handeling traps and faults it is offten necessary to know the size
of the instruction at epc. Add RV_INSN_LEN to calculate the
instruction size.

Signed-off-by: Jesse Taube <jesse@rivosinc.com>
---
 lib/riscv/asm/processor.h | 2 ++
 1 file changed, 2 insertions(+)

----------------------------------------------------------------------

New:  KVM: Rename CONFIG_KVM_PRIVATE_MEM to CONFIG_KVM_GMEM
[PATCH v11 01/18] KVM: Rename CONFIG_KVM_PRIVATE_MEM to CONFIG_KVM_GMEM
Author: Fuad Tabba <tabba@google.com>

The option KVM_PRIVATE_MEM enables guest_memfd in general. Subsequent
patches add shared memory support to guest_memfd. Therefore, rename it
to KVM_GMEM to make its purpose clearer.

Reviewed-by: Ira Weiny <ira.weiny@intel.com>
Reviewed-by: Gavin Shan <gshan@redhat.com>
Reviewed-by: Shivank Garg <shivankg@amd.com>
Reviewed-by: Vlastimil Babka <vbabka@suse.cz>
Co-developed-by: David Hildenbrand <david@redhat.com>
Signed-off-by: David Hildenbrand <david@redhat.com>
Signed-off-by: Fuad Tabba <tabba@google.com>
---
 arch/x86/include/asm/kvm_host.h |  2 +-
 include/linux/kvm_host.h        | 10 +++++-----
 virt/kvm/Kconfig                |  8 ++++----
 virt/kvm/Makefile.kvm           |  2 +-
 virt/kvm/kvm_main.c             |  4 ++--
 virt/kvm/kvm_mm.h               |  4 ++--
 6 files changed, 15 insertions(+), 15 deletions(-)

----------------------------------------------------------------------

New:  KVM: Mapping guest_memfd backed memory at the host
[PATCH v11 00/18] KVM: Mapping guest_memfd backed memory at the host
Author: Fuad Tabba <tabba@google.com>

Main changes since v10 [1]:
- Added bounds checking when faulting a shared page into the host, along
  with a selftest to verify the check.
- Refactored KVM/arm64's handling of guest faults (user_mem_abort()).
  I've dropped the Reviewed-by tags from "KVM: arm64: Refactor
  user_mem_abort()..." since it has changed significantly.
- Handled nested virtualization in KVM/arm64 when faulting guest_memfd
  backed pages into the guest.
- Addressed various points of feedback from the last revision.
- Still based on Linux 6.15

This patch series enables the mapping of guest_memfd backed memory in
the host. This is useful for VMMs like Firecracker that aim to run
guests entirely backed by guest_memfd [2]. When combined with Patrick's
series for direct map removal [3], this provides additional hardening
against Spectre-like transient execution attacks.

This series also lays the groundwork for restricted mmap() support for
guest_memfd backed memory in the host for Confidential Computing
platforms that permit in-place sharing of guest memory with the host
[4].

Patch breakdown:

Patches 1-7: Primarily refactoring and renaming to decouple the concept
of guest memory being "private" from it being backed by guest_memfd.

Patches 8-9: Add support for in-place shared memory and the ability for
the host to map it. This is gated by a new configuration option, toggled
by a new flag, and advertised to userspace by a new capability
(introduced in patch 16).

Patches 10-15: Implement the x86 and arm64 support for this feature.

Patch 16: Introduces the new capability to advertise this support and
updates the documentation.

Patches 17-18: Add and fix selftests for the new functionality.

For details on how to test this patch series, and on how to boot a guest
that uses the new features, please refer to v8 [5].

Cheers,
/fuad

[1] https://lore.kernel.org/all/20250527180245.1413463-1-tabba@google.com/
[2] https://github.com/firecracker-microvm/firecracker/tree/feature/secret-hiding
[3] https://lore.kernel.org/all/20250221160728.1584559-1-roypat@amazon.co.uk/
[4] https://lore.kernel.org/all/20250328153133.3504118-1-tabba@google.com/
[5] https://lore.kernel.org/all/20250430165655.605595-1-tabba@google.com/

Ackerley Tng (2):
  KVM: x86/mmu: Handle guest page faults for guest_memfd with shared
    memory
  KVM: x86: Consult guest_memfd when computing max_mapping_level

Fuad Tabba (16):
  KVM: Rename CONFIG_KVM_PRIVATE_MEM to CONFIG_KVM_GMEM
  KVM: Rename CONFIG_KVM_GENERIC_PRIVATE_MEM to
    CONFIG_KVM_GENERIC_GMEM_POPULATE
  KVM: Rename kvm_arch_has_private_mem() to kvm_arch_supports_gmem()
  KVM: x86: Rename kvm->arch.has_private_mem to kvm->arch.supports_gmem
  KVM: Rename kvm_slot_can_be_private() to kvm_slot_has_gmem()
  KVM: Fix comments that refer to slots_lock
  KVM: Fix comment that refers to kvm uapi header path
  KVM: guest_memfd: Allow host to map guest_memfd pages
  KVM: guest_memfd: Track shared memory support in memslot
  KVM: x86: Enable guest_memfd shared memory for SW-protected VMs
  KVM: arm64: Refactor user_mem_abort()
  KVM: arm64: Handle guest_memfd-backed guest page faults
  KVM: arm64: Enable host mapping of shared guest_memfd memory
  KVM: Introduce the KVM capability KVM_CAP_GMEM_SHARED_MEM
  KVM: selftests: Don't use hardcoded page sizes in guest_memfd test
  KVM: selftests: guest_memfd mmap() test when mapping is allowed

 Documentation/virt/kvm/api.rst                |   9 +
 arch/arm64/include/asm/kvm_host.h             |   5 +
 arch/arm64/kvm/Kconfig                        |   1 +
 arch/arm64/kvm/mmu.c                          | 200 +++++++++++++----
 arch/x86/include/asm/kvm_host.h               |  22 +-
 arch/x86/kvm/Kconfig                          |   5 +-
 arch/x86/kvm/mmu/mmu.c                        | 135 ++++++-----
 arch/x86/kvm/svm/sev.c                        |   4 +-
 arch/x86/kvm/svm/svm.c                        |   4 +-
 arch/x86/kvm/x86.c                            |   4 +-
 include/linux/kvm_host.h                      |  80 +++++--
 include/uapi/linux/kvm.h                      |   2 +
 tools/testing/selftests/kvm/Makefile.kvm      |   1 +
 .../testing/selftests/kvm/guest_memfd_test.c  | 212 +++++++++++++++---
 virt/kvm/Kconfig                              |  14 +-
 virt/kvm/Makefile.kvm                         |   2 +-
 virt/kvm/guest_memfd.c                        |  94 +++++++-
 virt/kvm/kvm_main.c                           |  16 +-
 virt/kvm/kvm_mm.h                             |   4 +-
 19 files changed, 645 insertions(+), 169 deletions(-)

----------------------------------------------------------------------

New:  SEV-SNP fix for cpu soft lockup on 1TB+ guests
[PATCH 0/3] SEV-SNP fix for cpu soft lockup on 1TB+ guests
Author: Liam Merwick <liam.merwick@oracle.com>

When creating SEV-SNP guests with a large amount of memory (940GB or greater)
the host experiences a soft cpu lockup while setting the per-page memory
attributes on the whole range of memory in the guest.

The underlying issue is that the implementation of setting the
memory attributes using an Xarray implementation is a time-consuming
operation (e.g. a 1.9TB guest takes over 30 seconds to set the attributes)

Fix the lockup by modifying kvm_vm_ioctl_set_mem_attributes() so that it
sets the attributes on, at most, a range of 512GB at a time and avoids
holding kvm->slot_lock for too long.

Apart from the lockup, the implementation to set memory attributes via Xarray
also results in a delay early in the boot of SEV-SNP/TDX guests - this fix
does not address that.  As it happens, the slowness of setting the attributes
was brought up by Michael Roth in the review of Ackerley Tng's series to add
1G page support for guest_memfd [1] where using a Maple Tree implementation
is being proposed to track shareability and Michael suggested that doing it
for KVM mem attributes would be useful also (it should avoid the SLU while
also taking less CPU time in general to populate). If that was implemented
in the future, it should address this lockup but I think there's benefit
in fixing the lockup issue now with a targeted fix.

[1] https://lore.kernel.org/all/20250529054227.hh2f4jmyqf6igd3i@amd.com 

Tested with VMs up to 1900GB in size (the limit of hardware available to me)

The functionality was introduced in v6.8 but I tagged as just needing
backporting as far as linux-6.12.y (applies cleanly)

Based on tag: kvm-6.16-1

Liam Merwick (3):
  KVM: Batch setting of per-page memory attributes to avoid soft lockup
  KVM: Add trace_kvm_vm_set_mem_attributes()
  KVM: fix typo in kvm_vm_set_mem_attributes() comment

 include/trace/events/kvm.h | 33 +++++++++++++++++++++++++++++
 virt/kvm/kvm_main.c        | 43 ++++++++++++++++++++++++++++++++------
 2 files changed, 70 insertions(+), 6 deletions(-)

----------------------------------------------------------------------

New:  KVM: Batch setting of per-page memory attributes to avoid soft lockup
[PATCH 1/3] KVM: Batch setting of per-page memory attributes to avoid soft lockup
Author: Liam Merwick <liam.merwick@oracle.com>

When booting an SEV-SNP guest with a sufficiently large amount of memory (1TB+),
the host can experience CPU soft lockups when running an operation in
kvm_vm_set_mem_attributes() to set memory attributes on the whole
range of guest memory.

watchdog: BUG: soft lockup - CPU#8 stuck for 26s! [qemu-kvm:6372]
CPU: 8 UID: 0 PID: 6372 Comm: qemu-kvm Kdump: loaded Not tainted 6.15.0-rc7.20250520.el9uek.rc1.x86_64 #1 PREEMPT(voluntary)
Hardware name: Oracle Corporation ORACLE SERVER E4-2c/Asm,MB Tray,2U,E4-2c, BIOS 78016600 11/13/2024
RIP: 0010:xas_create+0x78/0x1f0
Code: 00 00 00 41 80 fc 01 0f 84 82 00 00 00 ba 06 00 00 00 bd 06 00 00 00 49 8b 45 08 4d 8d 65 08 41 39 d6 73 20 83 ed 06 48 85 c0 <74> 67 48 89 c2 83 e2 03 48 83 fa 02 75 0c 48 3d 00 10 00 00 0f 87
RSP: 0018:ffffad890a34b940 EFLAGS: 00000286
RAX: ffff96f30b261daa RBX: ffffad890a34b9c8 RCX: 0000000000000000
RDX: 000000000000001e RSI: 0000000000000000 RDI: 0000000000000000
RBP: 0000000000000018 R08: 0000000000000000 R09: 0000000000000000
R10: 0000000000000000 R11: 0000000000000000 R12: ffffad890a356868
R13: ffffad890a356860 R14: 0000000000000000 R15: ffffad890a356868
FS:  00007f5578a2a400(0000) GS:ffff97ed317e1000(0000) knlGS:0000000000000000
CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
CR2: 00007f015c70fb18 CR3: 00000001109fd006 CR4: 0000000000f70ef0
PKRU: 55555554
Call Trace:
 <TASK>
 xas_store+0x58/0x630
 ? srso_alias_return_thunk+0x5/0xfbef5
 ? asm_sysvec_apic_timer_interrupt+0x1a/0x20
 __xa_store+0xa5/0x130
 xa_store+0x2c/0x50
 kvm_vm_set_mem_attributes+0x343/0x710 [kvm]
 kvm_vm_ioctl+0x796/0xab0 [kvm]
 ? srso_alias_return_thunk+0x5/0xfbef5
 ? srso_alias_return_thunk+0x5/0xfbef5
 ? rseq_ip_fixup+0x8c/0x1e0
 __x64_sys_ioctl+0xa3/0xd0
 do_syscall_64+0x8c/0x7a0
 ? srso_alias_return_thunk+0x5/0xfbef5
 ? __alloc_frozen_pages_noprof+0x18d/0x340
 ? srso_alias_return_thunk+0x5/0xfbef5
 ? try_charge_memcg+0x76/0x640
 ? srso_alias_return_thunk+0x5/0xfbef5
 ? __count_memcg_events+0xbb/0x150
 ? srso_alias_return_thunk+0x5/0xfbef5
 ? __mod_memcg_lruvec_state+0xb6/0x1b0
 ? srso_alias_return_thunk+0x5/0xfbef5
 ? __lruvec_stat_mod_folio+0x83/0xd0
 ? srso_alias_return_thunk+0x5/0xfbef5
 ? srso_alias_return_thunk+0x5/0xfbef5
 ? srso_alias_return_thunk+0x5/0xfbef5
 ? set_ptes.isra.0+0x36/0x90
 ? srso_alias_return_thunk+0x5/0xfbef5
 ? do_anonymous_page+0x103/0x4d0
 ? srso_alias_return_thunk+0x5/0xfbef5
 ? __handle_mm_fault+0x397/0x6f0
 ? srso_alias_return_thunk+0x5/0xfbef5
 ? __count_memcg_events+0xbb/0x150
 ? srso_alias_return_thunk+0x5/0xfbef5
 ? count_memcg_events.constprop.0+0x26/0x50
 ? srso_alias_return_thunk+0x5/0xfbef5
 ? handle_mm_fault+0x245/0x350
 ? srso_alias_return_thunk+0x5/0xfbef5
 ? do_user_addr_fault+0x221/0x686
 ? srso_alias_return_thunk+0x5/0xfbef5
 ? arch_exit_to_user_mode_prepare.isra.0+0x1e/0xd0
 entry_SYSCALL_64_after_hwframe+0x76/0x7e
RIP: 0033:0x7f5578d031bb
Code: ff ff ff 85 c0 79 9b 49 c7 c4 ff ff ff ff 5b 5d 4c 89 e0 41 5c c3 66 0f 1f 84 00 00 00 00 00 f3 0f 1e fa b8 10 00 00 00 0f 05 <48> 3d 01 f0 ff ff 73 01 c3 48 8b 0d 2d 4c 0f 00 f7 d8 64 89 01 48
RSP: 002b:00007ffe0a742b88 EFLAGS: 00000246 ORIG_RAX: 0000000000000010
RAX: ffffffffffffffda RBX: 000000004020aed2 RCX: 00007f5578d031bb
RDX: 00007ffe0a742c80 RSI: 000000004020aed2 RDI: 000000000000000b
RBP: 0000010000000000 R08: 0000010000000000 R09: 0000017680000000
R10: 0000000000000080 R11: 0000000000000246 R12: 00005575e5f95120
R13: 00007ffe0a742c80 R14: 0000000000000008 R15: 00005575e5f961e0

Limit the range of memory per operation when setting the attributes to
avoid holding kvm->slots_lock for too long and causing a cpu soft lockup.

Fixes: 5a475554db1e ("KVM: Introduce per-page memory attributes")
Cc: stable@vger.kernel.org # 6.12.x
Signed-off-by: Liam Merwick <liam.merwick@oracle.com>
---
 virt/kvm/kvm_main.c | 37 ++++++++++++++++++++++++++++++++-----
 1 file changed, 32 insertions(+), 5 deletions(-)

----------------------------------------------------------------------

New:  kvm: sev: Add SEV-SNP guest request throttling
[PATCH v6 1/2] kvm: sev: Add SEV-SNP guest request throttling
Author: Dionna Glaze <dionnaglaze@google.com>

The AMD-SP is a precious resource that doesn't have a scheduler other
than a mutex lock queue. To avoid customers from causing a DoS, a
kernel module parameter for rate limiting guest requests is added.

The default value does not impose any rate limiting.

Throttling vs scheduling:
Even though Linux kernel mutexes have fair scheduling, the SEV command
mutex is not enough to balance the AMD-SP load in a manner that favors
the host to run VM launches for low boot latency over traffic from the
guest in the form of guests requests that it can't predict.
Boot sequence commands and guest request commands all contend on
the same mutex, so boot latency is affected by increased guest request
contention.

A VM launch may see dozens of SNP_LAUNCH_UPDATE commands before
SNP_LAUNCH_FINISH, and boot times are a heavily protected metric in
hyperscalars.
To favor lower latency of VM launches over each VM's ability to request
attestations at a high rate, the guest requests need a secondary
scheduling mechanism.
It's not good practice to hold a lock and return to user space, so using
a secondary lock for VM launch sequences is not an appropriate solution.
For simplicity, merely set a rate limit for every VM's guest requests
and allow a system administrator to tune that rate limit to platform
needs.

Design decisions:
The throttle rate for a VM cannot be changed once it has been started.
The rate the VM gets is its level of service, so it should not be
degradable by a mem_enc_ioctl for example.

Empirical investigation:
With a test methodology of turning up N-1 "antagonist" VMs with 2 vCPUs
and 4GiB RAM that all request a SEV-SNP attestation a tight loop before
measuring the boot latency of the Nth VM, an effective quality of service
should keep the average boot latency at levels without any guest request
contention.

On a dedicated 256 core AMD Zen3 with 1TiB of RAM, continuous performance
testing shows that a boot latency of 220ms +- 50ms is typical with N in
{4, 16, 32, 64} when the request rate is set to 1/s.

After N=64, the rate limit of 1 HZ is insufficient to hold back enough
time for the final VM launch to succeed consistently in the contention.

Cc: Thomas Lendacky <Thomas.Lendacky@amd.com>
Cc: Paolo Bonzini <pbonzini@redhat.com>
Cc: Joerg Roedel <jroedel@suse.de>
Cc: Peter Gonda <pgonda@google.com>
Cc: Borislav Petkov <bp@alien8.de>
Cc: Sean Christopherson <seanjc@google.com>

Signed-off-by: Dionna Glaze <dionnaglaze@google.com>
---
 arch/x86/kvm/svm/sev.c | 17 +++++++++++++++++
 arch/x86/kvm/svm/svm.h |  3 +++
 2 files changed, 20 insertions(+)

----------------------------------------------------------------------

New:  kvm: sev: Add SNP guest request throttling
[PATCH v6 0/2] kvm: sev: Add SNP guest request throttling
Author: Dionna Glaze <dionnaglaze@google.com>

The GHCB specification recommends that SNP guest requests should be
rate limited. Add a kernel module parameter to ensure a system-wide
lower bound rate limit on a per-VM scale for all new VMs. Note that
this does not preclude the addition of a new KVM exit type for SEV-SNP
guest requests for userspace to impose any additional throttling logic.

The AMD-SP is a global resource that must be shared across VMs, so
its time should be multiplexed across VMs fairly. It is the
responsibility of the VMM to ensure all SEV-SNP VMs have a rate limit
set such that the collective set of VMs on the machine have a rate of
access that does not exceed the device's capacity.

The sev-guest device already respects the SNP_GUEST_VMM_ERR_BUSY
result code, so utilize that result to cause the guest to retry after
waiting momentarily.

Changes since v5:
  * Reverted the KVM command for setting the rate limit in favor of
    the module parameter solution. The default is no rate-limiting
    to maintain existing behavior.
Changes since v4:
  * Fixed build failure caused by rebase.
  * Added ratelimit.h include.
  * Added rate bounds checking to stay within ratelimit types.
Changes since v3:
  * Rebased on master, changed module parameter to mem_enc_ioctl
    command. Changed commit descriptions. Much time has passed.
Changes since v2:
  * Rebased on v7, changed "we" wording to passive voice.
Changes since v1:
  * Added missing Ccs to patches.

Dionna Glaze (2):
  kvm: sev: Add SEV-SNP guest request throttling
  kvm: sev: If ccp is busy, report busy to guest

 arch/x86/kvm/svm/sev.c | 22 ++++++++++++++++++++++
 arch/x86/kvm/svm/svm.h |  3 +++
 2 files changed, 25 insertions(+)

----------------------------------------------------------------------

New:  RISC-V: KVM: Fix the size parameter check in SBI SFENCE calls
[PATCH 01/13] RISC-V: KVM: Fix the size parameter check in SBI SFENCE calls
Author: Anup Patel <apatel@ventanamicro.com>

As-per the SBI specification, an SBI remote fence operation applies
to the entire address space if either:
1) start_addr and size are both 0
2) size is equal to 2^XLEN-1

From the above, only #1 is checked by SBI SFENCE calls so fix the
size parameter check in SBI SFENCE calls to cover #2 as well.

Fixes: 13acfec2dbcc ("RISC-V: KVM: Add remote HFENCE functions based on VCPU requests")
Signed-off-by: Anup Patel <apatel@ventanamicro.com>
---
 arch/riscv/kvm/vcpu_sbi_replace.c | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

----------------------------------------------------------------------

New:  MMU related improvements for KVM RISC-V
[PATCH 00/13] MMU related improvements for KVM RISC-V
Author: Anup Patel <apatel@ventanamicro.com>

This series primarily has various MMU improvements for KVM RISC-V
and it also serves as a preparatory series for the upcoming nested
virtualization support.

PATCH1 to PATCH2: SBI spec related fixes in SBI RFENCE extension
PATCH3 to PATCH6: Few cosmetic improvements
PATCH7 to PATCH8: TLB maintenance related improvements
PATCH9 to PATCH13: MMU related preparatory work for nested virtualization

These patches can also be found in the riscv_kvm_mmu_imp_v1 branch
at: https://github.com/avpatel/linux.git

Anup Patel (13):
  RISC-V: KVM: Fix the size parameter check in SBI SFENCE calls
  RISC-V: KVM: Don't treat SBI HFENCE calls as NOPs
  RISC-V: KVM: Check kvm_riscv_vcpu_alloc_vector_context() return value
  RISC-V: KVM: Drop the return value of kvm_riscv_vcpu_aia_init()
  RISC-V: KVM: Rename and move kvm_riscv_local_tlb_sanitize()
  RISC-V: KVM: Replace KVM_REQ_HFENCE_GVMA_VMID_ALL with
    KVM_REQ_TLB_FLUSH
  RISC-V: KVM: Don't flush TLB in gstage_set_pte() when PTE is unchanged
  RISC-V: KVM: Implement kvm_arch_flush_remote_tlbs_range()
  RISC-V: KVM: Factor-out MMU related declarations into separate headers
  RISC-V: KVM: Introduce struct kvm_gstage_mapping
  RISC-V: KVM: Add vmid field to struct kvm_riscv_hfence
  RISC-V: KVM: Factor-out g-stage page table management
  RISC-V: KVM: Pass VMID as parameter to kvm_riscv_hfence_xyz() APIs

 arch/riscv/include/asm/kvm_aia.h    |   2 +-
 arch/riscv/include/asm/kvm_gstage.h |  72 ++++
 arch/riscv/include/asm/kvm_host.h   | 103 +-----
 arch/riscv/include/asm/kvm_mmu.h    |  21 ++
 arch/riscv/include/asm/kvm_tlb.h    |  84 +++++
 arch/riscv/include/asm/kvm_vmid.h   |  27 ++
 arch/riscv/kvm/Makefile             |   1 +
 arch/riscv/kvm/aia_device.c         |   6 +-
 arch/riscv/kvm/aia_imsic.c          |  12 +-
 arch/riscv/kvm/gstage.c             | 336 +++++++++++++++++++
 arch/riscv/kvm/main.c               |   3 +-
 arch/riscv/kvm/mmu.c                | 499 ++++++----------------------
 arch/riscv/kvm/tlb.c                | 110 +++---
 arch/riscv/kvm/vcpu.c               |  27 +-
 arch/riscv/kvm/vcpu_exit.c          |   7 +-
 arch/riscv/kvm/vcpu_sbi_replace.c   |  25 +-
 arch/riscv/kvm/vcpu_sbi_v01.c       |  25 +-
 arch/riscv/kvm/vm.c                 |   7 +-
 arch/riscv/kvm/vmid.c               |  25 ++
 19 files changed, 791 insertions(+), 601 deletions(-)

----------------------------------------------------------------------

