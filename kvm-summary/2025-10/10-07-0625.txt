From c4c8932bd to 83929009f
KVM mailing list update from c4c8932bd to 83929009f

Top 15 contributor Email domains (Based on Email Body)

      5 linux.intel.com
      4 suse.com
      1 fb.com

Top 15 contributors (Based on Email Body)

      4  Tony Lindgren <tony.lindgren@linux.intel.com>
      4  Juergen Gross <jgross@suse.com>
      1  Dapeng Mi <dapeng1.mi@linux.intel.com>
      1  Alex Mastro <amastro@fb.com>

===== Patch list in this time period =====


===== Patch Commit Messages ====

New:  Documentation: kvm: Add KVM_MIGRATE_CMD
[RFC PATCH 1/3] Documentation: kvm: Add KVM_MIGRATE_CMD
Author: Tony Lindgren <tony.lindgren@linux.intel.com>

Document KVM_MIGRATE_CMD. To support live migration of confidential
computing guests, the hardware may need to pass migration command related
data between the source and destination. For confidential computing, the
migration command related data is not accessible to KVM.

Signed-off-by: Tony Lindgren <tony.lindgren@linux.intel.com>
---
 Documentation/virt/kvm/api.rst | 47 ++++++++++++++++++++++++++++++++++
 1 file changed, 47 insertions(+)

----------------------------------------------------------------------

New:  Add import export API for confidential guest live migration
[RFC PATCH 0/3] Add import export API for confidential guest live migration
Author: Tony Lindgren <tony.lindgren@linux.intel.com>

Hi all,

This patch series is intended to get the discussion going on the APIs
needed for confidential computing live migration for firmware related
calls.

These APIs are designed to work for TDX, while also attempting to make them
generic enough such that other coco VMs could use them. While the coco
live migrations are still in flux, we wanted to see where the pain points
are in making them work similarly from the userspace perspective. This way
we can try to have at least some unification between all the solutions.

Currently live migration between the source and destination host machine is
handled by the userspace with the help of KVM. The guest memory and VCPU
states are accessible to the userspace to migrate. Userspace gets the
list of dirty pages using the KVM_GET_DIRTY_LOG ioctl().

For coco VMs, things are different. The guest memory and VCPU states are
not accessible to the userspace or KVM. Userspace may need to ask the
source and destination host firmware to package and unpackage the guest
memory and VCPU states for migration. The memory and VCPU states may be
encrypted with a migration key between the source and destination
firmware. Additionally the userspace may need to communicate with the
host firmware to manage the live migration, and to pass migration related
data between the source and destination firmware. For getting the list of
dirty pages, KVM_GET_DIRTY_LOG is still used as earlier.

To allow handling coco VM live migration, introduce new ioctl() calls to
communicate with the host firmware:

  - KVM_MIGRATE_CMD
  - KVM_IMPORT/EXPORT_MEMORY
  - KVM_IMPORT/EXPORT_VCPU

No separate VM scope IMPORT/EXPORT calls are currently needed at least for
TDX. It can be covered with the VM scope KVM_MIGRATE_CMD sub-commands.

Note that these calls are limited to cases where userspace needs to
communicate with the hardware specific firmware. This may not apply to the
service VM solutions like Coconut SVSM as it can access the memory and VCPU
states directly.

To demonstrate how the userspace can make use of these calls, a simplified
imaginary QEMU pseudo-code example for live migration is shown below:

1. QEMU source sets up migration

migration_thread()
  migrate_cmd(MIGRATE_SETUP, data)
...
kvm_arch_vm_ioctl(KVM_MIGRATE_CMD, KVM_MIGRATE_SETUP, data)
 arch_migrate_cmd(MIGRATE_SETUP, data)

2. QEMU destination sets up migration

migrate_cmd(MIGRATE_SETUP, data)
...
kvm_arch_vm_ioctl(KVM_MIGRATE_CMD, KVM_MIGRATE_SETUP, data)
 arch_migrate_cmd(MIGRATE_SETUP, data)

QEMU return-path can be used as needed.

3. QEMU source saves memory

ram_save_target_page()
 ram_export_memory(data)
...
kvm_arch_vm_ioctl(KVM_EXPORT_MEMORY, data)
 arch_export_memory(data)

4. QEMU destination loads memory

ram_load_precopy()
 ram_import_memory(data)
...
kvm_arch_vm_ioctl(KVM_IMPORT_MEMORY, data)
 arch_import_memory(data)

5. QEMU source and destination configure additional VCPU VMState

static const VMStateInfo vcpu_state_info = {
 .name = "private-vcpu"
 .put = export_vcpu_state
 .get = import_vcpu_state
}

6. QEMU source saves VCPU state

vmstate_save_state()
 export_vcpu_state(data)
...
kvm_vcpu_arch_ioctl(KVM_EXPORT_VCPU, data)
 arch_export_vcpu_state(data)

7. QEMU destination loads VCPU state
vmstate_load_state()
 import_vcpu_state(data)
...
kvm_vcpu_arch_ioctl(KVM_IMPORT_VCPU, data)
 arch_import_vcpu_state(data)

8. QEMU source migration finish

migration_iteration_finish()
 migrate_cmd(KVM_MIGRATE_CMD, MIGRATE_FINISH, data)
...
kvm_arch_vm_ioctl(KVM_MIGRATE_CMD, MIGRATE_FINISH, data)
 arch_migrate_cmd(MIGRATE_FINISH, data)

Regards,

Tony

Tony Lindgren (3):
  Documentation: kvm: Add KVM_MIGRATE_CMD
  Documentation: kvm: Add KVM_IMPORT/EXPORT_MEMORY
  Documentation: kvm: Add KVM_IMPORT/EXPORT_VCPU

 Documentation/virt/kvm/api.rst | 203 +++++++++++++++++++++++++++++++++
 1 file changed, 203 insertions(+)

----------------------------------------------------------------------

New:  paravirt: cleanup and reorg
[PATCH v3 00/21] paravirt: cleanup and reorg
Author: Juergen Gross <jgross@suse.com>

Some cleanups and reorg of paravirt code and headers:

- The first 2 patches should be not controversial at all, as they
  remove just some no longer needed #include and struct forward
  declarations.

- The 3rd patch is removing CONFIG_PARAVIRT_DEBUG, which IMO has
  no real value, as it just changes a crash to a BUG() (the stack
  trace will basically be the same). As the maintainer of the main
  paravirt user (Xen) I have never seen this crash/BUG() to happen.

- The 4th patch is just a movement of code.

- I don't know for what reason asm/paravirt_api_clock.h was added,
  as all archs supporting it do it exactly in the same way. Patch
  5 is removing it.

- Patches 6-14 are streamlining the paravirt clock interfaces by
  using a common implementation across architectures where possible
  and by moving the related code into common sched code, as this is
  where it should live.

- Patches 15-20 are more like RFC material preparing the paravirt
  infrastructure to support multiple pv_ops function arrays.
  As a prerequisite for that it makes life in objtool much easier
  with dropping the Xen static initializers of the pv_ops sub-
  structures, which is done in patches 15-17.
  Patches 18-20 are doing the real preparations for multiple pv_ops
  arrays and using those arrays in multiple headers.

- Patch 21 is an example how the new scheme can look like using the
  PV-spinlocks.

Changes in V2:
- new patches 13-18 and 20
- complete rework of patch 21

Changes in V3:
- fixed 2 issues detected by kernel test robot

Juergen Gross (21):
  x86/paravirt: Remove not needed includes of paravirt.h
  x86/paravirt: Remove some unneeded struct declarations
  x86/paravirt: Remove PARAVIRT_DEBUG config option
  x86/paravirt: Move thunk macros to paravirt_types.h
  paravirt: Remove asm/paravirt_api_clock.h
  sched: Move clock related paravirt code to kernel/sched
  arm/paravirt: Use common code for paravirt_steal_clock()
  arm64/paravirt: Use common code for paravirt_steal_clock()
  loongarch/paravirt: Use common code for paravirt_steal_clock()
  riscv/paravirt: Use common code for paravirt_steal_clock()
  x86/paravirt: Use common code for paravirt_steal_clock()
  x86/paravirt: Move paravirt_sched_clock() related code into tsc.c
  x86/paravirt: Introduce new paravirt-base.h header
  x86/paravirt: Move pv_native_*() prototypes to paravirt.c
  x86/xen: Drop xen_irq_ops
  x86/xen: Drop xen_cpu_ops
  x86/xen: Drop xen_mmu_ops
  objtool: Allow multiple pv_ops arrays
  x86/paravirt: Allow pv-calls outside paravirt.h
  x86/paravirt: Specify pv_ops array in paravirt macros
  x86/pvlocks: Move paravirt spinlock functions into own header

 arch/Kconfig                                  |   3 +
 arch/arm/Kconfig                              |   1 +
 arch/arm/include/asm/paravirt.h               |  22 --
 arch/arm/include/asm/paravirt_api_clock.h     |   1 -
 arch/arm/kernel/Makefile                      |   1 -
 arch/arm/kernel/paravirt.c                    |  23 --
 arch/arm64/Kconfig                            |   1 +
 arch/arm64/include/asm/paravirt.h             |  14 -
 arch/arm64/include/asm/paravirt_api_clock.h   |   1 -
 arch/arm64/kernel/paravirt.c                  |  11 +-
 arch/loongarch/Kconfig                        |   1 +
 arch/loongarch/include/asm/paravirt.h         |  13 -
 .../include/asm/paravirt_api_clock.h          |   1 -
 arch/loongarch/kernel/paravirt.c              |  10 +-
 arch/powerpc/include/asm/paravirt.h           |   3 -
 arch/powerpc/include/asm/paravirt_api_clock.h |   2 -
 arch/powerpc/platforms/pseries/setup.c        |   4 +-
 arch/riscv/Kconfig                            |   1 +
 arch/riscv/include/asm/paravirt.h             |  14 -
 arch/riscv/include/asm/paravirt_api_clock.h   |   1 -
 arch/riscv/kernel/paravirt.c                  |  11 +-
 arch/x86/Kconfig                              |   8 +-
 arch/x86/entry/entry_64.S                     |   1 -
 arch/x86/entry/vsyscall/vsyscall_64.c         |   1 -
 arch/x86/hyperv/hv_spinlock.c                 |  11 +-
 arch/x86/include/asm/apic.h                   |   4 -
 arch/x86/include/asm/highmem.h                |   1 -
 arch/x86/include/asm/mshyperv.h               |   1 -
 arch/x86/include/asm/paravirt-base.h          |  29 ++
 arch/x86/include/asm/paravirt-spinlock.h      | 146 ++++++++
 arch/x86/include/asm/paravirt.h               | 331 +++++-------------
 arch/x86/include/asm/paravirt_api_clock.h     |   1 -
 arch/x86/include/asm/paravirt_types.h         | 269 +++++++-------
 arch/x86/include/asm/pgtable_32.h             |   1 -
 arch/x86/include/asm/ptrace.h                 |   2 +-
 arch/x86/include/asm/qspinlock.h              |  89 +----
 arch/x86/include/asm/spinlock.h               |   1 -
 arch/x86/include/asm/timer.h                  |   1 +
 arch/x86/include/asm/tlbflush.h               |   4 -
 arch/x86/kernel/Makefile                      |   2 +-
 arch/x86/kernel/apm_32.c                      |   1 -
 arch/x86/kernel/callthunks.c                  |   1 -
 arch/x86/kernel/cpu/bugs.c                    |   1 -
 arch/x86/kernel/cpu/vmware.c                  |   1 +
 arch/x86/kernel/kvm.c                         |  11 +-
 arch/x86/kernel/kvmclock.c                    |   1 +
 arch/x86/kernel/paravirt-spinlocks.c          |  26 +-
 arch/x86/kernel/paravirt.c                    |  42 +--
 arch/x86/kernel/tsc.c                         |  10 +-
 arch/x86/kernel/vsmp_64.c                     |   1 -
 arch/x86/kernel/x86_init.c                    |   1 -
 arch/x86/lib/cache-smp.c                      |   1 -
 arch/x86/mm/init.c                            |   1 -
 arch/x86/xen/enlighten_pv.c                   |  82 ++---
 arch/x86/xen/irq.c                            |  20 +-
 arch/x86/xen/mmu_pv.c                         | 100 ++----
 arch/x86/xen/spinlock.c                       |  11 +-
 arch/x86/xen/time.c                           |   2 +
 drivers/clocksource/hyperv_timer.c            |   2 +
 drivers/xen/time.c                            |   2 +-
 include/linux/sched/cputime.h                 |  18 +
 kernel/sched/core.c                           |   5 +
 kernel/sched/cputime.c                        |  13 +
 kernel/sched/sched.h                          |   3 +-
 tools/objtool/arch/x86/decode.c               |   8 +-
 tools/objtool/check.c                         |  78 ++++-
 tools/objtool/include/objtool/check.h         |   2 +
 67 files changed, 659 insertions(+), 827 deletions(-)

----------------------------------------------------------------------

New:  vfio: fix VFIO_IOMMU_UNMAP_DMA when end of range would
[PATCH] vfio: fix VFIO_IOMMU_UNMAP_DMA when end of range would
Author: Alex Mastro <amastro@fb.com>

vfio_find_dma_first_node is called to find the first dma node to unmap
given an unmap range of [iova..iova+size). The check at the end of the
function intends to test if the dma result lies beyond the end of the
unmap range. The condition is incorrectly satisfied when iova+size
overflows to zero, causing the function to return NULL.

The same issue happens inside vfio_dma_do_unmap's while loop.

Fix by comparing to the inclusive range end, which can be expressed
by u64.

This bug was discovered after querying for vfio_iova_range's via
VFIO_IOMMU_GET_INFO, making a VFIO_IOMMU_MAP_DMA inside the last range,
and then attempting to unmap the entirety of the last range i.e.
VFIO_IOMMU_UNMAP_DMA(iova=r.start, size=r.end-r.start+1).

---
I don't think iommufd is susceptible to the same issue since
iopt_unmap_iova computes the inclusive end using checked addition, and
iopt_unmap_iova_range acts on an inclusive range.

Signed-off-by: Alex Mastro <amastro@fb.com>
---
 drivers/vfio/vfio_iommu_type1.c | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

----------------------------------------------------------------------

