From cce23e993 to 82095394c
KVM mailing list update from cce23e993 to 82095394c

Top 15 contributor Email domains (Based on Email Body)

     17 kernel.org
      9 amd.com
      6 grsecurity.net
      5 redhat.com
      3 intel.com
      2 amazon.com=0A=
      2 163.com
      1 sifive.com
      1 amazon.co.uk

Top 15 contributors (Based on Email Body)

     17  Marc Zyngier <maz@kernel.org>
      6  Mathias Krause <minipli@grsecurity.net>
      5  Nikunj A Dadhania <nikunj@amd.com>
      4  Ashish Kalra <ashish.kalra@amd.com>
      3  Jason Wang <jasowang@redhat.com>
      3  Chao Gao <chao.gao@intel.com>
      2  Nikita Kalyazin <kalyazin@amazon.com>=0A=
      2  "Michael S. Tsirkin" <mst@redhat.com>
      2  Jinyu Tang <tjytimi@163.com>
      1  Samuel Holland <samuel.holland@sifive.com>
      1  "Kalyazin, Nikita" <kalyazin@amazon.co.uk>

===== Patch list in this time period =====


===== Patch Commit Messages ====

New:  Makefile: Provide a concept of late CFLAGS
[kvm-unit-tests PATCH v2 1/4] Makefile: Provide a concept of late CFLAGS
Author: Mathias Krause <minipli@grsecurity.net>

Allow architectures to provide CFLAGS that should be added only after
all other optional CFLAGS have been evaluated.

This will be useful for flags that depend on other, generic ones.

To allow 'LATE_CFLAGS' to make use of the $(cc-option ...) helper,
assume it'll be a lazily evaluated variable. To further ensure the
$(cc-option ...) compiler invocation overhead won't be per-use of
$(CFLAGS), enforce its evaluation prior to extending CFLAGS.

Signed-off-by: Mathias Krause <minipli@grsecurity.net>
---
 Makefile | 4 ++++
 1 file changed, 4 insertions(+)

----------------------------------------------------------------------

New:  Better backtraces for leaf functions
[kvm-unit-tests PATCH v2 0/4] Better backtraces for leaf functions
Author: Mathias Krause <minipli@grsecurity.net>

This is v2 of [1], trying to enhance backtraces involving leaf
functions.

This version fixes backtraces on ARM and ARM64 as well, as ARM currently
fails hard for leaf functions lacking a proper stack frame setup, making
it dereference invalid pointers. ARM64 just skips frames, much like x86
does.

v2 fixes this by introducing the concept of "late CFLAGS" that get
evaluated in the top-level Makefile once all other optional flags have
been added to $(CFLAGS), which is needed for x86's version at least.

Please apply!

Thanks,
Mathias

[1] https://lore.kernel.org/kvm/20250724181759.1974692-1-minipli@grsecurity.net/

Mathias Krause (4):
  Makefile: Provide a concept of late CFLAGS
  x86: Better backtraces for leaf functions
  arm64: Better backtraces for leaf functions
  arm: Fix backtraces involving leaf functions

 Makefile            |  4 ++++
 arm/Makefile.arm    |  8 ++++++++
 arm/Makefile.arm64  |  6 ++++++
 x86/Makefile.common | 11 +++++++++++
 lib/arm/stack.c     | 18 ++++++++++++++++--
 5 files changed, 45 insertions(+), 2 deletions(-)

----------------------------------------------------------------------

New:  x86/sev: Add new dump_rmp parameter to snp_leak_pages() API
[PATCH v5 1/3] x86/sev: Add new dump_rmp parameter to snp_leak_pages() API
Author: Ashish Kalra <Ashish.Kalra@amd.com>


When leaking certain page types, such as Hypervisor Fixed (HV_FIXED)
pages, it does not make sense to dump RMP contents for the 2MB range of
the page(s) being leaked. In the case of HV_FIXED pages, this is not an
error situation where the surrounding 2MB page RMP entries can provide
debug information.

Add new __snp_leak_pages() API with dump_rmp bool parameter to support
continue adding pages to the snp_leaked_pages_list but not issue
dump_rmpentry().

Make snp_leak_pages() a wrapper for the common case which also allows
existing users to continue to dump RMP entries.

Suggested-by: Thomas Lendacky <Thomas.Lendacky@amd.com>
Suggested-by: Sean Christopherson <seanjc@google.com>
Reviewed-by: Tom Lendacky <thomas.lendacky@amd.com>
Acked-by: Borislav Petkov (AMD) <bp@alien8.de>
Signed-off-by: Ashish Kalra <ashish.kalra@amd.com>
---
 arch/x86/include/asm/sev.h | 8 +++++++-
 arch/x86/virt/svm/sev.c    | 7 ++++---
 2 files changed, 11 insertions(+), 4 deletions(-)

----------------------------------------------------------------------

New:  crypto: ccp - Add AMD Seamless Firmware Servicing (SFS) driver
[PATCH v5 0/3] crypto: ccp - Add AMD Seamless Firmware Servicing (SFS) driver
Author: Ashish Kalra <Ashish.Kalra@amd.com>


AMD Seamless Firmware Servicing (SFS) is a secure method to allow
non-persistent updates to running firmware and settings without
requiring BIOS reflash and/or system reset.

SFS does not address anything that runs on the x86 processors and
it can be used to update ASP firmware, modules, register settings
and update firmware for other microprocessors like TMPM, etc.

SFS driver support adds ioctl support to communicate the SFS
commands to the ASP/PSP by using the TEE mailbox interface.

The Seamless Firmware Servicing (SFS) driver is added as a
PSP sub-device.

Includes pre-patch to add new generic SEV API interface to allocate/free
hypervisor fixed pages which abstracts hypervisor fixed page allocation
and free for PSP sub devices. The API internally uses SNP_INIT_EX to
transition pages to HV-Fixed page state.

For detailed information, please look at the SFS specifications:
https://www.amd.com/content/dam/amd/en/documents/epyc-technical-docs/specifications/58604.pdf

v5:
- Print pathname for firmware_request_nowarn() failure message.
- Changed all dev_warn() to dev_warn_ratelimited() for all ioctl
  error mesages.
- Add Reviewed-by's for all patches.

v4:
- Ensure that sev_cmd_mutex is acquired before the snp_initialized check
  in snp_alloc_hv_fixed_pages().
- Restore memory attributes back to the default "write-back" type in
  sfs_dev_destroy() and cleanup path for the error handling in
  sfs_dev_init().

v3:
- As not dumping the RMP entry is the rare case, crafting the APIs to
reflect that, i.e. make snp_leak_pages() a wrapper for the common case
to allow existing users to continue to dump RMP entries by calling
new __snp_leak_pages() API with dump_rmp bool parameter set to true.
The new parameter also adds support to continue adding pages to the
snp_leaked_pages_list but not issue dump_rmpentry().
- Fix kernel test robot build warning for uninitialized "ret" variable.

v2:
- Change API interface from adding/removing HV_Fixed pages to
  allocate/free HV_Fixed pages.
- Move to guard() for all mutexes/spinlocks.
- Handle case of SFS capability bit being set on multiple PSPs, add
  protection based on sev_dev_init() and sev_misc_init().
- Add new sfs_command structure and use it for programming both the
  GetFirmareVersions and UpdatePackage command.
- Use sfs_user_get_fw_versions and sfs_user_update_package structures
  for copy_to_/copy_from_user for the iotcls.
- Fix payload_path buffer size to prevent buffer overrun/stack
  corruption issues and also sanitize user provided payload_name to
  ensure it is null-terminated and use snprintf() to setup payload_path.
- Add new quiet parameter to snp_leak_pages() API and additionally change 
  all existing users of this API to pass quiet=false parameter
  maintaining current behavior.
- Remove mutex_init() and mutex_destroy() calls for statically declared
  mutex.
- Fix comments and commit logs.

Ashish Kalra (3):
  x86/sev: Add new dump_rmp parameter to snp_leak_pages() API
  crypto: ccp - Add new HV-Fixed page allocation/free API.
  crypto: ccp - Add AMD Seamless Firmware Servicing (SFS) driver

 arch/x86/include/asm/sev.h          |   8 +-
 arch/x86/virt/svm/sev.c             |   7 +-
 drivers/crypto/ccp/Makefile         |   3 +-
 drivers/crypto/ccp/psp-dev.c        |  20 ++
 drivers/crypto/ccp/psp-dev.h        |   8 +-
 drivers/crypto/ccp/sev-dev.c        | 182 ++++++++++++++++
 drivers/crypto/ccp/sev-dev.h        |   3 +
 drivers/crypto/ccp/sfs.c            | 311 ++++++++++++++++++++++++++++
 drivers/crypto/ccp/sfs.h            |  47 +++++
 include/linux/psp-platform-access.h |   2 +
 include/uapi/linux/psp-sfs.h        |  87 ++++++++
 11 files changed, 672 insertions(+), 6 deletions(-)

----------------------------------------------------------------------

New:  mm: guestmem: introduce guestmem library
[RFC PATCH v6 1/2] mm: guestmem: introduce guestmem library
Author: Kalyazin, Nikita <kalyazin@amazon.co.uk>

=0A=
Move MM-generic parts of guest_memfd from KVM to MM.  This allows other=0A=
hypervisors to use guestmem code and enables UserfaultFD implementation=0A=
for guest_memfd [1].  Previously it was not possible because KVM (and=0A=
guest_memfd code) might be built as a module.=0A=
=0A=
Based on a patch by Elliot Berman <quic_eberman@quicinc.com> [2].=0A=
=0A=
[1] https://lore.kernel.org/kvm/20250404154352.23078-1-kalyazin@amazon.com=
=0A=
[2] https://lore.kernel.org/kvm/20241122-guestmem-library-v5-2-450e92951a15=
@quicinc.com=0A=
=0A=
Signed-off-by: Nikita Kalyazin <kalyazin@amazon.com>=0A=
---=0A=
 MAINTAINERS              |   2 +=0A=
 include/linux/guestmem.h |  46 +++++=0A=
 mm/Kconfig               |   3 +=0A=
 mm/Makefile              |   1 +=0A=
 mm/guestmem.c            | 380 +++++++++++++++++++++++++++++++++++++++=0A=
 virt/kvm/Kconfig         |   1 +=0A=
 virt/kvm/guest_memfd.c   | 303 ++++---------------------------=0A=
 7 files changed, 465 insertions(+), 271 deletions(-)=0A=

----------------------------------------------------------------------

New:  mm: Refactor KVM guest_memfd to introduce guestmem
[RFC PATCH v6 0/2] mm: Refactor KVM guest_memfd to introduce guestmem
Author: Kalyazin, Nikita <kalyazin@amazon.co.uk>

This is a revival of the guestmem library patch series originated from=0A=
Elliot [1].  The reason I am bringing it up now is it would help=0A=
implement UserfaultFD support minor mode in guest_memfd.=0A=
=0A=
Background=0A=
=0A=
We are building a Firecracker version that uses guest_memfd to back=0A=
guest memory [2].  The main objective is to use guest_memfd to remove=0A=
guest memory from host kernel's direct map to reduce the surface for=0A=
Spectre-style transient execution issues [3].  Currently, Firecracker=0A=
supports restoring VMs from snapshots using UserfaultFD [4], which is=0A=
similar to the postcopy phase of live migration.  During restoration,=0A=
while we rely on a separate mechanism to handle stage-2 faults in=0A=
guest_memfd [5], UserfaultFD support in guest_memfd is still required to=0A=
handle faults caused either by the VMM itself or by MMIO access handling=0A=
on x86.=0A=
=0A=
The major problem in implementing UserfaultFD for guest_memfd is that=0A=
the MM code (UserfaultFD) needs to call KVM-specific interfaces.=0A=
Particularly for the minor mode, these are 1) determining the type of=0A=
the VMA (eg is_vma_guest_memfd()) and 2) obtaining a folio (ie=0A=
kvm_gmem_get_folio()).  Those may not be always available as KVM can be=0A=
compiled as a module.  Peter attempted to approach it via exposing an=0A=
ops structure where modules (such as KVM) could provide their own=0A=
callbacks, but it was not deemed to be sufficiently safe as it opens up=0A=
an unrestricted interface for all modules and may leave MM in an=0A=
inconsistent state [6].=0A=
=0A=
An alternative way to make these interfaces available to the UserfaultFD=0A=
code is extracting generic-MM guest_memfd parts into a library=0A=
(guestmem) under MM where they can be safely consumed by the UserfaultFD=0A=
code.  As far as I know, the original guestmem library series was=0A=
motivated by adding guest_memfd support in Gunyah hypervisor [7].=0A=
=0A=
This RFC=0A=
=0A=
I took Elliot's v5 (the latest) and rebased it on top of the guest_memfd=0A=
preview branch [8] because I also wanted to see how it would work with=0A=
direct map removal [3] and write syscall [9], which are building blocks=0A=
for the guest_memfd-based Firecracker version.  On top of it I added a=0A=
patch that implements UserfaultFD support for guest_memfd using=0A=
interfaces provided by the guestmem library to illustrate the complete=0A=
idea.=0A=
=0A=
I made the following modifications along the way:=0A=
 - Followed by a comment from Sean, converted invalidate_begin()=0A=
   callback back to void as it cannot fail in KVM, and the related=0A=
   Gunyah requirement is unknown to me=0A=
 - Extended the guestmem_ops structure with the supports_mmap() callback=0A=
   to provide conditional mmap support in guestmem=0A=
 - Extended the guestmem library interface with guestmem_allocate(),=0A=
   guestmem_test_no_direct_map(), guestmem_mark_prepared(),=0A=
   guestmem_mmap(), and guestmem_vma_is_guestmem()=0A=
 - Made (kvm_gmem)/(guestmem)_test_no_direct_map() use=0A=
   mapping_no_direct_map() instead of KVM-specific flag=0A=
   GUEST_MEMFD_FLAG_NO_DIRECT_MAP to make it KVM-independent=0A=
=0A=
Feedback that I would like to receive:=0A=
 - Is this the right solution to the "UserfaultFD in guest_memfd"=0A=
   problem?=0A=
 - What requirements from other hypervisors than KVM do we need to=0A=
   consider at this point?=0A=
 - Does the line between generic-MM and KVM-specific guest_memfd parts=0A=
   look sensible?=0A=
=0A=
Previous iterations of UserfaultFD support in guest_memfd patches:=0A=
v3:=0A=
 - https://lore.kernel.org/kvm/20250404154352.23078-1-kalyazin@amazon.com=
=0A=
 - minor changes to address review comments (James)=0A=
v2:=0A=
 - https://lore.kernel.org/kvm/20250402160721.97596-1-kalyazin@amazon.com=
=0A=
 - implement a full minor trap instead of hybrid missing/minor trap=0A=
   (James/Peter)=0A=
 - make UFFDIO_CONTINUE implementation generic calling vm_ops->fault()=0A=
v1:=0A=
 - https://lore.kernel.org/kvm/20250303133011.44095-1-kalyazin@amazon.com=
=0A=
=0A=
Nikita=0A=
=0A=
[1]: https://lore.kernel.org/kvm/20241122-guestmem-library-v5-2-450e92951a1=
5@quicinc.com=0A=
[2]: https://github.com/firecracker-microvm/firecracker/tree/feature/secret=
-hiding=0A=
[3]: https://lore.kernel.org/kvm/20250912091708.17502-1-roypat@amazon.co.uk=
=0A=
[4]: https://github.com/firecracker-microvm/firecracker/blob/main/docs/snap=
shotting/handling-page-faults-on-snapshot-resume.md=0A=
[5]: https://lore.kernel.org/kvm/20250618042424.330664-1-jthoughton@google.=
com=0A=
[6]: https://lore.kernel.org/linux-mm/20250627154655.2085903-1-peterx@redha=
t.com=0A=
[7]: https://lore.kernel.org/lkml/20240222-gunyah-v17-0-1e9da6763d38@quicin=
c.com=0A=
[8]: https://git.kernel.org/pub/scm/linux/kernel/git/david/linux.git/log/?h=
=3Dguestmemfd-preview=0A=
[9]: https://lore.kernel.org/kvm/20250902111951.58315-1-kalyazin@amazon.com=
=0A=
=0A=
Nikita Kalyazin (2):=0A=
  mm: guestmem: introduce guestmem library=0A=
  userfaulfd: add minor mode for guestmem=0A=
=0A=
 Documentation/admin-guide/mm/userfaultfd.rst |   4 +-=0A=
 MAINTAINERS                                  |   2 +=0A=
 fs/userfaultfd.c                             |   3 +-=0A=
 include/linux/guestmem.h                     |  46 +++=0A=
 include/linux/userfaultfd_k.h                |   8 +-=0A=
 include/uapi/linux/userfaultfd.h             |   8 +-=0A=
 mm/Kconfig                                   |   3 +=0A=
 mm/Makefile                                  |   1 +=0A=
 mm/guestmem.c                                | 380 +++++++++++++++++++=0A=
 mm/userfaultfd.c                             |  14 +-=0A=
 virt/kvm/Kconfig                             |   1 +=0A=
 virt/kvm/guest_memfd.c                       | 303 ++-------------=0A=
 12 files changed, 493 insertions(+), 280 deletions(-)=0A=

----------------------------------------------------------------------

New:  vhost-net: unbreak busy polling
[PATCH v3 1/3] vhost-net: unbreak busy polling
Author: Michael S. Tsirkin <mst@redhat.com>


Commit 67a873df0c41 ("vhost: basic in order support") pass the number
of used elem to vhost_net_rx_peek_head_len() to make sure it can
signal the used correctly before trying to do busy polling. But it
forgets to clear the count, this would cause the count run out of sync
with handle_rx() and break the busy polling.

Fixing this by passing the pointer of the count and clearing it after
the signaling the used.

Acked-by: Michael S. Tsirkin <mst@redhat.com>
Cc: stable@vger.kernel.org
Fixes: 67a873df0c41 ("vhost: basic in order support")
Signed-off-by: Jason Wang <jasowang@redhat.com>
Message-Id: <20250915024703.2206-1-jasowang@redhat.com>
Signed-off-by: Michael S. Tsirkin <mst@redhat.com>
---
 drivers/vhost/net.c | 7 ++++---
 1 file changed, 4 insertions(+), 3 deletions(-)

----------------------------------------------------------------------

New:  x86/eventinj: Use global asm label for nested NMI IP address verification
[PATCH 1/2] x86/eventinj: Use global asm label for nested NMI IP address verification
Author: Chao Gao <chao.gao@intel.com>

Use a global asm label to get the expected IP address for nested NMI
interception instead of reading a hardcoded offset from the stack.

the NMI test in eventinj.c verifies that a nested NMI occurs immediately at
the return address (IP register) in the IRET frame, as IRET opens the
NMI window. Currently, nested_nmi_iret_isr() reads the return address
using a magic offset (iret_stack[-3]), which is unclear and may break if
more values are pushed to the "iret_stack".

To improve readability, add a global 'ip_after_iret' label for the expected
return address, push it to the IRET frame, and verify it matches the
interrupted address in the nested NMI handler.

Signed-off-by: Chao Gao <chao.gao@intel.com>
---
 x86/eventinj.c | 11 +++++++----
 1 file changed, 7 insertions(+), 4 deletions(-)

----------------------------------------------------------------------

New:  Fix triple fault in eventinj test
[kvm-unit-tests PATCH 0/2] Fix triple fault in eventinj test
Author: Chao Gao <chao.gao@intel.com>

As reported in [1], the eventinj test can cause a triple fault due to an
invalid RSP after IRET. Fix this by pushing a valid stack pointer to the
crafted IRET frame in do_iret(), ensuring RSP is restored to a valid
stack in 64-bit mode.

[1]: https://lore.kernel.org/kvm/aMahfvF1r39Xq6zK@intel.com/

Chao Gao (2):
  x86/eventinj: Use global asm label for nested NMI IP address
    verification
  x86/eventinj: Push SP to IRET frame

 x86/eventinj.c | 12 ++++++++----
 1 file changed, 8 insertions(+), 4 deletions(-)

----------------------------------------------------------------------

New:  KVM: riscv: Power on secondary vCPUs from migration
[PATCH] KVM: riscv: Power on secondary vCPUs from migration
Author: Jinyu Tang <tjytimi@163.com>

The current logic keeps all secondary VCPUs powered off on their
first run in kvm_arch_vcpu_postcreate(), relying on the boot VCPU 
to wake them up by sbi call. This is correct for a fresh VM start,
where VCPUs begin execution at the bootaddress (0x80000000).

However, this behavior is not suitable for VCPUs that are being
restored from a state (e.g., during migration resume or snapshot
load). These VCPUs have a saved program counter (sepc). Forcing
them to wait for a wake-up from the boot VCPU, which may not
happen or may happen incorrectly, leaves them in a stuck state
when using Qemu to migration if smp is larger than one.

So check a cold start and a warm resumption by the value of the 
guest's sepc register. If the VCPU is running for the first time 
*and* its sepc is not the hardware boot address, it indicates a 
resumed vCPU that must be powered on immediately to continue 
execution from its saved context.

Signed-off-by: Jinyu Tang <tjytimi@163.com>
Tested-by: Tianshun Sun <stsmail163@163.com>
---
 arch/riscv/kvm/vcpu.c | 10 +++++++++-
 1 file changed, 9 insertions(+), 1 deletion(-)

----------------------------------------------------------------------

New:  KVM: arm64: Add helper computing the state of 52bit PA support
[PATCH v2 01/16] KVM: arm64: Add helper computing the state of 52bit PA support
Author: Marc Zyngier <maz@kernel.org>

Track whether the guest is using 52bit PAs, either LPA or LPA2.
This further simplifies the handling of LVA for 4k and 16k pages,
as LPA2 implies LVA in this case.

Signed-off-by: Marc Zyngier <maz@kernel.org>
---
 arch/arm64/include/asm/kvm_nested.h |  1 +
 arch/arm64/kvm/at.c                 | 31 ++++++++++++++++++++++++-----
 2 files changed, 27 insertions(+), 5 deletions(-)

----------------------------------------------------------------------

New:  KVM: arm64: TTW reporting on SEA and 52bit PA in S1 PTW
[PATCH v2 00/16] KVM: arm64: TTW reporting on SEA and 52bit PA in S1 PTW
Author: Marc Zyngier <maz@kernel.org>

Yes, $SUBJECT rolls off the tongue.

This series was triggered by the realisation that when injecting an
SEA while on a S1PTW fault, we don't report the level of the walk and
instead give a bare SEA, which definitely violates the architecture.

This state of things dates back to the pre-NV days, when we didn't
have a S1 page table walker, and really didn't want to implement one.
I've since moved on and reluctantly implemented one, which means we
now *could* provide the level if we really wanted to.

However, nothing is that simple. The current code in at.c is firmly
48bit, as our NV implementation doesn't yet support 52bit PA, while an
EL1 VM can happily enjoy LPA and LPA2. As a result, it is necessary to
expand the S1 PTW to support both LPA and LPA2. Joy.

Then, once the above is achieved, we need to hook into the PTW
machinery to match the first level of the walk that results in
accessing the faulty address. For this, we introduce a simple filter
mechanism that could be expanded if we needed to (no, please no).

Finally, we can plug this into the fault injection path, and enjoy
seeing the translation level being populated in the ESR_ELx register.

Patches on top of 6.16-rc4. I intend to take this into 6.18, so shout
if you don't like the idea!

* From v1 [1]:

  - Rebased on -rc4 to avoid a simple conflict.

[1] https://lore.kernel.org/r/20250827161039.938958-1-maz@kernel.org

Marc Zyngier (16):
  KVM: arm64: Add helper computing the state of 52bit PA support
  KVM: arm64: Account for 52bit when computing maximum OA
  KVM: arm64: Compute 52bit TTBR address and alignment
  KVM: arm64: Decouple output address from the PT descriptor
  KVM: arm64: Pass the walk_info structure to compute_par_s1()
  KVM: arm64: Compute shareability for LPA2
  KVM: arm64: Populate PAR_EL1 with 52bit addresses
  KVM: arm64: Expand valid block mappings to FEAT_LPA/LPA2 support
  KVM: arm64: Report faults from S1 walk setup at the expected start
    level
  KVM: arm64: Allow use of S1 PTW for non-NV vcpus
  KVM: arm64: Allow EL1 control registers to be accessed from the CPU
    state
  KVM: arm64: Don't switch MMU on translation from non-NV context
  KVM: arm64: Add filtering hook to S1 page table walk
  KVM: arm64: Add S1 IPA to page table level walker
  KVM: arm64: Populate level on S1PTW SEA injection
  KVM: arm64: selftest: Expand external_aborts test to look for TTW
    levels

 arch/arm64/include/asm/kvm_nested.h           |  25 +-
 arch/arm64/kvm/at.c                           | 341 +++++++++++++-----
 arch/arm64/kvm/inject_fault.c                 |  27 +-
 arch/arm64/kvm/nested.c                       |   2 +-
 .../selftests/kvm/arm64/external_aborts.c     |  43 +++
 .../selftests/kvm/include/arm64/processor.h   |   1 +
 .../selftests/kvm/lib/arm64/processor.c       |  13 +-
 7 files changed, 362 insertions(+), 90 deletions(-)

----------------------------------------------------------------------

New:  KVM: x86: Carve out PML flush routine
[PATCH v2 1/4] KVM: x86: Carve out PML flush routine
Author: Nikunj A Dadhania <nikunj@amd.com>

Move the PML (Page Modification Logging) buffer flushing logic from
VMX-specific code to common x86 KVM code to enable reuse by SVM and avoid
code duplication.

The AMD SVM PML implementations share the same behavior as VMX PML:
 1) The PML buffer is a 4K page with 512 entries
 2) Hardware records dirty GPAs in reverse order (from index 511 to 0)
 3) Hardware clears bits 11:0 when recording GPAs

The PML constants (PML_LOG_NR_ENTRIES and PML_HEAD_INDEX) are moved from
vmx.h to x86.h to make them available to both VMX and SVM.

No functional change intended for VMX, except tone down the WARN_ON() to
WARN_ON_ONCE() for the page alignment check. If hardware exhibits this
behavior once, it's likely to occur repeatedly, so use WARN_ON_ONCE() to
avoid log flooding while still capturing the unexpected condition.

The refactoring prepares for SVM to leverage the same PML flushing
implementation.

Signed-off-by: Nikunj A Dadhania <nikunj@amd.com>
---
 arch/x86/kvm/vmx/vmx.c | 26 ++------------------------
 arch/x86/kvm/vmx/vmx.h |  5 -----
 arch/x86/kvm/x86.c     | 31 +++++++++++++++++++++++++++++++
 arch/x86/kvm/x86.h     |  7 +++++++
 4 files changed, 40 insertions(+), 29 deletions(-)

----------------------------------------------------------------------

New:  KVM: SVM: Add Page Modification Logging (PML) support
[PATCH v2 0/4] KVM: SVM: Add Page Modification Logging (PML) support
Author: Nikunj A Dadhania <nikunj@amd.com>

This series implements Page Modification Logging (PML) for guests, bringing
hardware-assisted dirty logging support. PML is designed to track guest
modified memory pages. PML enables the hypervisor to identify which pages in a
guest's memory have been modified since the last checkpoint or during live
migration.

The PML feature uses two new VMCB fields (PML_ADDR and PML_INDEX) and
generates a VMEXIT when the 4KB log buffer becomes full.

Patch breakdown:
1. Refactor existing VMX PML code to be shared between VMX and SVM
2. Prepare to share the PML page in VMX and SVM
3. Add AMD SVM PML CPUID
4. Implement SVM PML support using the shared infrastructure

The feature is enabled by default when hardware support is detected and
can be disabled via the 'pml' module parameter.

Changelog:

v2:
* Rebased on latest kvm/next
* Added patch to move pml_pg field from struct vcpu_vmx to struct kvm_vcpu_arch
  to share the PML page. (Kai Huang)
* Dropped the SNP safe allocation optimization patch, will submit it separately.
* Update commit message adding explicit mention that AMD PML follows VMX behavior
  (Kai Huang)
* Updated SNP erratum comment to include PML buffer alongside VMCB, VMSA, and
  AVIC pages. (Kai Huang)

RFC: https://lore.kernel.org/kvm/20250825152009.3512-1-nikunj@amd.com/


Nikunj A Dadhania (4):
  KVM: x86: Carve out PML flush routine
  KVM: x86: Move PML page to common vcpu arch structure
  x86/cpufeatures: Add Page modification logging
  KVM: SVM: Add Page modification logging support

 arch/x86/include/asm/cpufeatures.h |  1 +
 arch/x86/include/asm/kvm_host.h    |  2 +
 arch/x86/include/asm/svm.h         |  6 +-
 arch/x86/include/uapi/asm/svm.h    |  2 +
 arch/x86/kernel/cpu/scattered.c    |  1 +
 arch/x86/kvm/svm/sev.c             |  2 +-
 arch/x86/kvm/svm/svm.c             | 99 +++++++++++++++++++++++++++++-
 arch/x86/kvm/svm/svm.h             |  4 ++
 arch/x86/kvm/vmx/vmx.c             | 48 ++++-----------
 arch/x86/kvm/vmx/vmx.h             |  7 ---
 arch/x86/kvm/x86.c                 | 31 ++++++++++
 arch/x86/kvm/x86.h                 |  7 +++
 12 files changed, 163 insertions(+), 47 deletions(-)

----------------------------------------------------------------------

New:  RISC-V: KVM: Fix SBI_FWFT_POINTER_MASKING_PMLEN algorithm
[PATCH] RISC-V: KVM: Fix SBI_FWFT_POINTER_MASKING_PMLEN algorithm
Author: Samuel Holland <samuel.holland@sifive.com>

The implementation of SBI_FWFT_POINTER_MASKING_PMLEN from commit
aa04d131b88b ("RISC-V: KVM: Add support for SBI_FWFT_POINTER_MASKING_PMLEN")
was based on a draft of the SBI 3.0 specification, and is not compliant
with the ratified version.

Update the algorithm to be compliant. Specifically, do not fall back to
a pointer masking mode with a larger PMLEN if the mode with the
requested PMLEN is unsupported by the hardware.

Fixes: aa04d131b88b ("RISC-V: KVM: Add support for SBI_FWFT_POINTER_MASKING_PMLEN")
Signed-off-by: Samuel Holland <samuel.holland@sifive.com>
---
I saw that the RFC version of this patch already made it into
riscv_kvm_queue, but it needs an update for ratified SBI 3.0. Feel free
to squash this into the original commit, or I can send a replacement v2
patch if you prefer.

 arch/riscv/kvm/vcpu_sbi_fwft.c | 17 +++++++++++++----
 1 file changed, 13 insertions(+), 4 deletions(-)

----------------------------------------------------------------------

Exist: [PATCH v3 1/3] vhost-net: unbreak busy polling
 Skip: [PATCH net V2 1/2] vhost-net: unbreak busy polling
